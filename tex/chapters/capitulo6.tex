\chapter{FORMAS BILINEALES Y CUADRÁTICAS}\label{chapter:bilineal}
%\startcontents
\printchaptertableofcontents

\section{Eigenvalores y eigenvectores}

%Los conceptos de eigenvalores y eigenvectores son fundamentales para entender las transformaciones lineales y sus efectos sobre el espacio vectorial.
Dada una transformación lineal $T: V \longrightarrow W$, en diversas aplicaciones  resulta útil encontrar un vector $\mathbb{v}$ en $V$ tal que $T\mathbb{v}$ y $\mathbb{v}$ son paralelos. Es decir, se busca un vector $\mathbb{v}$ no nulo y un escalar $\lambda$ tal que
$$T\mathbb{v} = \lambda \mathbb{v}$$
Al escalar $\lambda$ se le denomina \emph{eigenvalor} asociado al \emph{eigenvector} $\mathbb{v}$. Es importante destacar que cada eigenvector puede tener asociado más de un eigenvalor.

\begin{definition}\label{def:eigenvalor}
    Sea $A$ una matriz de $n \times n$ con componentes reales y $\lambda$ un número real o complejo. Se dice que $\lambda$ es un eigenvalor de la matriz $A$, si existe $\mathbb{v} \in \CC[n]$ no nulo tal que
    $$A\mathbb{v} = \lambda \mathbb{v}$$
    A $\mathbb{v} \neq \mathbb{0}$ se le denomina eigenvector de $A$ correspondiente al eigenvalor $\lambda$.
\end{definition}

Hemos visto la definición de un eigenvalor y eigenvector, pero ¿cómo podemos calcular estos mismos? Observemos que para calcular los eigenvalores y eigenvectores de una matriz cuadrada, necesitamos encontrar los valores de $\lambda$ que permiten que la ecuación $A \mathbb{v} = \lambda \mathbb{v}$ se cumpla para un vector no nulo $\mathbb{v}$. Esto se puede hacer reescribiendo la ecuación como
$$A \mathbb{v} - \lambda \mathbb{v} = \mathbb{0}$$

\infoBulle{Los eigenvalores y eigenvectores son comúnmente conocidos como valores y vectores propios, respectivamente, o valores y vectores característicos. El término alemán \emph{eigen} significa “propio”.}
\infoBulle{La definición \ref{def:eigenvalor} es válida incluso si $A$ tiene componentes complejas. Sin embargo, dado que las matrices que estamos considerando principalmente tienen componentes reales, la definición es adecuada para nuestros propósitos.}

\newpage
\infoBulle{Como se mostrará más adelante, incluso una matriz con elementos reales puede poseer eigenvalores y eigenvectores complejos. Por esta razón, en la definición \ref{def:eigenvalor}, se especifica que el eigenvector tiene entradas complejas, es decir, $\mathbb{v} \in \CC[n]$. Aunque no se abordarán exhaustivamente muchos aspectos relacionados con los números complejos en esta obra, se incluye una exposición básica de los conceptos indispensables en el \hyperref[chap:numeros-complejos]{Apéndice B}.}

\noindent o bien,
$$(A - \lambda I_n) \mathbb{v} = \mathbb{0}$$
donde $I_n$ es la matriz identidad del mismo tamaño que la matriz $A$. Esto lleva a buscar valores de $\lambda$ que hagan que $A - \lambda I_n$ sea singular. Una vez que se encuentran estos eigenvalores, se puede calcular el eigenvector correspondiente resolviendo el sistema de ecuaciones lineales asociado.

\begin{theorem}
    Sea $A$ una matriz de $n \times n$. Entonces $\lambda$ es un eigenvalor de $A$ si y solo si
    $$p(\lambda) = \det(A - \lambda I_n) = 0$$
    A esta expresión se le denomina ecuación característica de $A$, y la función
    $$p(\lambda) = \det(A - \lambda I_n)$$
    se conoce como el polinomio característico de $A$. \\
    \demostracion Si $A$ tiene un eigenvalor $\lambda$ correspondiente a un eigenvector $\mathbb{v}$, entonces, por definición, $A \mathbb{v} = \lambda \mathbb{v}$ o equivalentemente
    $$A \mathbb{v} - \lambda \mathbb{v} = \mathbb{0}$$
    Es decir,
    $$(A - \lambda I_n) \mathbb{v} = \mathbb{0}$$
    Dado que $\mathbb{v} \neq \mathbb{0}$, de la expresión anterior se deduce que $A - \lambda I_n$ es singular. Por lo tanto, los eigenvalores de $A$ son solo aquellos números $\lambda$ reales o complejos para los cuales $A - \lambda I_n$ son singulares. Así, del teorema \ref{singular_determinante} se hereda que $A - \lambda I_n$ es singular si
    $$\det(A - \lambda I_n) = 0$$
    Recíprocamente, supongamos que $\det(A - \lambda I_n) = 0$. Entonces, de nuevo por el teorema \ref{singular_determinante}, $A - \lambda I_n$ no es invertible. Luego existe $\mathbb{v} \neq \mathbb{0}$ tal que $(A - \lambda I_n) \mathbb{v} = \mathbb{0}$ y claramente $A \mathbb{v} = \lambda \mathbb{v}$. Por lo tanto, $\mathbb{v}$ es un eigenvector (con $\lambda$ como eigenvalor asociado) de $A$.
\end{theorem}

\begin{example}\label{example_primero_eigenvalores}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*}$. \\
    \solucion Usando el teorema anterior, tenemos
    \begin{align*}
        \det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & \phantom{-} 0 \\
            0 & -1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)(-1 - \lambda) \\
        & = (\lambda - 1)(\lambda + 1) \\
        & = \lambda^2 - 1
    \end{align*}
    Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - 1 = 0$. De esta forma, los eigenvalores de $A$ son $\lambda_1 = 1$ y $\lambda_2 = -1$. Ahora, determinemos los eigenvectores de $A$. Sea $\mathbb{v}_1 = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda_1 = 1$
    $$A \mathbb{v}_1 = \lambda_1 \mathbb{v}_1$$
    entonces
    $$\begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = 1 \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    y, por ende
    \begin{align*}
        a & = a \\
        - b & = b
    \end{align*}\newpage\noindent
    Observemos que $b = 0$ y $a$ puede ser cualquier número real distinto de $0$. Así, el eigenvector $\mathbb{v}_1$ correspondiente a $\lambda_1 = 1$ está dado por
    $$\mathbb{v}_1 = \begin{pmatrix}
        a \\
        0
    \end{pmatrix}$$
    En particular,
    $$\mathbb{v}_1 = \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}$$
    De manera análoga, sea $\mathbb{v}_2 = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda_2 = -1$
    $$A \mathbb{v}_2 = \lambda_2 \mathbb{v}_2$$
    entonces
    $$\begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = - 1 \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    y, por ende
    \begin{align*}
        a & = - a \\
        b & = b
    \end{align*}
    Observemos que $a = 0$ y $b$ puede ser cualquier número real distinto de $0$. Así, el eigenvector $\mathbb{v}_2$ correspondiente a $\lambda_2 = -1$ está dado por
    $$\mathbb{v}_2 = \begin{pmatrix}
        0 \\
        b
    \end{pmatrix}$$
    En particular,
    $$\mathbb{v}_2 = \begin{pmatrix}
        0 \\
        1
    \end{pmatrix}$$
\end{example}

\begin{example}\label{ejemplo_eigenvalores_complejos}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*}$. \\
    \solucion Usando el teorema anterior, tenemos
    \begin{align*}
        \det(A - \lambda I_n) & = \begin{vmatrix}
            3 - \lambda & \phantom{-} -5 \\
            1 & -1 - \lambda
        \end{vmatrix} \\
        & = (3 - \lambda)(-1 - \lambda) + 5 \\
        & = \lambda^2 - 2\lambda + 2
    \end{align*}
    Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - 2\lambda + 2 = 0$. De esta forma, los eigenvalores de $A$ son $\lambda = 1 + i$ y $\overline{\lambda} = 1 - i$. Ahora, determinemos los eigenvectores de $A$. Sea $\mathbb{v} = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda = 1 + i$
    $$A \mathbb{v} = \lambda \mathbb{v}$$
    entonces
    $$\begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = (1 + i) \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    es decir
    \begin{align*}
        3a - 5b & = (1 + i)a \\
        a - b & = (1 + i)b
    \end{align*}
    lo cual se traduce en
    \begin{align*}
        3a - (1 + i)a - 5b & = 0 \\
        a - b - (1 + i)b & = 0
    \end{align*}\newpage\noindent
    y, por ende,
    \begin{align*}
        (2 - i)a - 5b & = 0 \\
        a - (2 + i)b & = 0
    \end{align*}
    Observemos que la segunda ecuación es igual a la primera, pues multiplicando la segunda ecuación por $2 - i$, se sigue que
    $$(2 - i)a - (2 + i)(2 - i)b = 0$$
    y se obtiene
    $$(2 - i)a - 5b = 0$$
    que es la primer ecuación. Así que basta resolver esta ecuación para encontrar el eigenvector deseado. De la segunda ecuación del sistema, se sigue que $a = (2 + i)b$, por lo que
    $$\mathbb{v} = \begin{pmatrix}
        (2 + i)b \\
        b
    \end{pmatrix}$$
    En particular, el eigenvector $\mathbb{v}$ correspondiente a $\lambda = 1 + i$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        2 + i \\
        1
    \end{pmatrix}$$
    De manera análoga, sea $\mathbb{v} = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\overline{\lambda} = 1 - i$
    $$A \mathbb{v} = \overline{\lambda} \mathbb{v}$$
    entonces
    $$\begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = (1 - i) \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    es decir
    \begin{align*}
        3a - 5b & = (1 - i)a \\
        a - b & = (1 - i)b
    \end{align*}
    lo cual se traduce en
    \begin{align*}
        3a - (1 - i)a - 5b & = 0 \\
        a - b - (1 - i)b & = 0
    \end{align*}
    y, por ende,
    \begin{align*}
        (2 + i)a - 5b & = 0 \\
        a - (2 - i)b & = 0
    \end{align*}
    Observemos que la segunda ecuación es igual a la primera, pues multiplicando la segunda ecuación por $2 + i$, se sigue que
    $$(2 + i)a - (2 + i)(2 - i)b = 0$$
    y se obtiene
    $$(2 + i)a - 5b = 0$$
    que es la primer ecuación. Así que basta resolver esta ecuación para encontrar el eigenvector deseado. De la segunda ecuación del sistema, se sigue que $a = (2 - i)b$, por lo que
    $$\mathbb{v} = \begin{pmatrix}
        (2 - i)b \\
        b
    \end{pmatrix}$$
    En particular, el eigenvector $\mathbb{v}$ correspondiente a $\overline{\lambda} = 1 - i$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        2 - i \\
        1
    \end{pmatrix}$$
    Observemos que este vector tiene como entradas el conjugado del primer eigenvector que obtuvimos, por lo que podemos denotarlo como $\overline{\mathbb{v}}$.
\end{example}

\newpage

\begin{observation}
    Los eigenvalores de una matriz real ocurren en pares de números complejos conjugados, y los eigenvectores correspondientes son también complejos conjugados entre sí.
\end{observation}

\begin{definition}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y $\lambda$ un eigenvalor de $A$. Definimos el eigenespacio o espacio propio de la matriz $A$ correspondiente a $\lambda$ como
    $$E_{\lambda} = \left\{ \mathbb{v} \in \CC[n] \mid A \mathbb{v} = \lambda \mathbb{v} \right\}$$
\end{definition}

\begin{theorem}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y $\lambda$ un eigenvalor de $A$. Entonces $E_{\lambda}$ es un subespacio de $\CC[n]$. \\
    \demostracion Para probar que $E_{\lambda}$ es subespacio de $\CC[n]$, debemos probar que $E_{\lambda}$ cumple los dos axiomas de cerradura. Sean $\mathbb{v}_1$, $\mathbb{v}_2 \in E_{\lambda}$, entonces
    $$A \mathbb{v}_1 = \lambda \mathbb{v}_1$$
    y
    $$A \mathbb{v}_2 = \lambda \mathbb{v}_2$$
    Así
    \begin{align*}
        A(\mathbb{v}_1 + \mathbb{v}_2) & = A\mathbb{v}_1 + A\mathbb{v}_2 \\
        & = \lambda \mathbb{v}_1 + \lambda \mathbb{v}_2 \\
        & = \lambda (\mathbb{v}_1 + \mathbb{v}_2)
    \end{align*}
    Por lo tanto, $A(\mathbb{v}_1 + \mathbb{v}_2) = \lambda (\mathbb{v}_1 + \mathbb{v}_2)$ de donde se sigue que $\mathbb{v}_1 + \mathbb{v}_2 \in E_{\lambda}$. De manera análoga, sean $\mathbb{v} \in E_{\lambda}$ y $\alpha \in \CC$, entonces
    \begin{align*}
        A(\alpha \mathbb{v}) & = \alpha (A \mathbb{v}) \\
        & = \alpha (\lambda \mathbb{v}) \\
        & = \lambda \alpha \mathbb{v} \\
        & = \lambda (\alpha \mathbb{v})
    \end{align*}
    En consecuencia, $\alpha \mathbb{v} \in E_{\lambda}$. Por lo tanto, dado que se cumplen ambas propiedades de cerradura, se concluye que $E_{\lambda}$ es un subespacio de $\CC[n]$.
\end{theorem}

\begin{theorem}\label{eigenvalores_distintos_eigenvectoresli}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y sean $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_k$ eigenvalores distintos entre sí de $A$ correspondientes a los eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_k$. Entonces el conjunto de vectores $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_k \right\}$ es linealmente independiente. \\
    \demostracion Procedamos por inducción sobre $k$. Es evidente que, para $k = 1$, obtenemos un eigenvalor $\lambda_1$ asociado con un eigenvector $\mathbb{v}_1$, lo que da lugar a un único vector que es linealmente independiente. Si $k = 2$, entonces tenemos dos eigenvalores $\lambda_1$ y $\lambda_2$ asociados con los eigenvectores $\mathbb{v}_1$ y $\mathbb{v}_2$ respectivamente. Demostremos que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2 \right\}$ es, en efecto, linealmente independiente. Por definición,
    \begin{equation}
        a\mathbb{v}_1 + b\mathbb{v}_2 = \mathbb{0}, \text{ con } a, b \in \CC \label{JAJAJASGQQTAUSOSOQIAOAOA}
    \end{equation}
    así que debemos probar que $a = 0 = b$. Para ello, de la anterior expresión si multiplicamos por la matriz $A$, entonces
    $$A \left( a\mathbb{v}_1 + b\mathbb{v}_2 \right) = \mathbb{0}$$
    lo que se traduce en
    $$A(a\mathbb{v}_1) + A(b\mathbb{v}_2) = \mathbb{0}$$
    y, por ende
    $$aA\mathbb{v}_1 + bA\mathbb{v}_2 = \mathbb{0}$$
    entonces
    \begin{equation}
        a \lambda_1 \mathbb{v}_1 + b \lambda_2 \mathbb{v}_2 = \mathbb{0} \label{JAJAJSBDJJDJDJDJDJD}
    \end{equation}
    Ahora, multiplicando por $\lambda_1$ la ecuación \eqref{JAJAJASGQQTAUSOSOQIAOAOA}, obtenemos
    \begin{equation}
        a \lambda_1 \mathbb{v}_1 + b \lambda_1 \mathbb{v}_2 = \mathbb{0} \label{JAJAJQQUWJSJSISIS}
    \end{equation}
    Al restar la ecuación \eqref{JAJAJQQUWJSJSISIS} de la ecuación \eqref{JAJAJSBDJJDJDJDJDJD}, se sigue que
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        a\lambda_1\mathbb{v}_1 + b\lambda_2\mathbb{v}_2 = \mathbb{0} \\
        a\lambda_1\mathbb{v}_1 + b\lambda_1\mathbb{v}_2 = \mathbb{0} \\
        \hline
        b (\lambda_2 - \lambda_1) \mathbb{v}_2 = \mathbb{0}
    \end{array}}$$
    Sin embargo, según la hipótesis, se cumple que $\lambda_2 - \lambda_1 \neq 0$, ya que los eigenvalores de $A$ son diferentes entre sí. Además, dado que $\mathbb{v}_2$ es un eigenvector, se tiene que, por definición, $\mathbb{v}_2 \neq \mathbb{0}$; lo que implica que necesariamente $b = 0$. Al sustituir el valor de $b$ en la ecuación \eqref{JAJAJASGQQTAUSOSOQIAOAOA}, se obtiene que $a\mathbb{v}_1 = \mathbb{0}$. Dado que $\mathbb{v}_1$ es un eigenvector, se cumple por definición que $\mathbb{v}_1 \neq \mathbb{0}$, lo que implica que $a = 0$. Esto prueba que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2 \right\}$ es, en efecto, linealmente independiente. Supongamos que el teorema se cumple para $k = l$ eigenvectores, es decir, supongamos que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_l \right\}$ es linealmente independiente. Entonces debemos probar el teorema para $k = l + 1$ eigenvectores, es decir, debemos probar que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_{l + 1} \right\}$ es linealmente independiente. Por definición,
    \begin{equation}
        \alpha_1 \mathbb{v}_1 + \alpha_2 \mathbb{v}_2 + \cdots + \alpha_{l} \mathbb{v}_{l} + \alpha_{l + 1} \mathbb{v}_{l + 1} = \mathbb{0} \label{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO}
    \end{equation}
    con $\alpha_i \in \CC$. Así que debemos probar que $\alpha_i = 0$ y para ello, de la anterior expresión si la multiplicamos por la matriz $A$, entonces
    $$A \left( \alpha_1 \mathbb{v}_1 + \alpha_2 \mathbb{v}_2 + \cdots + \alpha_{l} \mathbb{v}_{l} + \alpha_{l + 1} \mathbb{v}_{l + 1} \right) = \mathbb{0}$$
    lo que de traduce en
    $$A(\alpha_1\mathbb{v}_1) + A(\alpha_2\mathbb{v}_2) + \cdots + A(\alpha_l\mathbb{v}_l)+ A(\alpha_{l+1}\mathbb{v}_{l+1}) = \mathbb{0}$$
    y, por ende
    $$\alpha_1A\mathbb{v}_1 + \alpha_2A\mathbb{v}_2 + \cdots + \alpha_lA\mathbb{v}_l + \alpha_{l+1}A\mathbb{v}_{l+1} = \mathbb{0}$$
    entonces
    \begin{equation}
        \alpha_1\lambda_1\mathbb{v}_1 + \alpha_2\lambda_2\mathbb{v}_2 + \cdots + \alpha_l\lambda_l\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \label{IAJQUUQJQVQHQHAHHAHAHAVACACAVA}
    \end{equation}
    Ahora, multiplicando $\lambda_{l+1}$ la ecuación \eqref{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO}, obtenemos
    \begin{equation}
        \alpha_1\lambda_{l+1}\mathbb{v}_1 + \alpha_2\lambda_{l+1}\mathbb{v}_2 + \cdots + \alpha_l\lambda_{l+1}\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \label{AAOSIKQKWJWHGWHWVSVS}
    \end{equation}
    Al restar la ecuación \eqref{AAOSIKQKWJWHGWHWVSVS} de la ecuación \eqref{IAJQUUQJQVQHQHAHHAHAHAVACACAVA}, se sigue que
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        \alpha_1\lambda_1\mathbb{v}_1 + \alpha_2\lambda_2\mathbb{v}_2 + \cdots + \alpha_l\lambda_l\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \\
        \alpha_1\lambda_{l+1}\mathbb{v}_1 + \alpha_2\lambda_{l+1}\mathbb{v}_2 + \cdots + \alpha_l\lambda_{l+1}\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \\
        \hline
        \alpha_1(\lambda_1 - \lambda_{l+1})\mathbb{v}_1 + \alpha_2(\lambda_2 - \lambda_{l+1})\mathbb{v}_2 + \cdots + \alpha_l(\lambda_l - \lambda_{l+1})\mathbb{v}_l = \mathbb{0}
    \end{array}}$$
    Pero de acuerdo con la suposición de inducción, el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_l \right\}$ es linealmente independiente. Así,
    $$\alpha_1(\lambda_1 - \lambda_{l+1}) = \alpha_2(\lambda_2 - \lambda_{l+1}) = \cdots = \alpha_l(\lambda_l - \lambda_{l+1}) = 0$$
    y como los eigenvalores de $A$ son diferentes entre sí, se concluye que
    $$\alpha_1 = \alpha_2 = \cdots = \alpha_l = 0$$
    Pero, de la expresión \eqref{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO} se sigue que $\alpha_{l+1} = 0$. Por lo tanto, el teorema se cumple para $k = l + 1$. Por lo tanto, dados $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_k$ eigenvalores distintos entre sí de $A$ correspondientes a los eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_k$, el conjunto de vectores $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_k \right\}$ es linealmente independiente.
\end{theorem}

\newpage

\begin{example}
    En el ejemplo \ref{ejemplo_eigenvalores_complejos}, se determinó que $\lambda = 1 + i$ y $\overline{\lambda} = 1 - i$ son eigenvalores de la matriz $A$ y, además, son distintos entre sí. En ese mismo ejemplo, se calcularon los eigenvectores asociados a $\lambda$ y $\overline{\lambda}$, que son
    $$\mathbb{v} = \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} \quad \text{ y } \quad \overline{\mathbb{v}} = \begin{pmatrix} 2 - i \\ 1 \end{pmatrix}$$
    respectivamente. Demostremos que estos vectores son linealmente independientes, es decir, demostremos que no existen $\alpha_1$, $\alpha_2 \in \CC$ no nulos tales que
    $$\alpha_1 \mathbb{v} + \alpha_2 \overline{\mathbb{v}} = \mathbb{0}$$
    Sustituyendo los valores de $\mathbb{v}$ y $\overline{\mathbb{v}}$, se sigue que
    $$\alpha_1 \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 2 - i \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$
    Del cual, obtenemos el siguiente sistema
    \begin{align*}
        \alpha_1 (2 + i) + \alpha_2 (2 - i) & = 0 \\
        \alpha_1 + \alpha_2 & = 0
    \end{align*}
    Al multiplicar la segunda ecuación por $2 + i$ y restarla de la primera ecuación, se obtiene lo siguiente:
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        \alpha_1 (2 + i) + \alpha_2 (2 - i) = 0 \\
        \alpha_1(2 + i) + \alpha_2(2 + i) = 0 \\
        \hline
        \alpha_2 [(2 - i) - (2 + i)] = 0
    \end{array}}$$
    Por lo tanto, $\alpha_2(-2i) = 0$, de donde se deduce que $\alpha_2 = 0$. Al sustituir el valor de $\alpha_2$ en la primera ecuación, se concluye que $\alpha_1 = 0$. De esta forma, se demuestra que $\mathbb{v}$ y $\overline{\mathbb{v}}$ son linealmente independientes. Además, este ejemplo particular ilustra la validez del teorema anterior.
\end{example}

\begin{theorem}
    Los eigenvalores de una matriz triangular superior o inferior son exactamente los elementos de la diagonal de la matriz. \\
    \demostracion Sea $A$ una matriz de $n \times n$ triangular superior como sigue:
    $$A = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        0 & a_{22} & \cdots & a_{2n} \\
        \vdots & & \ddots & \\
        0 & 0 & \cdots & a_{nn}
    \end{bmatrix}$$
    Del teorema \ref{determinante_triangular} se sigue que
    \begin{align*}
        \Det (A - \lambda I_n) & = \begin{vmatrix}
            a_{11} - \lambda & a_{12} & \cdots & a_{1n} \\
            0 & a_{22} - \lambda & \cdots & a_{2n} \\
            \vdots & & \ddots & \\
            0 & 0 & \cdots & a_{nn} - \lambda
        \end{vmatrix} \\
        & = (a_{11} - \lambda)(a_{22} - \lambda) \cdots (a_{nn} - \lambda) \\
        & = (-1)^n (\lambda - a_{11})(\lambda - a_{22}) \cdots (\lambda - a_{nn})
    \end{align*}
    Por lo tanto, los eigenvectores son las raíces del polinomio
    $$(-1)^n (\lambda - a_{11})(\lambda - a_{22}) \cdots (\lambda - a_{nn}) = 0$$
    Es evidente que las raíces de este polinomio son $a_{11}$, $a_{22}$, $\dots$, $a_{nn}$, que coinciden exactamente con los elementos de la diagonal de la matriz $A$. La demostración para una matriz triangular inferior es prácticamente idéntica.
\end{theorem}

\newpage

\section*{Un método eficiente para encontrar los eigenvalores y eigenvectores de una matriz {\boldmath$2 \times 2$}}\label{metodo_eigen_2x2}

Calcular manualmente eigenvalores y eigenvectores de matrices puede ser una tarea difícil. Para matrices de $2 \times 2$, $3 \times 3$ y $4 \times 4$, se pueden dar fórmulas algebraicas explícitas para las soluciones. Para matrices de $5 \times 5$, ya no se puede encontrar una solución algebraica explícita, ya que habría que dar fórmulas de las raíces de un polinomio de grado $5$ y eso no es posible (vea la sección \ref{ecuacion_quinto_grado}). A continuación, presentamos la solución para el caso de $2 \times 2$. Consideremos una matriz $A$ de $2 \times 2$ con coeficientes reales o complejos dada por
$$A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}$$
Sea $T$ la traza de $A$, es decir, $T = a + d$ y sea $D$ el determinante de $A$, es decir, $D = ad - bc$. Calculemos primero el polinomio característico de $A$:
\begin{align*}
    \Det(A - \lambda I_n) & = \begin{vmatrix} a - \lambda & b \\ c & d - \lambda \end{vmatrix} \\
    & = (a - \lambda)(d - \lambda) - bc \\
    %& = ad - \lambda d - \lambda a + \lambda^2 - bc \\
    & = \lambda^2 - (a + d)\lambda + (ad - bc) \\
    & = \lambda^2 - T\lambda + D
\end{align*}
Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - T\lambda + D = 0$. Para resolver dicha ecuación, podemos usar la fórmula general. Así
\begin{align*}
    \lambda & = \frac{-(-T) \pm \sqrt{(-T)^2 - 4(1)(D)}}{2(1)} \\
    & = \frac{T \pm \sqrt{T^2 - 4D}}{2}
\end{align*}
Es decir, los eigenvalores de $A$ están dados por
$$\lambda_1 = \frac{T + \sqrt{T^2 - 4D}}{2} \quad \text{ y } \quad \lambda_2 = \frac{T - \sqrt{T^2 - 4D}}{2}$$
Para hallar los eigenvectores correspondientes, buscamos vectores no nulos que cumplan con la ecuación característica
$$(A - \lambda I_n)\mathbb{v} = \mathbb{0}$$
Esto implica resolver el sistema de ecuaciones
\begin{align*}
    (a - \lambda)x + by & = 0 \\
    cx + (d - \lambda)y & = 0
\end{align*}
Es decir, buscamos soluciones no triviales (no cero) para $x$ e $y$. Del sistema anterior, obtenemos los siguientes casos:
\begin{enumerate}[label=\roman*)]
    \item Si $c \neq 0$, podemos resolver la segunda ecuación para $x$, obteniendo
    $$x = \frac{( \lambda - d )y}{c}$$
    Si elegimos $y = c$, entonces $x = \lambda - d$, y obtenemos el eigenvector
    $$\mathbb{v} = \begin{pmatrix} \lambda - d \\ c \end{pmatrix}$$
    En particular, para $\lambda_1$ y $\lambda_2$, los eigenvectores están dados respectivamente por
    $$\mathbb{v}_1 = \begin{pmatrix} \lambda_1 - d \\ c \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} \lambda_2 - d \\ c \end{pmatrix}$$\newpage
    Para demostrar que $\mathbb{v}_1$ y $\mathbb{v}_2$ son efectivamente eigenvectores de $A$, es necesario verificar que satisfacen las siguientes ecuaciones respectivamente: $A\mathbb{v}_1 = \lambda_1\mathbb{v}_1$ y $A\mathbb{v}_2 = \lambda_2\mathbb{v}_2$. Comencemos demostrando que $\mathbb{v}_1$ es un eigenvector. Sustituyendo en la ecuación, obtenemos el siguiente sistema:
    \begin{align*}
        a(\lambda_1 - d) + bc & = \lambda_1 (\lambda_1 - d) \\
        c(\lambda_1 - d) + dc & = \lambda_1 c
    \end{align*}
    La segunda ecuación se satisface trivialmente. La primera ecuación se reduce a
    $$a\lambda_1 - ad + bc = \lambda_1^2 - d\lambda_1,$$
    que es cierta al sustituir el valor de $\lambda_1$, pues del lado izquierdo obtenemos que
    \begin{align*}
        a\lambda_1 - ad + bc & = a \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right) - ad + bc \\
        & = \frac{a^2 + ad + a\sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} - ad + bc \\
        & = \frac{a^2 + 2bc - ad + a\sqrt{a^2 - 2ad + d^2 + 4bc}}{2}
    \end{align*}
    y del lado derecho, tenemos que
    \begin{align*}
        \lambda_1^2 - d\lambda_1 & = \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right)^2 \\
        & \hspace{1.5cm} - d \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right) \\
        & = \left( \frac{a + d + \sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} \right)^2 \\
        & \hspace{1.5cm} - \frac{ad + d^2 + d\sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} \\
        & = \frac{a^2 + 2bc - ad + a\sqrt{a^2 - 2ad + d^2 + 4bc}}{2}
    \end{align*}
    En consecuencia, $\mathbb{v}_1$ es un eigenvector de $A$ correspondiente a $\lambda_1$. De manera similar, se puede demostrar que $\mathbb{v}_2$ es un eigenvector de $A$ correspondiente a $\lambda_2$.
    \item Si $b \neq 0$, podemos resolver la segunda ecuación para $x$, obteniendo
    $$y = \frac{( \lambda - a )x}{b}$$
    Si elegimos $x = b$, entonces $y = \lambda - a$, y obtenemos el eigenvector
    $$\mathbb{v} = \begin{pmatrix} b \\ \lambda - a \end{pmatrix}$$
    En particular, para $\lambda_1$ y $\lambda_2$, los eigenvectores están dados respectivamente por
    $$\mathbb{v}_1 = \begin{pmatrix} b \\ \lambda_1 - a \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} b \\ \lambda_2 - a \end{pmatrix}$$
    Siguiendo un procedimiento similar al anterior inciso, se puede demostrar que estos vectores son eigenvectores de $A$.
    \item Si $b = c = 0$, entonces la matriz es diagonal, lo que significa que sus eigenvectores son simplemente $\mathbb{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ y $\mathbb{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$.
\end{enumerate}

\infoBulle{Cuando hablamos de eigenvalores y eigenvectores, estamos tratando con propiedades especiales de las matrices cuadradas. Un eigenvalor es un número escalar que representa cómo un eigenvector se estira o contrae cuando se multiplica por la matriz. En otras palabras, si tenemos una matriz $A$ y un eigenvector $\mathbb{v}$, entonces tenemos
$$A\mathbb{v} = \lambda \mathbb{v}$$
Ahora, consideremos un escalar no nulo $k$ complejo o real, y multiplicamos el eigenvector $\mathbb{v}$ por $k$, obteniendo $k\mathbb{v}$. Al aplicar la matriz $A$ a $k\mathbb{v}$, tenemos:
$$A (k\mathbb{v}) = k(A\mathbb{v}) = k(\lambda \mathbb{v}) = \lambda (k\mathbb{v})$$
Esto demuestra que $k\mathbb{v}$ también es un eigenvector de $A$ asociado al mismo eigenvalor $\lambda$. Por lo tanto, cualquier múltiplo escalar no nulo de un eigenvector es también un eigenvector de la matriz $A$. En resumen, al multiplicar los eigenvectores de $A$, previamente obtenidos en los incisos (i) y (ii), por un escalar $k$ no nulo, podemos obtener eigenvectores más fácilmente manipulables, los cuales conservarán su condición de eigenvectores de $A$.}

\newpage

\begin{example}
    Tomemos la matriz del ejemplo \ref{ejemplo_eigenvalores_complejos}. Primero calculemos los eigenvalores, sabiendo que
    $$T = 3 - 1 = 2 \quad \text{ y } \quad D = (3)(-1) - (-5)(1) = 2$$
    Por lo tanto, los eigenvalores están dados por
    \begin{align*}
        \lambda_1 & = \frac{2 + \sqrt{(2)^2 - 4(2)}}{2} & \lambda_2 & = \frac{2 - \sqrt{(2)^2 - 4(2)}}{2} \\
        & = 1 - i & & = 1 + i
    \end{align*}
    Para calcular los eigenvectores, observemos que $b \neq 0$ y $c \neq 0$, por lo que podemos utilizar cualquiera de las dos fórmulas proporcionadas en los incisos (i) o (ii). En este caso, dado que $c = 1$, optamos por usar las fórmulas del inciso (i). De esta manera, obtenemos los eigenvectores $\mathbb{v}_1$ y $\mathbb{v}_2$ correspondientes a los eigenvalores $\lambda_1 = 1 - i$ y $\lambda_2 = 1 + i$, respectivamente están dados por:
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} (1 - i) - (-1) \\ 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} (1 + i) - (-1) \\ 1 \end{pmatrix} \\
        & = \begin{pmatrix} 2 - i \\ 1 \end{pmatrix} & & = \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} 
    \end{align*}
    que son los mismos resultados que obtuvimos en el ejemplo \ref{ejemplo_eigenvalores_complejos}.
\end{example}

\begin{example}
    Tomemos la matriz del ejemplo \ref{example_primero_eigenvalores}. Primero calculemos los eigenvalores, sabiendo que
    $$T = 1 + (-1) = 0 \quad \text{ y } \quad D = (1)(-1) - (0)(0) = - 1$$
    Por lo tanto, los eigenvalores están dados por
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(-1)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(-1)}}{2} \\
        & = 1 & & = - 1
    \end{align*}
    Ahora, en este caso observemos que $b = 0$ y $c = 0$. Por lo tanto, los eigenvalores correspondientes a $\lambda_1$ y $\lambda_2$ respectivamente están dados por
    $$\mathbb{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$$
    que son los mismos resultados que obtuvimos en el ejemplo \ref{example_primero_eigenvalores}.
\end{example}

\begin{observation}
    De acuerdo con el teorema fundamental del álgebra, cualquier polinomio de grado $n$ con coeficientes reales o complejos tiene exactamente $n$ raíces, considerando sus multiplicidades (vea el \hyperref[FUNDAMENTAL]{Apéndice C}, pág. \pageref{CONSECUENCIA1_FUNDAMENTAL}). Esto significa, por ejemplo, que el polinomio $(\lambda - 2)^5$ tiene cinco raíces, todas iguales a $2$. Dado que cualquier eigenvalor de $A$ es una raíz de la ecuación característica de $A$, se puede concluir que, contando multiplicidades, toda matriz de tamaño $n \times n$ tiene exactamente $n$ eigenvalores.
\end{observation}

\begin{example}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
    \end{bmatrix}$. \\
    \solucion Primero calculemos el polinomio característico de la matriz $A$ como sigue:
    \begin{align*}
        \Det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & 1 & 1 \\
            0 & 1 - \lambda & 1 \\
            0 & 0 & 1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)^3 \\
        & = (-1)^3 (\lambda - 1)^3
    \end{align*}\newpage\noindent
    Por lo tanto, los eigenvalores son las raíces del polinomio $(-1)^3 (\lambda - 1)^3 = 0$, cuya única raíz es $\lambda = 1$. De esta manera, la matriz $A$ tiene un único eigenvalor, $\lambda = 1$, con una multiplicidad algebraica de $3$. A continuación, determinemos el eigenvector asociado con $\lambda = 1$. Sea $\mathbb{v} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ un eigenvector de $A$. Esto significa que, para $\lambda = 1$
    $$\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        a \\
        b \\
        c
    \end{bmatrix} = \begin{bmatrix}
        a \\
        b \\
        c
    \end{bmatrix}$$
    del cual se obtiene el siguiente sistema
    \begin{align*}
        a + b + c & = a \\
        b + c & = b \\
        c & = c
    \end{align*}
    De este sistema se concluye inequívocamente que $c = 0$. Como resultado, también se deduce que $b = 0$ y, por lo tanto, $a$ puede tomar cualquier valor real. Así, el eigenvector correspondiente a $\lambda = 1$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        a \\
        0 \\
        0
    \end{pmatrix}$$
    En particular, si $a = 1$, obtenemos
    $$\mathbb{v} = \begin{pmatrix}
        1 \\
        0 \\
        0
    \end{pmatrix}$$
\end{example}

\begin{definition}
    Sea $\lambda$ un eigenvalor de una matriz $A$; entonces la multiplicidad geométrica de $\lambda$ es la dimensión del espacio característico correspondiente a $\lambda$ (que es la nulidad de la matriz $A - \lambda I_n$). Esto es, $\Dim (E_{\lambda})$.
\end{definition}

\begin{example}
    Si consideramos el ejemplo anterior, podemos observar que
    $$E_{\lambda} = \Gen \left(\left\{ \begin{pmatrix}
        1 \\
        0 \\
        0
    \end{pmatrix} \right\}\right)$$
    Por lo tanto, $\dim(E_{\lambda}) = 1$. En consecuencia, la multiplicidad geométrica del eigenvalor $\lambda = 1$ es igual a $1$.
\end{example}

\begin{example}
    Determine los eigenvalores de la matriz $A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 1
    \end{bmatrix}$. \\
    \solucion Primero calculemos el polinomio característico de la matriz $A$ usando la regla de Sarrus,
    \begin{align*}
        \Det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & 1 & 1 \\
            0 & 1 - \lambda & 1 \\
            1 & 0 & 1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)(1 - \lambda)(1 - \lambda) + (1)(1)(1) + (1)(0)(0) \\
        & \hspace{1.5cm} - (1)(1 - \lambda)(1) - (0)(1)(1 - \lambda) - (1 - \lambda)(0)(1) \\
        & = (1 - \lambda)^3 + 1 - 1 + \lambda \\
        & = 1 - 3\lambda + 3\lambda^2 - \lambda^3 + \lambda \\
        & = - \lambda^3 + 3 \lambda^2 - 2\lambda + 1
    \end{align*}\newpage\noindent
    Por lo tanto, los eigenvalores son las raíces del polinomio
    $$- \lambda^3 + 3 \lambda^2 - 2\lambda + 1 = 0$$
    o equivalentemente
    \begin{equation}
        \lambda^3 - 3 \lambda^2 + 2\lambda - 1 = 0 \label{ec:tercer_grado_eigen}
    \end{equation}
    Es fácil probar que la anterior ecuación no tiene raíces enteras ni racionales. Para resolver esta ecuación, es necesario que el lector haya visto la sección \ref{sec:B4} y la sección \ref{disc_tercergrado}. Usando la sustitución $\lambda = y + 1$, reducimos la ecuación \eqref{ec:tercer_grado_eigen} a la forma
    \begin{equation}
        y^3 - y - 1 = 0 \label{ec:tercer_grado_eigen2}
    \end{equation}
    Notemos que $p = -1$ y $q = -1$, por lo cual
    $$\frac{q^2}{4} + \frac{p^3}{27} = \frac{23}{108} > 0$$
    Entonces la ecuación
    $$y^3 - y - 1 = 0$$
    tiene una raíz real y dos raíces complejas conjugadas. Según las expresiones \eqref{APPCOSIPA1} y \eqref{APPCOSIPA2},
    $$u_1=\sqrt[3]{\frac{1}{2}+\frac{\sqrt{69}}{18}} \quad \text{ y } \quad v_1=\sqrt[3]{\frac{1}{2}-\frac{\sqrt{69}}{18}}.$$
    Entones una raíz de \eqref{ec:tercer_grado_eigen2}, es
    $$y_1 = u_1 + v_1 = \sqrt[3]{\frac{1}{2}+\frac{\sqrt{69}}{18}} + \sqrt[3]{\frac{1}{2}-\frac{\sqrt{69}}{18}}$$
    Las otras dos raíces se hallan por las fórmulas \eqref{APPCOSIPA3}. Así
    $$y_2 = -\sqrt[3]{\frac{1}{16}+\frac{\sqrt{69}}{144}}-\sqrt[3]{\frac{1}{16}-\frac{\sqrt{69}}{144}}+\frac{\sqrt[3]{12 \sqrt{3}+4 \sqrt{23}} i-\sqrt[3]{12 \sqrt{3}-4 \sqrt{23}} i}{4}$$
    y
    $$y_3 = -\sqrt[3]{\frac{1}{16}-\frac{\sqrt{69}}{144}}-\sqrt[3]{\frac{1}{16}+\frac{\sqrt{69}}{144}}+\frac{\sqrt[3]{12 \sqrt{3}-4 \sqrt{23}} i-\sqrt[3]{12 \sqrt{3}+4 \sqrt{23}} i}{4}$$
    Finalmente, revirtiendo la sustitución inicial obtenemos que las raíces de la ecuación \eqref{ec:tercer_grado_eigen} está dadas por
    $$\lambda_1 = y_1 + 1, \quad \lambda_2 = y_2 + 1, \quad \lambda_3 = y_3 + 1$$
\end{example}

\begin{observation}
    El cálculo de los eigenvalores y eigenvectores para matrices de dimensión $3 \times 3$ representa un proceso considerablemente más laborioso y complejo que el correspondiente a matrices de $2 \times 2$. En el caso de matrices $2 \times 2$, es posible llevar a cabo el procedimiento de manera relativamente directa, empleando métodos como el que se describe en la página \pageref{metodo_eigen_2x2}. Por otro lado, para matrices de $3 \times 3$, se hace necesario resolver un sistema de ecuaciones más amplio y elaborado para determinar los eigenvectores, lo cual demanda una inversión mayor de tiempo y esfuerzo. Adicionalmente, al tratar con matrices de $3 \times 3$, surge la obligación de solucionar ecuaciones polinómicas de mayor complejidad para hallar los eigenvalores, incrementando así el nivel de dificultad del proceso. La magnitud de las operaciones y la complejidad matemática inherente al cálculo de eigenvalores y eigenvectores para matrices de $3 \times 3$ convierten a esta tarea en un desafío significativamente más exigente en comparación con su análogo de menor dimensión. \newpage
    Por esta razón, en el presente texto de Álgebra Lineal, optamos por concentrarnos exclusivamente en matrices de $2 \times 2$. Esta elección está motivada por el deseo de ofrecer a los lectores una exposición clara y concisa de los conceptos esenciales, evitando sobrecargarlos con la intrincada complejidad que conllevan las matrices de dimensiones superiores. Al restringir nuestro enfoque a matrices de $2 \times 2$, permitimos que los estudiantes se enfoquen en la comprensión profunda de los principios fundamentales y en el desarrollo de habilidades robustas que les servirán de cimiento para abordar conceptos más avanzados en etapas educativas subsiguientes.
\end{observation}

\section{Matrices semejantes y diagonalización}

\begin{definition}
    Sean $A$, $B \in \mathcal{M}_{n \times n}(\RR)$. Se dice que la matriz $B$ es semejante con la matriz $A$ o que la matriz $A$ es semejante con la matriz $B$ si existe una matriz $C$ de $n \times n$ (con entradas reales o complejas) invertible tal que
    $$B = C^{-1} A C$$
\end{definition}

\begin{definition}
    Sea $T: \mathcal{M}_{n \times n}(\RR) \longrightarrow \mathcal{M}_{n \times n}(\CC)$ una transformación definida de la siguiente manera:
    \begin{align*}
        T: \mathcal{M}_{n \times n}(\RR) & \longrightarrow \mathcal{M}_{n \times n}(\CC) \\
        A & \longmapsto T(A) = C^{-1} A C = B
    \end{align*}
    A esta transformación $T$, que lleva la matriz $A$ a la matriz $B$, se le conoce como \emph{transformación de semejanza}.
\end{definition}

\begin{theorem}
    Sean $A$, $B \in \mathcal{M}_{n \times n}(\RR)$ dos matrices semejantes, entonces $A$ y $B$ tienen el mismo polinomio característico y, por consiguiente, tienen los mismos eigenvalores. \\
    \demostracion Como $A$ y $B$ son semejantes, entonces existe una matriz $C$ invertible tal que
    $$B = C^{-1}AC$$
    Así,
    \begin{align*}
        \Det(B - \lambda I_n) & = \Det\left(C^{-1}AC - \lambda I_n\right) \\
        & = \Det\left(C^{-1}AC - \lambda C^{-1}I_nC\right) \\
        & = \Det\left(C^{-1}(AC - \lambda I_nC)\right) \\
        & = \Det\left(C^{-1}(A - \lambda I_n)C\right) \\
        & = \Det\left(C^{-1}\right) \Det(A - \lambda I_n) \Det(C) \\
        & = \Det(A - \lambda I_n)
    \end{align*}
    Esto significa que $A$ y $B$ tienen la misma ecuación característica, y como los eigenvalores son raíces de la ecuación característica, tienen los mismos eigenvalores.
\end{theorem}

\begin{definition}
    Se dice que una matriz $A$ de $n \times n$ es diagonalizable si existe una matriz diagonal $D$ de $n \times n$ tal que $A$ es semejante a $D$.
\end{definition}

\begin{theorem}
    Una matriz $A$ de $n \times n$ es diagonalizable si y solo si tiene $n$ eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_n$ linealmente independientes correspondientes a los eigenvalores $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_n$. En tal caso, la matriz diagonal $D$ semejante a $A$ está dada por
    $$D = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & & \ddots & \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}$$\newpage\noindent
    Si $C$ es una matriz cuyas columnas son eigenvectores linealmente independientes de $A$, entonces
    $$D = C^{-1}AC$$
    \demostracion Sea $A$ una matriz de $n \times n$. Supongamos que $A$ tiene $n$ eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_n$ linealmente independientes correspondientes a los eigenvalores $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_n$ (no necesariamente diferentes). Sean
    $$\mathbb{v}_1 = \begin{pmatrix} c_{11} \\ c_{21} \\ \vdots \\ c_{n1} \end{pmatrix}, \mathbb{v}_2 = \begin{pmatrix} c_{12} \\ c_{22} \\ \vdots \\ c_{n2} \end{pmatrix}, \dots, \mathbb{v}_n = \begin{pmatrix} c_{1n} \\ c_{2n} \\ \vdots \\ c_{nn} \end{pmatrix}$$
    y sea\infoBulle{Si los vectores columna de una matriz de $n \times n$ son linealmente independientes, entonces la matriz tiene rango $n$, lo que significa que su determinante es distinto de cero. Ya que el determinante distinto de cero es una condición necesaria y suficiente para que una matriz sea invertible, podemos concluir que la matriz es invertible si sus vectores columna son linealmente independientes.}
    \begin{equation*}
        C = \left[\begin{array}{cccc}
            \tikzmarkin[ver=style azull]{col 1-a}c_{11} & \tikzmarkin[ver=style azull]{col 2-a}c_{12} & \cdots & \tikzmarkin[ver=style azull]{col n-a}c_{1n} \\
            c_{12} & c_{22} & \cdots & c_{2n} \\
            \vdots & & \ddots & \\
            c_{n1} \tikzmarkend{col 1-a} & c_{n2}\tikzmarkend{col 2-a} & \cdots & c_{nn}\tikzmarkend{col n-a} \\
        \end{array}\right]
        \begin{tikzpicture}[overlay, remember picture]
            \node[below=50pt of col 1-a.south west,xshift=4pt](A) {};
            \node[right=2pt of A] (B) {};
            \node[below=25pt of B] (C) {$\mathbb{v}_1$};
            \draw[-latex] (C) -- (B);

            \node[below=50pt of col 2-a.south west,xshift=4pt](D) {};
            \node[right=2pt of D] (E) {};
            \node[below=25pt of E] (F) {$\mathbb{v}_2$};
            \draw[-latex] (F) -- (E);

            \node[below=50pt of col n-a.south west,xshift=4pt](G) {};
            \node[right=2pt of G] (H) {};
            \node[below=25pt of H] (I) {$\mathbb{v}_n$};
            \draw[-latex] (I) -- (H);
        \end{tikzpicture}
    \end{equation*}
    \,\\ \,\\ \,\\ \,\\
    Entonces $C$ es invertible ya que sus columnas son linealmente independientes. Ahora bien,
    $$AC = \begin{bmatrix}
        A\mathbb{v}_1 & A\mathbb{v}_2 & \cdots & A\mathbb{v}_n
    \end{bmatrix}$$
    Pero por la definición \ref{def:eigenvalor}, se sigue que
    $$AC = \begin{bmatrix}
        \lambda_1\mathbb{v}_1 & \lambda_2\mathbb{v}_2 & \cdots & \lambda_n\mathbb{v}_n
    \end{bmatrix}$$
    Es decir,
    $$AC = \begin{bmatrix}
        \lambda_1c_{11} & \lambda_2c_{12} & \cdots & \lambda_nc_{1n} \\
        \lambda_1c_{21} & \lambda_2c_{22} & \cdots & \lambda_nc_{2n} \\
        \vdots & & \ddots & \\
        \lambda_1c_{n1} & \lambda_2c_{n2} & \cdots & \lambda_nc_{nn}
    \end{bmatrix}$$
    Pero
    \begin{align*}
        CD & = \begin{bmatrix}
            c_{11} & c_{12} & \cdots & c_{1n} \\
            c_{21} & c_{22} & \cdots & c_{2n} \\
            \vdots & & \ddots & \\
            c_{n1} & c_{n2} & \cdots & c_{nn}
        \end{bmatrix} \begin{bmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & & \ddots & \\
            0 & 0 & \cdots & \lambda_n
        \end{bmatrix} \\
        & = \begin{bmatrix}
            \lambda_1c_{11} & \lambda_2c_{12} & \cdots & \lambda_nc_{1n} \\
            \lambda_1c_{21} & \lambda_2c_{22} & \cdots & \lambda_nc_{2n} \\
            \vdots & & \ddots & \\
            \lambda_1c_{n1} & \lambda_2c_{n2} & \cdots & \lambda_nc_{nn}
        \end{bmatrix}
    \end{align*}
    Entonces
    $$AC = BD$$
    y como $C$ es invertible, se sigue que
    $$D = C^{-1}AC$$
    lo que por definición, significa que $D$ es semejante a $A$. Recíprocamente, se procede de manera similar.
\end{theorem}

\begin{theorem}
    Si una matriz $A$ de $n \times n$ tiene $n$ eigenvalores diferentes, entonces $A$ es diagonalizable. \\
    \demostracion Dado que $A$ tiene $n$ eigenvalores distintos, según el teorema \ref{eigenvalores_distintos_eigenvectoresli}, posee $n$ eigenvectores linealmente independientes. Además, con base en el teorema previo, se deduce que $A$ es diagonalizable.
\end{theorem}

\newpage

\begin{example}
    Sea $A = \begin{bmatrix*}[r]
        2 & -1 \\
        5 & -2
    \end{bmatrix*}$. ¿Es diagonalizable? Sí lo es, determine la matriz diagonal. \\
    \solucion En primer lugar, determinemos los eigenvalores de $A$. Utilizando el método expuesto en la página \pageref{metodo_eigen_2x2}, se obtiene que los eigenvalores son
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(1)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(1)}}{2} \\
        & = i & & = - i
    \end{align*}
    De acuerdo con el teorema anterior, como $\lambda_1 \neq \lambda_2$, se concluye que $A$ es diagonalizable. Ahora, determinemos los eigenvectores correspondientes a $\lambda_1 = i$ y $\lambda_2 = -i$. Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (ii), obteniendo así
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} -1 \\ i - 2 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} -1 \\ - i -2 \end{pmatrix} \\
        & = \begin{pmatrix} 1 \\ 2 - i \end{pmatrix} & & = \begin{pmatrix} 1 \\ 2 + i \end{pmatrix}
    \end{align*}
    Ahora bien, sea $C$ la matriz formada por los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$. Es decir,
    \begin{align*}
        C & = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} 1 & 1 \\ 2 - i & 2 + i \end{bmatrix}
    \end{align*}
    Ahora, determinemos $C^{-1}$ mediante la expresión \ref{Jjaksksksisid},
    \begin{align*}
        C^{-1} & = \frac{1}{2i} \begin{bmatrix*}[r]
            2 + i & - 1 \\
            -(2 - i) & 1
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \dfrac{1}{2} - i & \dfrac{1}{2}i \\[2mm]
            \dfrac{1}{2} + i & - \dfrac{1}{2}i
        \end{bmatrix*}
    \end{align*}
    Así, obtenemos
    \begin{align*}
        C^{-1}AC & = \begin{bmatrix*}[r]
            \dfrac{1}{2} - i & \dfrac{1}{2}i \\[2mm]
            \dfrac{1}{2} + i & - \dfrac{1}{2}i
        \end{bmatrix*} \begin{bmatrix*}
            2 & - 1 \\
            5 & - 2
        \end{bmatrix*} \begin{bmatrix}
            1 & 1 \\
            2 - i & 2 + i
        \end{bmatrix} \\
        & = \begin{bmatrix}
            1 + \dfrac{1}{2}i & - \dfrac{1}{2} \\[2mm]
            1 - \dfrac{1}{2}i & - \dfrac{1}{2}
        \end{bmatrix} \begin{bmatrix}
            1 & 1 \\
            2 - i & 2 + i
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            i & 0 \\
            0 & - i
        \end{bmatrix*}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvectores de $A$.
\end{example}

\section{Análisis de matrices simétricas y diagonalización ortogonal}\label{sec:matrices_sim_her_uni}

\begin{theorem}\label{theorem_simetrica1}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ una matriz simétrica. Entonces los eigenvalores de $A$ son números reales. \\
    \demostracion Sea $\lambda$ un eigenvalor de $A$ con eigenvector $\mathbb{v}$, es decir,
    $$A\mathbb{v} = \lambda \mathbb{v}$$\newpage\noindent
    En general, $\lambda \in \CC$, por lo tanto, $\mathbb{v} \in \CC[n]$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \langle \lambda \mathbb{v}, \mathbb{v} \rangle \\
        & = \lambda \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Dado que $A$ es simétrica, entonces $A = A^T$, y según el teorema \ref{JAJSJSJSJJSJSHSTTYQUUQHIAIIQVVCGTQYHHAHAHA}, se sigue que
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \left\langle \mathbb{v}, A^T \mathbb{v} \right\rangle \\
        & = \langle \mathbb{v}, A\mathbb{v} \rangle \\
        & = \langle \mathbb{v}, \lambda \mathbb{v} \rangle \\
        & = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Por lo tanto,
    $$\lambda \langle \mathbb{v}, \mathbb{v} \rangle = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle$$
    pero $\langle \mathbb{v}, \mathbb{v} \rangle = \| \mathbb{v} \| \neq 0$, ya que $\mathbb{v}$ es un eigenvector, es decir, $\mathbb{v} \neq \mathbb{0}$. Dividiendo ambos lados entre $\langle \mathbb{v}, \mathbb{v} \rangle$, obtenemos
    $$\lambda = \overline{\lambda}$$
    y según la proposición \ref{conjugado-real}, se sigue que $\lambda$ es un número real.
\end{theorem}

En el teorema \ref{eigenvalores_distintos_eigenvectoresli}, se demostró que los eigenvectores asociados a eigenvalores distintos son linealmente independientes. Sin embargo, para matrices simétricas reales, el resultado es aún más sorprendente: \emph{los eigenvectores de una matriz simétrica real correspondientes a eigenvalores distintos son ortogonales}.

\begin{theorem}\label{theorem_simetrica2}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ simétrica. Si $\lambda_1$ y $\lambda_2$ son eigenvalores distintos con eigenvectores reales correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$, entonces $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales. \\
    \demostracion Sea $\lambda_1$ y $\lambda_2$ eigenvalores de $A$ distintos correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \langle \lambda_1 \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Dado que $A$ es simétrica, entonces $A = A^T$. Por el teorema \ref{JAJSJSJSJJSJSHSTTYQUUQHIAIIQVVCGTQYHHAHAHA} y el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \left\langle \mathbb{v}_1, A^T \mathbb{v}_2 \right\rangle \\
        & = \langle \mathbb{v}_1, A\mathbb{v}_2 \rangle \\
        & = \langle \mathbb{v}_1, \lambda_2 \mathbb{v}_2 \rangle \\
        & = \overline{\lambda_2} \langle \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Por lo tanto, consideremos la siguiente expresión
    $$\lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle$$
    De esta ecuación, podemos deducir lo siguiente
    $$(\lambda_1 - \lambda_2) \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$$
    Dado que los eigenvalores $\lambda_1$ y $\lambda_2$ son distintos, podemos afirmar que $\lambda_1 \neq \lambda_2$. Por lo tanto, $\lambda_1 - \lambda_2 \neq 0$. Esto nos lleva a la conclusión de que $\langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$. Según la definición \ref{orto_prodinterno}, esto implica que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales.
\end{theorem}

\newpage

\begin{definition}
    Se dice que una matriz $A \in \mathcal{M}_{n \times n}(\RR)$ es diagonalizable ortogonalmente si existe una matriz ortogonal $Q$ tal que
    $$D = Q^{T}AQ$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$.
\end{definition}

\begin{theorem}\label{theorem_simetrica3}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$. Entonces $A$ es diagonalizable ortogonalmente si y solo si $A$ es simétrica. \\
    \demostracion Por la definición anterior, $A$ es diagonalizable ortogonalmente si y solo si existe una matriz $Q$ ortogonal tal que
    $$D = Q^{T}AQ$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$. Así
    $$D = Q^{T}AQ$$
    Al multiplicar la ecuación anterior por la izquierda por $Q$, se sigue que
    $$QD = QQ^{T}AQ$$
    Al utilizar el hecho de que $QQ^{T} = I_n$ y al multiplicar por la derecha por $Q^{T}$, obtenemos
    $$QDQ^{T} = AQQ^{T}$$
    De igual manera, al utilizar el hecho de que $QQ^{T} = I_n$, se sigue que
    $$QDQ^{T} = A$$
    Ahora bien, utilizando el teorema \ref{theo:matrixtranspu}, se sigue que
    \begin{align*}
        A^{T} & = \left( QDQ^{T} \right)^{T} \\
        & = \left( Q^{T} \right)^{T} D^{T} Q^{T}
    \end{align*}
    Utilizando el inciso (i) de la proposición \ref{propiedades_transpuesta} y el inciso (iv) de la proposición \ref{propiedades_diagonal}, obtenemos
    \begin{align*}
        A^T & = QDQ^{T} \\
        & = A
    \end{align*}
    Así, $A$ es simétrica y el teorema queda demostrado.
\end{theorem}

\begin{example}
    Consideremos la matriz $A = \begin{bmatrix*}[r]
        1 & -1 \\
        -1 & 1
    \end{bmatrix*}$, la cual es simétrica. Según el teorema \ref{theorem_simetrica1}, podemos inferir que sus eigenvalores son números reales. Además, conforme al teorema \ref{theorem_simetrica2}, los eigenvectores correspondientes a sus eigenvalores son ortogonales. Finalmente, derivado del teorema \ref{theorem_simetrica3}, concluimos que $A$ es diagonalizable ortogonalmente. Para demostrar lo anteriormente mencionado, primero calcularemos los eigenvalores de $A$. Para esto, emplearemos el método detallado en la página \pageref{metodo_eigen_2x2}. Así
    \begin{align*}
        \lambda_1 & = \frac{2 + \sqrt{(2)^2 - 4(0)}}{2} & \lambda_2 & = \frac{2 - \sqrt{(2)^2 - 4(0)}}{2} \\
        & = 2 & & = 0
    \end{align*}
    Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (i), obteniendo
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} 2 - 1 \\ - 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} 0 - 1 \\ - 1 \end{pmatrix} \\
        & = \begin{pmatrix*}[r] 1 \\ -1 \end{pmatrix*} & & = \begin{pmatrix} -1 \\ -1 \end{pmatrix}
    \end{align*}\newpage\noindent
    Para determinar la matriz $Q$, podemos emplear el teorema \ref{Qorto_vectoresorto}. Notemos que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales, ya que $\mathbb{v}_1 \bullet \mathbb{v}_2 = 0$. Por lo tanto,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \begin{pmatrix*}[r]
            \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}}
        \end{pmatrix*} & & = \begin{pmatrix}
            -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}}
        \end{pmatrix}
    \end{align*}
    Así, sea $Q = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$, entonces $Q^T = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$. Por lo tanto,
    \begin{align*}
        Q^TAQ & = \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            1 & -1 \\
            -1 & 1
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \sqrt{2} & -\sqrt{2} \\
            0 & 0
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            2 & 0 \\
            0 & 0
        \end{bmatrix}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$.
\end{example}

En esta sección se han probado resultados para matrices simétricas reales. Estos resultados se pueden extender a matrices complejas.

\begin{definition}
    Si $A = \llparenthesis a_{ij} \rrparenthesis$ es una matriz compleja, entonces la transpuesta conjugada de $A$, denotada por $A^*$, está definida por el elemento $ij$ de $A = \overline{\llparenthesis a_{ij} \rrparenthesis}$. La matriz $A$ se denomina hermitiana si $A^* = A$.
\end{definition}

\begin{example}
    Si $A = \begin{bmatrix}
        5 & 3 + 7i \\
        3 - 7i & 2
    \end{bmatrix}$, entonces $A^* = \begin{bmatrix}
        5 & 3 + 7i \\
        3 - 7i & 2
    \end{bmatrix}$. Por lo tanto, $A^* = A$ y se concluye que $A$ es una matriz hermitiana.
\end{example}

\begin{theorem}\label{inner_prod_A_Ahermitiana}
    Sean $A \in \mathcal{M}_{n \times n}(\CC)$, $\mathbb{x}$, $\mathbb{y} \in \CC[n]$. Entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left\langle \mathbb{x}, A^*\mathbb{y} \right\rangle$$
    \demostracion Consideremos el producto interno usual en $\CC[n]$, es decir,
    $$\langle \mathbb{u}, \mathbb{v} \rangle = u_1 \overline{\mathbb{v}_1} + u_2 \overline{\mathbb{v}_2} + \cdots + u_n \overline{\mathbb{v}_n}$$
    De esta forma, tenemos que si
    $$A\mathbb{x} = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots &  & \ddots & \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix} = \begin{bmatrix}
        \displaystyle\sum_{j=1}^n a_{1j}x_j \\
        \displaystyle\sum_{j=1}^n a_{2j}x_j \\
        \vdots \\
        \displaystyle\sum_{j=1}^n a_{nj}x_j
    \end{bmatrix} \quad \text{ y } \quad \mathbb{y} = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}$$\newpage\noindent
    entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left( \sum_{j=1}^n a_{1j}x_j \right) \overline{y_1} + \left( \sum_{j=1}^n a_{2j}x_j \right) \overline{y_2} + \cdots + \left( \sum_{j=1}^n a_{nj}x_j \right) \overline{y_n}$$
    lo que se traduce en
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left( \sum_{i=1}^n a_{i1} \overline{y_i} \right) x_1 + \left( \sum_{i=1}^n a_{i2} \overline{y_i} \right) x_2 + \cdots + \left( \sum_{i=1}^n a_{in} \overline{y_i} \right) x_n$$
    es decir
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \overline{\overline{\left( \sum_{i=1}^n a_{i1} \overline{y_i} \right)}} x_1 + \overline{\overline{\left( \sum_{i=1}^n a_{i2} \overline{y_i} \right)}} x_2 + \cdots + \overline{\overline{\left( \sum_{i=1}^n a_{in} \overline{y_i} \right)}} x_n$$
    entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \overline{\left( \sum_{i=1}^n \overline{a_{i1} \overline{y_i}} \right)} x_1 + \overline{\left( \sum_{i=1}^n \overline{a_{i2} \overline{y_i}} \right)} x_2 + \cdots + \overline{\left( \sum_{i=1}^n \overline{a_{in} \overline{y_i}} \right)} x_n$$
    de donde se deduce que
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = x_1 \overline{\left( \sum_{i=1}^n \overline{a_{i1}} y_i \right)} + x_2 \overline{\left( \sum_{i=1}^n \overline{a_{i2}} y_i \right)} + \cdots + x_n \overline{\left( \sum_{i=1}^n \overline{a_{in}} y_i \right)}$$
    Pero observemos que
    $$A^* \mathbb{y} = \begin{bmatrix}
        \overline{a_{11}} & \overline{a_{21}} & \cdots & \overline{a_{n1}} \\
        \overline{a_{12}} & \overline{a_{22}} & \cdots & \overline{a_{n2}} \\
        \vdots &  & \ddots & \\
        \overline{a_{1n}} & \overline{a_{2n}} & \cdots & \overline{a_{nn}}
    \end{bmatrix} \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} = \begin{bmatrix}
        \displaystyle\sum_{i=1}^n \overline{a_{i1}} y_i \\
        \displaystyle\sum_{i=1}^n \overline{a_{i2}} y_i \\
        \vdots \\
        \displaystyle\sum_{i=1}^n \overline{a_{in}} y_i
    \end{bmatrix}$$
    así que de la última expresión se hereda que
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left\langle \mathbb{x}, A^*\mathbb{y} \right\rangle$$
\end{theorem}

\begin{theorem}\label{theorem_hermitiana1}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$ una matriz hermitiana. Entonces los eigenvalores de $A$ son números reales. \\
    \demostracion Sea $\lambda$ un eigenvalor de $A$ con eigenvector $\mathbb{v}$, es decir,
    $$A\mathbb{v} = \lambda \mathbb{v}$$
    En general, $\lambda \in \CC$, por lo tanto, $\mathbb{v} \in \CC[n]$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \langle \lambda \mathbb{v}, \mathbb{v} \rangle \\
        & = \lambda \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Dado que $A$ es hermitiana, entonces $A = A^*$, y según el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \left\langle \mathbb{v}, A^* \mathbb{v} \right\rangle \\
        & = \langle \mathbb{v}, A\mathbb{v} \rangle \\
        & = \langle \mathbb{v}, \lambda \mathbb{v} \rangle \\
        & = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Por lo tanto,
    $$\lambda \langle \mathbb{v}, \mathbb{v} \rangle = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle$$\newpage\noindent
    pero $\langle \mathbb{v}, \mathbb{v} \rangle = \| \mathbb{v} \| \neq 0$, ya que $\mathbb{v}$ es un eigenvector, es decir, $\mathbb{v} \neq \mathbb{0}$. Dividiendo ambos lados entre $\langle \mathbb{v}, \mathbb{v} \rangle$, obtenemos
    $$\lambda = \overline{\lambda}$$
    y según la proposición \ref{conjugado-real}, se sigue que $\lambda$ es un número real.
\end{theorem}

\begin{theorem}\label{theorem_hermitiana2}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$ hermitiana. Si $\lambda_1$ y $\lambda_2$ son eigenvalores distintos con eigenvectores correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$, entonces $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales. \\
    \demostracion Sea $\lambda_1$ y $\lambda_2$ eigenvalores de $A$ distintos correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \langle \lambda_1 \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Dado que $A$ es hermitiana, entonces $A = A^*$. Por el teorema \ref{inner_prod_A_Ahermitiana} y el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \left\langle \mathbb{v}_1, A^* \mathbb{v}_2 \right\rangle \\
        & = \langle \mathbb{v}_1, A\mathbb{v}_2 \rangle \\
        & = \langle \mathbb{v}_1, \lambda_2 \mathbb{v}_2 \rangle \\
        & = \overline{\lambda_2} \langle \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Por lo tanto, consideremos la siguiente expresión
    $$\lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle$$
    De esta ecuación, podemos deducir lo siguiente
    $$(\lambda_1 - \lambda_2) \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$$
    Dado que los eigenvalores $\lambda_1$ y $\lambda_2$ son distintos, podemos afirmar que $\lambda_1 \neq \lambda_2$. Por lo tanto, $\lambda_1 - \lambda_2 \neq 0$. Esto nos lleva a la conclusión de que $\langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$. Según la definición \ref{orto_prodinterno}, esto implica que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales.
\end{theorem}

\begin{definition}
    Una matriz unitaria es una matriz cuadrada cuya inversa es igual a su transpuesta conjugada. Es decir, una matriz $U \in \mathcal{M}_{n \times n}(\CC)$ se llama unitaria si $U$ es invertible y $U^{-1} = U^*$. Formalmente, una matriz $U$ se considera unitaria si cumple con la siguiente propiedad:
    \[ U^* U = I_n \]
\end{definition}

\begin{theorem}\label{Uorto_vectoresorto}
    Dada $U \in \mathcal{M}_{n \times n}(\CC)$. Entonces $U$ es unitaria si y solo si sus vectores columnas son ortonormales. \\
    \demostracion Supongamos que $U$ es una matriz unitaria de $n \times n$. Entonces, por definición de matriz unitaria, $U^* U = I_n$. Dado que la entrada \( (i, j) \) de $U^*U$ es el producto interno entre la $j$-ésima columna de $U^*$ y la $i$-ésima columna de $U$, y este producto interno es igual a $0$ si $i$ y $j$ son distintos y $1$ si $i = j$ (debido a que las columnas de $U$ son ortonormales), entonces la matriz $U^* U$ es la matriz identidad, lo que implica que $U$ es unitaria. Por otro lado, si las columnas de $U$ son ortonormales, entonces la matriz $U^* U$ será la matriz identidad, ya que las entradas fuera de la diagonal serán $0$ y las entradas diagonales serán $1$ (debido a la normalización de los vectores columna). Por lo tanto, $U$ es unitaria.
\end{theorem}

\begin{definition}
    Se dice que una matriz $A \in \mathcal{M}_{n \times n}(\CC)$ es diagonalizable unitariamente si existe una matriz unitaria $U$ tal que
    $$D = U^*AU$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$.
\end{definition}

\newpage

\begin{theorem}\label{theorem_hermitiana3}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$. Entonces $A$ es diagonalizable unitariamente si y solo si $A$ es hermitiana. \\
    \demostracion Usando la demostración del teorema \ref{theorem_simetrica3}, se puede demostrar este teorema. Este hecho se propone como ejercicio al lector.
\end{theorem}

\begin{example}
    Consideremos la matriz $A = \begin{bmatrix*}[r]
        1 & i \\
        -i & -1
    \end{bmatrix*}$, la cual es hermitiana. Según el teorema \ref{theorem_hermitiana1}, podemos inferir que sus eigenvalores son números reales. Además, conforme al teorema \ref{theorem_hermitiana2}, los eigenvectores correspondientes a sus eigenvalores son ortogonales. Finalmente, derivado del teorema \ref{theorem_hermitiana3}, concluimos que $A$ es diagonalizable unitariamemte. Para demostrar lo anteriormente mencionado, primero calcularemos los eigenvalores de $A$. Para esto, emplearemos el método detallado en la página \pageref{metodo_eigen_2x2}. Así
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(-2)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(-2)}}{2} \\
        & = \sqrt{2} & & = -\sqrt{2}
    \end{align*}
    Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (ii), obteniendo
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} i \\ \sqrt{2} - 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} i \\ - \sqrt{2} - 1 \end{pmatrix}
    \end{align*}
    Para determinar la matriz $U$, podemos emplear el teorema \ref{Uorto_vectoresorto}. Notemos que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales, ya que
    \begin{align*}
        \langle \mathbb{v}_1, \mathbb{v}_2 \rangle & = (i)(-i) + (\sqrt{2} - 1)(- \sqrt{2} - 1) \\
        & = - i^2 + (- 1 + \sqrt{2})(- 1 - \sqrt{2}) \\
        & = 1 + 1 - 2 \\
        & = 0
    \end{align*}
    Por lo tanto,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \frac{1}{\sqrt{4 - 2\sqrt{2}}} \begin{pmatrix} i \\ \sqrt{2} - 1 \end{pmatrix} & & = \frac{1}{\sqrt{4 + 2\sqrt{2}}} \begin{pmatrix} i \\ - \sqrt{2} - 1 \end{pmatrix} \\
        & = \begin{pmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}}
        \end{pmatrix*} & & = \begin{pmatrix}
            \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{pmatrix}
    \end{align*}
    Así, sea
    $$U = \begin{bmatrix*}[r]
        \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
        \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*}$$
    entonces
    $$U^* = \begin{bmatrix*}[r]
        \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
        \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*}$$
    Observe que $U$, en efecto, es unitaria, pues
    $$U^*U = \begin{bmatrix*}[r]
        \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
        \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*} \begin{bmatrix*}[r]
        \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
        \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*} = \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$\newpage\noindent
    Por lo tanto,
    \begin{align*}
        U^*AU & = \begin{bmatrix*}[r]
            \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
            \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            1 & i \\
            -i & -1
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            -\dfrac{\sqrt{4 + 2\sqrt{2}}}{2}i & \dfrac{\sqrt{4 - 2\sqrt{2}}}{2} \\[3.5mm]
            \dfrac{\sqrt{4 - 2\sqrt{2}}}{2}i & \dfrac{\sqrt{4 + 2\sqrt{2}}}{2}
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \sqrt{2} & 0 \\
            0 & - \sqrt{2}
        \end{bmatrix*}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$.
\end{example}

\section{Formas bilineales y cuadráticas}

La teoría de las formas cuadráticas tiene su origen en la geometría analítica, más precisamente, en la teoría de las curvas (y superficies) de segundo orden. Es bien sabido que la ecuación de una curva central de segundo orden en el plano, después de trasladar el origen de coordenadas rectangulares al centro de esta curva, tiene la forma
\begin{equation}
    Ax^2 + 2Bxy + Cy^2 = D \label{cuadratica1}
\end{equation}
Se sabe también que se puede efectuar una rotación de los ejes coordenados en un ángulo $\theta$, o sea, un cambio de las coordenadas $x$ e $y$, por las coordenadas $x_1$ e $y_1$ donde
\begin{equation}
    \begin{aligned}
        x & = x_1 \cos \theta - y_1 \sen \theta \\
        y & = x_1 \sen \theta + y_1 \cos \theta
    \end{aligned} \label{cuadratica2}
\end{equation}
de modo que en las nuevas coordenadas la ecuación de la curva tome la forma “canónica”:
\begin{equation}
    A_1x_1^2 + C_1y_1^2 = D \label{cuadratica3}
\end{equation}
por consiguiente, en esta ecuación, el coeficiente del producto $x_1y_1$ de las indeterminadas es igual a cero. Evidentemente, la transformación de coordenadas \eqref{cuadratica2} se puede interpretar como una transformación lineal de las indeterminadas (véase el ejemplo \ref{tl:rotacion}), la cual, además, no es degenerada, puesto que el determinante de sus coeficientes es igual a la unidad. Esta transformación se aplica al primer miembro de la ecuación \eqref{cuadratica1}. Por lo tanto, se puede decir que mediante la transformación lineal no degenerada \eqref{cuadratica2}, el primer miembro de la ecuación \eqref{cuadratica1} se convierte en el primer miembro de la ecuación \eqref{cuadratica3}.


Numerosas aplicaciones reclamaron la elaboración de una teoría análoga para el caso en que el número de las indeterminadas, en lugar de dos, sea igual a cualquier $n$, y los coeficientes sean, o bien números reales, o bien números complejos cualesquiera. Generalizando la expresión que figura en el primer miembro de la ecuación \eqref{cuadratica1}, llegamos al siguiente concepto.

\section{El teorema de Cayley-Hamilton}

\newpage

\section{Ejercicios}

\noindent
De los problemas 1 al 27 calcule los eigenvalores y los espacios característicos de la matriz dada. Si la multiplicidad algebraica de un eigenvalor es mayor que $1$, calcule su multiplicidad geométrica.
\begin{tasks}[
    style=enumerate,
    label-offset = 3mm,
    %label-width = 13.97498pt,
    ](2)
    \task $\begin{bmatrix*}[r]-81 & 16 \\ -420 & 83\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-2 & -2 \\ -5 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-12 & 7 \\ -7 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]23 & 12 \\ -42 & -22\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]2 & -1 \\ 5 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-3 & 0 \\ 0 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-62 & -20 \\ 192 & 62\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]3 & 2 \\ -5 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-3 & 2 \\ 0 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-10 & -71 & -19 \\ 3 & 34 & 9 \\ -1 & -61 & -16\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}5 & 4 & 2 \\ 4 & 5 & 2 \\ 2 & 2 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]13 & 3 & 1 \\ -56 & -13 & -4 \\ -14 & -3 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & -3 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & 2 & 2 \\ 0 & 2 & 1 \\ -1 & 2 & 2\end{bmatrix*}$
    %\task $\begin{bmatrix*}[r]260 & 0 & \\ -1 & 0 & 1 \\ -1 & -2 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-2 & 5 & 0 \\ 5 & -2 & 0 \\ 0 & 0 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]7 & -2 & -4 \\ 3 & 0 & -2 \\ 6 & -2 & -3\end{bmatrix*}$
    %\task $\begin{bmatrix*}[r]-662 & 5 & \\ 0 & 3 & 0 \\ -10 & 0 & 9\end{bmatrix*}$
    \task $\begin{bmatrix*}1 & 2 & 4 \\ 0 & 2 & 3 \\ 0 & 0 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]4 & 6 & 6 \\ 1 & 3 & 2 \\ -1 & -5 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]18 & 42 & 26 & -10 \\ 22 & 70 & 37 & -17 \\ -20 & -60 & -31 & 15 \\ 62 & 186 & 104 & -44\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]4 & 1 & 0 & 1 \\ 2 & 3 & 0 & 1 \\ -2 & 1 & 2 & -3 \\ 2 & -1 & 0 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & 0 & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b \neq 0$
    \task $\begin{bmatrix*}a & 0 & 0 & 0 \\ 0 & a & b & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & c & 0 \\ 0 & 0 & a & d \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b c d \neq 0$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & c & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b c \neq 0$
    \task $\begin{bmatrix*}3 & 1 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 4 & 1 \\ 0 & 0 & 0 & 4\end{bmatrix*}$
\end{tasks}
\begin{enumerate}[start=27]
    \item Demuestre que para cualesquiera $a$, $b \in \RR$, la matriz $A=\begin{bmatrix*}[r]a & b \\ -b & a\end{bmatrix*}$ tiene eigenvalores $a \pm i b$.
\end{enumerate}
De los problemas 28 al 34 suponga que la matriz $A$ tiene eigenvalores $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$.
\begin{enumerate}[resume]
    \item Demuestre que los eigenvalores de $A^{T}$ son $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$.\newpage
    \item Demuestre que los eigenvalores de $\alpha A$ son $\alpha \lambda_{1}, \alpha \lambda_{2}, \dots, \alpha \lambda_{k}$.
    \item Demuestre que $A^{-1}$ existe si y sólo si $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k} \neq 0$.
    \item Si $A^{-1}$ existe, demuestre que los eigenvalores de $A^{-1}$ están dados por $\dfrac{1}{\lambda_{1}}, \dfrac{1}{\lambda_{2}}, \dots, \dfrac{1}{\lambda_{k}}$.
    \item Demuestre que la matriz $A-\alpha I_n$ tiene como eigenvalores a los escalares $\lambda_{1}-\alpha, \lambda_{2}-\alpha, \dots, \lambda_{k}-\alpha$.
    \item Demuestre que los eigenvalores de $A^{2}$ son $\lambda_{1}^{2}, \lambda_{2}^{2}, \dots, \lambda_{k}^{2}$.
    \item Demuestre que los eigenvalores de $A^{m}$ son $\lambda_{1}^{m}, \lambda_{2}^{m}, \dots, \lambda_{k}^{m}$ para $m=1,2,3, \dots$
    \item Sea $\lambda$ un eigenvalor de $A$ con $\mathbb{v}$ como el eigenvector correspondiente. Sea $p(\lambda)=a_{0}+a_{1} \lambda+a_{2} \lambda^{2}+\cdots+a_{n} \lambda^{n}$. Defina la matriz $p(A)$ por $p(A)=a_{0} I_n+a_{1} A+a_{2} A^{2}$ $+\cdots+a_{n} A^{n}$. Demuestre que, en efecto, $p(A) \mathbb{v}=p(\lambda) \mathbb{v}$.
    \item Utilizando el resultado del problema 37, demuestre que si $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ son eigenvalores de $A$, entonces $p\left(\lambda_{1}\right), p\left(\lambda_{2}\right), \dots, p\left(\lambda_{k}\right)$ son vectores característicos de $p(A)$.
    \item Demuestre que si $A$ es una matriz diagonal, entonces los eigenvalores de $A$ son las componentes de la diagonal de $A$.
    \item Sea $A_{1}=\begin{bmatrix*}2 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{2}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{3}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{4}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2\end{bmatrix*}$.
    Demuestre que para cada matriz $\lambda=2$ es un eigenvalor con multiplicidad algebraica $4$. En cada caso calcule la multiplicidad geométrica de $\lambda=2$.
    \item Sea $A$ una matriz real de $n \times n$. Demuestre que si $\lambda_{1}$ es un eigenvalor complejo de $A$ con eigenvector $\mathbb{v}_{1}$, entonces $\overline{\lambda_{1}}$ es un eigenvalor de $A$ con eigenvector $\overline{\mathbb{v}_{1}}$.
    \item Una matriz de probabilidad es una matriz de $n \times n$ que tiene dos propiedades:
    \begin{enumerate}
        \item $a_{i j} \geq 0$ para toda $i$ y $j$.
        \item La suma de las componentes en cada columna es $1$.
    \end{enumerate}
    Demuestre que $1$ es un eigenvalor de toda matriz de probabilidad.
    \item Sea $A=\begin{bmatrix*}a & b \\ c & d\end{bmatrix*}$ una matriz de $2 \times 2$. Suponga que $b \neq 0$. Sea $m$ una raíz (real o compleja) de la ecuación
    $$b m^{2}+(a-d) m-c=0$$
    Demuestre que $a+b m$ es un eigenvalor de $A$ con eigenvector correspondiente $\mathbb{v}=\begin{pmatrix*}1 \\ m\end{pmatrix*}$. Esto proporciona un método sencillo para calcular los valores y vectores característicos de las matrices de $2 \times 2$.
    \item Sea $A=\begin{bmatrix*}[r]a & 0 \\ c & d\end{bmatrix*}$ una matriz de $2 \times 2$. Demuestre que $d$ es un eigenvalor de $A$ con eigenvector correspondiente $\begin{pmatrix*}1 \\ 0\end{pmatrix*}$.\newpage
    \item Sea $A=\begin{bmatrix*}[r]\alpha & \beta \\ -\beta & \alpha\end{bmatrix*}$, donde $\alpha, \beta \in \RR$. Encuentre los eigenvalores de la matriz $B=A^{T} A$.
\end{enumerate}
De los problemas 44 al 53 encuentre la matriz ortogonal $Q$ que diagonaliza la matriz simétrica dada. Después verifique que $Q^{T} A Q=D$, una matriz diagonal cuyas componentes diagonales son los eigenvalores de $A$.
\begin{tasks}[
    style=enumerate,
    label-offset = 3mm,
    start=44,
    %label-width = 13.97498pt,
    ](2)
    %\task $\begin{bmatrix*}3 & 2 \\ 2 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]3 & 4 \\ 4 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}2 & 1 \\ 1 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-4 & 2 \\ 2 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 \\ -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & -1 \\ -1 & 1 & -1 \\ -1 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]20 & 16 & -4 \\ 16 & 32 & 16 \\ -4 & 16 & 20\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-1 & 2 & 2 \\ 2 & -1 & 2 \\ 2 & 2 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$
\end{tasks}
\begin{enumerate}[start=54]
    \item Sea $Q$ una matriz ortogonal simétrica. Demuestre que si $\lambda$ es un eigenvalor de $Q$, entonces $\lambda= \pm 1$.
    \item La matriz $A$ es ortogonalmente semejante a la matriz $B$ si existe una matriz ortogonal $Q$ tal que $B=Q^{T} A Q$. Suponga que $A$ es ortogonalmente semejante a $B$ y que $B$ es ortogonalmente semejante a $C$. Demuestre que $A$ es ortogonalmente semejante a $C$.
    \item Demuestre que si $Q=\begin{bmatrix*}a & b \\ c & d\end{bmatrix*}$ es ortogonal, entonces $b= \pm c$.
    \item Suponga que $A$ es una matriz simétrica real para la que todos sus eigenvalores son cero. Demuestre que $A$ es la matriz cero.
    \item Demuestre que si una matriz real $A$ de $2 \times 2$ tiene eigenvectores ortogonales, entonces $A$ es simétrica.
    \item Sea $A$ una matriz real antisimétrica. Demuestre que todo eigenvalor de $A$ es de la forma $i \alpha$, donde $\alpha \in \RR$ e $i=\sqrt{-1}$. Es decir, demuestre que todo eigenvalor de $A$ es un número imaginario.
    \item Demuestre que los eigenvalores de una matriz hermitiana compleja de $n \times n$ son reales.
    \item Si $A$ es una matriz hermitiana de $n \times n$, demuestre que los eigenvectores correspondientes a eigenvalores distintos son ortogonales.
    \item Repitiendo la demostración del teorema \ref{theorem_simetrica2}, pero sustituyendo $\overline{\mathbb{v}}_i^{t}$ por $\mathbb{v}_i^{t}$ donde sea adecuado, demuestre que cualquier matriz hermitiana de $n \times n$ tiene $n$ eigenvectores ortonormales.
    \item Encuentre una matriz unitaria $U$ tal que $U^* A U$ es diagonal, donde $A=\begin{bmatrix*}0 & 3-2 i \\ 3+2 i & 0\end{bmatrix*}$.
    \item Haga lo mismo que en el problema anterior para $A=\begin{bmatrix*}-2 & -3+2 i \\ -3-4 i & 2\end{bmatrix*}$.
    \item Demuestre que el determinante de una matriz hermitiana es real.
\end{enumerate}