\chapter{DIAGONALIZACIÓN Y FORMAS BILINEALES}\label{chapter:bilineal}
%\startcontents
\printchaptertableofcontents

\section{Eigenvalores y eigenvectores}

%Los conceptos de eigenvalores y eigenvectores son fundamentales para entender las transformaciones lineales y sus efectos sobre el espacio vectorial.
Dada una transformación lineal $T: V \longrightarrow W$, en diversas aplicaciones  resulta útil encontrar un vector $\mathbb{v}$ en $V$ tal que $T\mathbb{v}$ y $\mathbb{v}$ son paralelos. Es decir, se busca un vector $\mathbb{v}$ no nulo y un escalar $\lambda$ tal que
$$T\mathbb{v} = \lambda \mathbb{v}$$
Al escalar $\lambda$ se le denomina \emph{eigenvalor} asociado al \emph{eigenvector} $\mathbb{v}$. Es importante destacar que cada eigenvector puede tener asociado más de un eigenvalor.

\begin{definition}\label{def:eigenvalor}
    Sea $A$ una matriz de $n \times n$ con componentes reales y $\lambda$ un número real o complejo. Se dice que $\lambda$ es un eigenvalor de la matriz $A$, si existe $\mathbb{v} \in \CC[n]$ no nulo tal que
    $$A\mathbb{v} = \lambda \mathbb{v}$$
    A $\mathbb{v} \neq \mathbb{0}$ se le denomina eigenvector de $A$ correspondiente al eigenvalor $\lambda$.
\end{definition}

Hemos visto la definición de un eigenvalor y eigenvector, pero ¿cómo podemos calcular estos mismos? Observemos que para calcular los eigenvalores y eigenvectores de una matriz cuadrada, necesitamos encontrar los valores de $\lambda$ que permiten que la ecuación $A \mathbb{v} = \lambda \mathbb{v}$ se cumpla para un vector no nulo $\mathbb{v}$. Esto se puede hacer reescribiendo la ecuación como
$$A \mathbb{v} - \lambda \mathbb{v} = \mathbb{0}$$

\infoBulle{Los eigenvalores y eigenvectores son comúnmente conocidos como valores y vectores propios, respectivamente, o valores y vectores característicos. El término alemán \emph{eigen} significa “propio”.}
\infoBulle{La definición \ref{def:eigenvalor} es válida incluso si $A$ tiene componentes complejas. Sin embargo, dado que las matrices que estamos considerando principalmente tienen componentes reales, la definición es adecuada para nuestros propósitos.}

\newpage
\infoBulle{Como se mostrará más adelante, incluso una matriz con elementos reales puede poseer eigenvalores y eigenvectores complejos. Por esta razón, en la definición \ref{def:eigenvalor}, se especifica que el eigenvector tiene entradas complejas, es decir, $\mathbb{v} \in \CC[n]$. Aunque no se abordarán exhaustivamente muchos aspectos relacionados con los números complejos en esta obra, se incluye una exposición básica de los conceptos indispensables en el \hyperref[chap:numeros-complejos]{Apéndice B}.}

\noindent o bien,
$$(A - \lambda I_n) \mathbb{v} = \mathbb{0}$$
donde $I_n$ es la matriz identidad del mismo tamaño que la matriz $A$. Esto lleva a buscar valores de $\lambda$ que hagan que $A - \lambda I_n$ sea singular. Una vez que se encuentran estos eigenvalores, se puede calcular el eigenvector correspondiente resolviendo el sistema de ecuaciones lineales asociado.

\begin{theorem}
    Sea $A$ una matriz de $n \times n$. Entonces $\lambda$ es un eigenvalor de $A$ si y solo si
    $$p(\lambda) = \det(A - \lambda I_n) = 0$$
    A esta expresión se le denomina ecuación característica de $A$, y la función
    $$p(\lambda) = \det(A - \lambda I_n)$$
    se conoce como el polinomio característico de $A$. \\
    \demostracion Si $A$ tiene un eigenvalor $\lambda$ correspondiente a un eigenvector $\mathbb{v}$, entonces, por definición, $A \mathbb{v} = \lambda \mathbb{v}$ o equivalentemente
    $$A \mathbb{v} - \lambda \mathbb{v} = \mathbb{0}$$
    Es decir,
    $$(A - \lambda I_n) \mathbb{v} = \mathbb{0}$$
    Dado que $\mathbb{v} \neq \mathbb{0}$, de la expresión anterior se deduce que $A - \lambda I_n$ es singular. Por lo tanto, los eigenvalores de $A$ son solo aquellos números $\lambda$ reales o complejos para los cuales $A - \lambda I_n$ son singulares. Así, del teorema \ref{singular_determinante} se hereda que $A - \lambda I_n$ es singular si
    $$\det(A - \lambda I_n) = 0$$
    Recíprocamente, supongamos que $\det(A - \lambda I_n) = 0$. Entonces, de nuevo por el teorema \ref{singular_determinante}, $A - \lambda I_n$ no es invertible. Luego existe $\mathbb{v} \neq \mathbb{0}$ tal que $(A - \lambda I_n) \mathbb{v} = \mathbb{0}$ y claramente $A \mathbb{v} = \lambda \mathbb{v}$. Por lo tanto, $\mathbb{v}$ es un eigenvector (con $\lambda$ como eigenvalor asociado) de $A$.
\end{theorem}

\begin{example}\label{example_primero_eigenvalores}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*}$. \\
    \solucion Usando el teorema anterior, tenemos
    \begin{align*}
        \det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & \phantom{-} 0 \\
            0 & -1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)(-1 - \lambda) \\
        & = (\lambda - 1)(\lambda + 1) \\
        & = \lambda^2 - 1
    \end{align*}
    Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - 1 = 0$. De esta forma, los eigenvalores de $A$ son $\lambda_1 = 1$ y $\lambda_2 = -1$. Ahora, determinemos los eigenvectores de $A$. Sea $\mathbb{v}_1 = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda_1 = 1$
    $$A \mathbb{v}_1 = \lambda_1 \mathbb{v}_1$$
    entonces
    $$\begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = 1 \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    y, por ende
    \begin{align*}
        a & = a \\
        - b & = b
    \end{align*}\newpage\noindent
    Observemos que $b = 0$ y $a$ puede ser cualquier número real distinto de $0$. Así, el eigenvector $\mathbb{v}_1$ correspondiente a $\lambda_1 = 1$ está dado por
    $$\mathbb{v}_1 = \begin{pmatrix}
        a \\
        0
    \end{pmatrix}$$
    En particular,
    $$\mathbb{v}_1 = \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}$$
    De manera análoga, sea $\mathbb{v}_2 = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda_2 = -1$
    $$A \mathbb{v}_2 = \lambda_2 \mathbb{v}_2$$
    entonces
    $$\begin{bmatrix*}[r]
        1 & 0 \\
        0 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = - 1 \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    y, por ende
    \begin{align*}
        a & = - a \\
        b & = b
    \end{align*}
    Observemos que $a = 0$ y $b$ puede ser cualquier número real distinto de $0$. Así, el eigenvector $\mathbb{v}_2$ correspondiente a $\lambda_2 = -1$ está dado por
    $$\mathbb{v}_2 = \begin{pmatrix}
        0 \\
        b
    \end{pmatrix}$$
    En particular,
    $$\mathbb{v}_2 = \begin{pmatrix}
        0 \\
        1
    \end{pmatrix}$$
\end{example}

\begin{example}\label{ejemplo_eigenvalores_complejos}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*}$. \\
    \solucion Usando el teorema anterior, tenemos
    \begin{align*}
        \det(A - \lambda I_n) & = \begin{vmatrix}
            3 - \lambda & \phantom{-} -5 \\
            1 & -1 - \lambda
        \end{vmatrix} \\
        & = (3 - \lambda)(-1 - \lambda) + 5 \\
        & = \lambda^2 - 2\lambda + 2
    \end{align*}
    Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - 2\lambda + 2 = 0$. De esta forma, los eigenvalores de $A$ son $\lambda = 1 + i$ y $\overline{\lambda} = 1 - i$. Ahora, determinemos los eigenvectores de $A$. Sea $\mathbb{v} = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\lambda = 1 + i$
    $$A \mathbb{v} = \lambda \mathbb{v}$$
    entonces
    $$\begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = (1 + i) \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    es decir
    \begin{align*}
        3a - 5b & = (1 + i)a \\
        a - b & = (1 + i)b
    \end{align*}
    lo cual se traduce en
    \begin{align*}
        3a - (1 + i)a - 5b & = 0 \\
        a - b - (1 + i)b & = 0
    \end{align*}\newpage\noindent
    y, por ende,
    \begin{align*}
        (2 - i)a - 5b & = 0 \\
        a - (2 + i)b & = 0
    \end{align*}
    Observemos que la segunda ecuación es igual a la primera, pues multiplicando la segunda ecuación por $2 - i$, se sigue que
    $$(2 - i)a - (2 + i)(2 - i)b = 0$$
    y se obtiene
    $$(2 - i)a - 5b = 0$$
    que es la primer ecuación. Así que basta resolver esta ecuación para encontrar el eigenvector deseado. De la segunda ecuación del sistema, se sigue que $a = (2 + i)b$, por lo que
    $$\mathbb{v} = \begin{pmatrix}
        (2 + i)b \\
        b
    \end{pmatrix}$$
    En particular, el eigenvector $\mathbb{v}$ correspondiente a $\lambda = 1 + i$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        2 + i \\
        1
    \end{pmatrix}$$
    De manera análoga, sea $\mathbb{v} = \begin{pmatrix} a \\ b \end{pmatrix}$ un eigenvector de $A$, esto significa que para $\overline{\lambda} = 1 - i$
    $$A \mathbb{v} = \overline{\lambda} \mathbb{v}$$
    entonces
    $$\begin{bmatrix*}[r]
        3 & -5 \\
        1 & -1
    \end{bmatrix*} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = (1 - i) \begin{bmatrix}
        a \\
        b
    \end{bmatrix}$$
    es decir
    \begin{align*}
        3a - 5b & = (1 - i)a \\
        a - b & = (1 - i)b
    \end{align*}
    lo cual se traduce en
    \begin{align*}
        3a - (1 - i)a - 5b & = 0 \\
        a - b - (1 - i)b & = 0
    \end{align*}
    y, por ende,
    \begin{align*}
        (2 + i)a - 5b & = 0 \\
        a - (2 - i)b & = 0
    \end{align*}
    Observemos que la segunda ecuación es igual a la primera, pues multiplicando la segunda ecuación por $2 + i$, se sigue que
    $$(2 + i)a - (2 + i)(2 - i)b = 0$$
    y se obtiene
    $$(2 + i)a - 5b = 0$$
    que es la primer ecuación. Así que basta resolver esta ecuación para encontrar el eigenvector deseado. De la segunda ecuación del sistema, se sigue que $a = (2 - i)b$, por lo que
    $$\mathbb{v} = \begin{pmatrix}
        (2 - i)b \\
        b
    \end{pmatrix}$$
    En particular, el eigenvector $\mathbb{v}$ correspondiente a $\overline{\lambda} = 1 - i$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        2 - i \\
        1
    \end{pmatrix}$$
    Observemos que este vector tiene como entradas el conjugado del primer eigenvector que obtuvimos, por lo que podemos denotarlo como $\overline{\mathbb{v}}$.
\end{example}

\newpage

\begin{observation}
    Los eigenvalores de una matriz real ocurren en pares de números complejos conjugados, y los eigenvectores correspondientes son también complejos conjugados entre sí.
\end{observation}

\begin{definition}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y $\lambda$ un eigenvalor de $A$. Definimos el eigenespacio o espacio propio de la matriz $A$ correspondiente a $\lambda$ como
    $$E_{\lambda} = \left\{ \mathbb{v} \in \CC[n] \mid A \mathbb{v} = \lambda \mathbb{v} \right\}$$
\end{definition}

\begin{theorem}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y $\lambda$ un eigenvalor de $A$. Entonces $E_{\lambda}$ es un subespacio de $\CC[n]$. \\
    \demostracion Para probar que $E_{\lambda}$ es subespacio de $\CC[n]$, debemos probar que $E_{\lambda}$ cumple los dos axiomas de cerradura. Sean $\mathbb{v}_1$, $\mathbb{v}_2 \in E_{\lambda}$, entonces
    $$A \mathbb{v}_1 = \lambda \mathbb{v}_1$$
    y
    $$A \mathbb{v}_2 = \lambda \mathbb{v}_2$$
    Así
    \begin{align*}
        A(\mathbb{v}_1 + \mathbb{v}_2) & = A\mathbb{v}_1 + A\mathbb{v}_2 \\
        & = \lambda \mathbb{v}_1 + \lambda \mathbb{v}_2 \\
        & = \lambda (\mathbb{v}_1 + \mathbb{v}_2)
    \end{align*}
    Por lo tanto, $A(\mathbb{v}_1 + \mathbb{v}_2) = \lambda (\mathbb{v}_1 + \mathbb{v}_2)$ de donde se sigue que $\mathbb{v}_1 + \mathbb{v}_2 \in E_{\lambda}$. De manera análoga, sean $\mathbb{v} \in E_{\lambda}$ y $\alpha \in \CC$, entonces
    \begin{align*}
        A(\alpha \mathbb{v}) & = \alpha (A \mathbb{v}) \\
        & = \alpha (\lambda \mathbb{v}) \\
        & = \lambda \alpha \mathbb{v} \\
        & = \lambda (\alpha \mathbb{v})
    \end{align*}
    En consecuencia, $\alpha \mathbb{v} \in E_{\lambda}$. Por lo tanto, dado que se cumplen ambas propiedades de cerradura, se concluye que $E_{\lambda}$ es un subespacio de $\CC[n]$.
\end{theorem}

\begin{theorem}\label{eigenvalores_distintos_eigenvectoresli}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ y sean $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_k$ eigenvalores distintos entre sí de $A$ correspondientes a los eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_k$. Entonces el conjunto de vectores $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_k \right\}$ es linealmente independiente. \\
    \demostracion Procedamos por inducción sobre $k$. Es evidente que, para $k = 1$, obtenemos un eigenvalor $\lambda_1$ asociado con un eigenvector $\mathbb{v}_1$, lo que da lugar a un único vector que es linealmente independiente. Si $k = 2$, entonces tenemos dos eigenvalores $\lambda_1$ y $\lambda_2$ asociados con los eigenvectores $\mathbb{v}_1$ y $\mathbb{v}_2$ respectivamente. Demostremos que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2 \right\}$ es, en efecto, linealmente independiente. Por definición,
    \begin{equation}
        a\mathbb{v}_1 + b\mathbb{v}_2 = \mathbb{0}, \text{ con } a, b \in \CC \label{JAJAJASGQQTAUSOSOQIAOAOA}
    \end{equation}
    así que debemos probar que $a = 0 = b$. Para ello, de la anterior expresión si multiplicamos por la matriz $A$, entonces
    $$A \left( a\mathbb{v}_1 + b\mathbb{v}_2 \right) = \mathbb{0}$$
    lo que se traduce en
    $$A(a\mathbb{v}_1) + A(b\mathbb{v}_2) = \mathbb{0}$$
    y, por ende
    $$aA\mathbb{v}_1 + bA\mathbb{v}_2 = \mathbb{0}$$
    entonces
    \begin{equation}
        a \lambda_1 \mathbb{v}_1 + b \lambda_2 \mathbb{v}_2 = \mathbb{0} \label{JAJAJSBDJJDJDJDJDJD}
    \end{equation}
    Ahora, multiplicando por $\lambda_1$ la ecuación \eqref{JAJAJASGQQTAUSOSOQIAOAOA}, obtenemos
    \begin{equation}
        a \lambda_1 \mathbb{v}_1 + b \lambda_1 \mathbb{v}_2 = \mathbb{0} \label{JAJAJQQUWJSJSISIS}
    \end{equation}
    Al restar la ecuación \eqref{JAJAJQQUWJSJSISIS} de la ecuación \eqref{JAJAJSBDJJDJDJDJDJD}, se sigue que
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        a\lambda_1\mathbb{v}_1 + b\lambda_2\mathbb{v}_2 = \mathbb{0} \\
        a\lambda_1\mathbb{v}_1 + b\lambda_1\mathbb{v}_2 = \mathbb{0} \\
        \hline
        b (\lambda_2 - \lambda_1) \mathbb{v}_2 = \mathbb{0}
    \end{array}}$$
    Sin embargo, según la hipótesis, se cumple que $\lambda_2 - \lambda_1 \neq 0$, ya que los eigenvalores de $A$ son diferentes entre sí. Además, dado que $\mathbb{v}_2$ es un eigenvector, se tiene que, por definición, $\mathbb{v}_2 \neq \mathbb{0}$; lo que implica que necesariamente $b = 0$. Al sustituir el valor de $b$ en la ecuación \eqref{JAJAJASGQQTAUSOSOQIAOAOA}, se obtiene que $a\mathbb{v}_1 = \mathbb{0}$. Dado que $\mathbb{v}_1$ es un eigenvector, se cumple por definición que $\mathbb{v}_1 \neq \mathbb{0}$, lo que implica que $a = 0$. Esto prueba que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2 \right\}$ es, en efecto, linealmente independiente. Supongamos que el teorema se cumple para $k = l$ eigenvectores, es decir, supongamos que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_l \right\}$ es linealmente independiente. Entonces debemos probar el teorema para $k = l + 1$ eigenvectores, es decir, debemos probar que el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_{l + 1} \right\}$ es linealmente independiente. Por definición,
    \begin{equation}
        \alpha_1 \mathbb{v}_1 + \alpha_2 \mathbb{v}_2 + \cdots + \alpha_{l} \mathbb{v}_{l} + \alpha_{l + 1} \mathbb{v}_{l + 1} = \mathbb{0} \label{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO}
    \end{equation}
    con $\alpha_i \in \CC$. Así que debemos probar que $\alpha_i = 0$ y para ello, de la anterior expresión si la multiplicamos por la matriz $A$, entonces
    $$A \left( \alpha_1 \mathbb{v}_1 + \alpha_2 \mathbb{v}_2 + \cdots + \alpha_{l} \mathbb{v}_{l} + \alpha_{l + 1} \mathbb{v}_{l + 1} \right) = \mathbb{0}$$
    lo que de traduce en
    $$A(\alpha_1\mathbb{v}_1) + A(\alpha_2\mathbb{v}_2) + \cdots + A(\alpha_l\mathbb{v}_l)+ A(\alpha_{l+1}\mathbb{v}_{l+1}) = \mathbb{0}$$
    y, por ende
    $$\alpha_1A\mathbb{v}_1 + \alpha_2A\mathbb{v}_2 + \cdots + \alpha_lA\mathbb{v}_l + \alpha_{l+1}A\mathbb{v}_{l+1} = \mathbb{0}$$
    entonces
    \begin{equation}
        \alpha_1\lambda_1\mathbb{v}_1 + \alpha_2\lambda_2\mathbb{v}_2 + \cdots + \alpha_l\lambda_l\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \label{IAJQUUQJQVQHQHAHHAHAHAVACACAVA}
    \end{equation}
    Ahora, multiplicando $\lambda_{l+1}$ la ecuación \eqref{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO}, obtenemos
    \begin{equation}
        \alpha_1\lambda_{l+1}\mathbb{v}_1 + \alpha_2\lambda_{l+1}\mathbb{v}_2 + \cdots + \alpha_l\lambda_{l+1}\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \label{AAOSIKQKWJWHGWHWVSVS}
    \end{equation}
    Al restar la ecuación \eqref{AAOSIKQKWJWHGWHWVSVS} de la ecuación \eqref{IAJQUUQJQVQHQHAHHAHAHAVACACAVA}, se sigue que
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        \alpha_1\lambda_1\mathbb{v}_1 + \alpha_2\lambda_2\mathbb{v}_2 + \cdots + \alpha_l\lambda_l\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \\
        \alpha_1\lambda_{l+1}\mathbb{v}_1 + \alpha_2\lambda_{l+1}\mathbb{v}_2 + \cdots + \alpha_l\lambda_{l+1}\mathbb{v}_l + \alpha_{l+1}\lambda_{l+1}\mathbb{v}_{l+1} = \mathbb{0} \\
        \hline
        \alpha_1(\lambda_1 - \lambda_{l+1})\mathbb{v}_1 + \alpha_2(\lambda_2 - \lambda_{l+1})\mathbb{v}_2 + \cdots + \alpha_l(\lambda_l - \lambda_{l+1})\mathbb{v}_l = \mathbb{0}
    \end{array}}$$
    Pero de acuerdo con la suposición de inducción, el conjunto $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_l \right\}$ es linealmente independiente. Así,
    $$\alpha_1(\lambda_1 - \lambda_{l+1}) = \alpha_2(\lambda_2 - \lambda_{l+1}) = \cdots = \alpha_l(\lambda_l - \lambda_{l+1}) = 0$$
    y como los eigenvalores de $A$ son diferentes entre sí, se concluye que
    $$\alpha_1 = \alpha_2 = \cdots = \alpha_l = 0$$
    Pero, de la expresión \eqref{JAJSJSBBAJQHQJQJQVCQFQTQIOAOAO} se sigue que $\alpha_{l+1} = 0$. Por lo tanto, el teorema se cumple para $k = l + 1$. Por lo tanto, dados $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_k$ eigenvalores distintos entre sí de $A$ correspondientes a los eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_k$, el conjunto de vectores $\left\{ \mathbb{v}_1, \mathbb{v}_2, \dots, \mathbb{v}_k \right\}$ es linealmente independiente.
\end{theorem}

\newpage

\begin{example}
    En el ejemplo \ref{ejemplo_eigenvalores_complejos}, se determinó que $\lambda = 1 + i$ y $\overline{\lambda} = 1 - i$ son eigenvalores de la matriz $A$ y, además, son distintos entre sí. En ese mismo ejemplo, se calcularon los eigenvectores asociados a $\lambda$ y $\overline{\lambda}$, que son
    $$\mathbb{v} = \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} \quad \text{ y } \quad \overline{\mathbb{v}} = \begin{pmatrix} 2 - i \\ 1 \end{pmatrix}$$
    respectivamente. Demostremos que estos vectores son linealmente independientes, es decir, demostremos que no existen $\alpha_1$, $\alpha_2 \in \CC$ no nulos tales que
    $$\alpha_1 \mathbb{v} + \alpha_2 \overline{\mathbb{v}} = \mathbb{0}$$
    Sustituyendo los valores de $\mathbb{v}$ y $\overline{\mathbb{v}}$, se sigue que
    $$\alpha_1 \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 2 - i \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$
    Del cual, obtenemos el siguiente sistema
    \begin{align*}
        \alpha_1 (2 + i) + \alpha_2 (2 - i) & = 0 \\
        \alpha_1 + \alpha_2 & = 0
    \end{align*}
    Al multiplicar la segunda ecuación por $2 + i$ y restarla de la primera ecuación, se obtiene lo siguiente:
    $$- {\extrarowheight = 0.5ex
    \begin{array}{r}
        \alpha_1 (2 + i) + \alpha_2 (2 - i) = 0 \\
        \alpha_1(2 + i) + \alpha_2(2 + i) = 0 \\
        \hline
        \alpha_2 [(2 - i) - (2 + i)] = 0
    \end{array}}$$
    Por lo tanto, $\alpha_2(-2i) = 0$, de donde se deduce que $\alpha_2 = 0$. Al sustituir el valor de $\alpha_2$ en la primera ecuación, se concluye que $\alpha_1 = 0$. De esta forma, se demuestra que $\mathbb{v}$ y $\overline{\mathbb{v}}$ son linealmente independientes. Además, este ejemplo particular ilustra la validez del teorema anterior.
\end{example}

\begin{theorem}
    Los eigenvalores de una matriz triangular superior o inferior son exactamente los elementos de la diagonal de la matriz. \\
    \demostracion Sea $A$ una matriz de $n \times n$ triangular superior como sigue:
    $$A = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        0 & a_{22} & \cdots & a_{2n} \\
        \vdots & & \ddots & \\
        0 & 0 & \cdots & a_{nn}
    \end{bmatrix}$$
    Del teorema \ref{determinante_triangular} se sigue que
    \begin{align*}
        \Det (A - \lambda I_n) & = \begin{vmatrix}
            a_{11} - \lambda & a_{12} & \cdots & a_{1n} \\
            0 & a_{22} - \lambda & \cdots & a_{2n} \\
            \vdots & & \ddots & \\
            0 & 0 & \cdots & a_{nn} - \lambda
        \end{vmatrix} \\
        & = (a_{11} - \lambda)(a_{22} - \lambda) \cdots (a_{nn} - \lambda) \\
        & = (-1)^n (\lambda - a_{11})(\lambda - a_{22}) \cdots (\lambda - a_{nn})
    \end{align*}
    Por lo tanto, los eigenvectores son las raíces del polinomio
    $$(-1)^n (\lambda - a_{11})(\lambda - a_{22}) \cdots (\lambda - a_{nn}) = 0$$
    Es evidente que las raíces de este polinomio son $a_{11}$, $a_{22}$, $\dots$, $a_{nn}$, que coinciden exactamente con los elementos de la diagonal de la matriz $A$. La demostración para una matriz triangular inferior es prácticamente idéntica.
\end{theorem}

\newpage

\section*{Un método eficiente para encontrar los eigenvalores y eigenvectores de una matriz {\boldmath$2 \times 2$}}\label{metodo_eigen_2x2}

Calcular manualmente eigenvalores y eigenvectores de matrices puede ser una tarea difícil. Para matrices de $2 \times 2$, $3 \times 3$ y $4 \times 4$, se pueden dar fórmulas algebraicas explícitas para las soluciones. Para matrices de $5 \times 5$, ya no se puede encontrar una solución algebraica explícita, ya que habría que dar fórmulas de las raíces de un polinomio de grado $5$ y eso no es posible (vea la sección \ref{ecuacion_quinto_grado}). A continuación, presentamos la solución para el caso de $2 \times 2$. Consideremos una matriz $A$ de $2 \times 2$ con coeficientes reales o complejos dada por
$$A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}$$
Sea $T$ la traza de $A$, es decir, $T = a + d$ y sea $D$ el determinante de $A$, es decir, $D = ad - bc$. Calculemos primero el polinomio característico de $A$:
\begin{align*}
    \Det(A - \lambda I_n) & = \begin{vmatrix} a - \lambda & b \\ c & d - \lambda \end{vmatrix} \\
    & = (a - \lambda)(d - \lambda) - bc \\
    %& = ad - \lambda d - \lambda a + \lambda^2 - bc \\
    & = \lambda^2 - (a + d)\lambda + (ad - bc) \\
    & = \lambda^2 - T\lambda + D
\end{align*}
Por lo tanto, los eigenvalores son las raíces del polinomio $\lambda^2 - T\lambda + D = 0$. Para resolver dicha ecuación, podemos usar la fórmula general. Así
\begin{align*}
    \lambda & = \frac{-(-T) \pm \sqrt{(-T)^2 - 4(1)(D)}}{2(1)} \\
    & = \frac{T \pm \sqrt{T^2 - 4D}}{2}
\end{align*}
Es decir, los eigenvalores de $A$ están dados por
$$\lambda_1 = \frac{T + \sqrt{T^2 - 4D}}{2} \quad \text{ y } \quad \lambda_2 = \frac{T - \sqrt{T^2 - 4D}}{2}$$
Para hallar los eigenvectores correspondientes, buscamos vectores no nulos que cumplan con la ecuación característica
$$(A - \lambda I_n)\mathbb{v} = \mathbb{0}$$
Esto implica resolver el sistema de ecuaciones
\begin{align*}
    (a - \lambda)x + by & = 0 \\
    cx + (d - \lambda)y & = 0
\end{align*}
Es decir, buscamos soluciones no triviales (no cero) para $x$ e $y$. Del sistema anterior, obtenemos los siguientes casos:
\begin{enumerate}[label=\roman*)]
    \item Si $c \neq 0$, podemos resolver la segunda ecuación para $x$, obteniendo
    $$x = \frac{( \lambda - d )y}{c}$$
    Si elegimos $y = c$, entonces $x = \lambda - d$, y obtenemos el eigenvector
    $$\mathbb{v} = \begin{pmatrix} \lambda - d \\ c \end{pmatrix}$$
    En particular, para $\lambda_1$ y $\lambda_2$, los eigenvectores están dados respectivamente por
    $$\mathbb{v}_1 = \begin{pmatrix} \lambda_1 - d \\ c \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} \lambda_2 - d \\ c \end{pmatrix}$$\newpage
    Para demostrar que $\mathbb{v}_1$ y $\mathbb{v}_2$ son efectivamente eigenvectores de $A$, es necesario verificar que satisfacen las siguientes ecuaciones respectivamente: $A\mathbb{v}_1 = \lambda_1\mathbb{v}_1$ y $A\mathbb{v}_2 = \lambda_2\mathbb{v}_2$. Comencemos demostrando que $\mathbb{v}_1$ es un eigenvector. Sustituyendo en la ecuación, obtenemos el siguiente sistema:
    \begin{align*}
        a(\lambda_1 - d) + bc & = \lambda_1 (\lambda_1 - d) \\
        c(\lambda_1 - d) + dc & = \lambda_1 c
    \end{align*}
    La segunda ecuación se satisface trivialmente. La primera ecuación se reduce a
    $$a\lambda_1 - ad + bc = \lambda_1^2 - d\lambda_1,$$
    que es cierta al sustituir el valor de $\lambda_1$, pues del lado izquierdo obtenemos que
    \begin{align*}
        a\lambda_1 - ad + bc & = a \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right) - ad + bc \\
        & = \frac{a^2 + ad + a\sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} - ad + bc \\
        & = \frac{a^2 + 2bc - ad + a\sqrt{a^2 - 2ad + d^2 + 4bc}}{2}
    \end{align*}
    y del lado derecho, tenemos que
    \begin{align*}
        \lambda_1^2 - d\lambda_1 & = \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right)^2 \\
        & \hspace{1.5cm} - d \left( \frac{a + d + \sqrt{(a + d)^2 - 4(ad - bc)}}{2} \right) \\
        & = \left( \frac{a + d + \sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} \right)^2 \\
        & \hspace{1.5cm} - \frac{ad + d^2 + d\sqrt{a^2 + 2ad + d^2 - 4ad + 4bc}}{2} \\
        & = \frac{a^2 + 2bc - ad + a\sqrt{a^2 - 2ad + d^2 + 4bc}}{2}
    \end{align*}
    En consecuencia, $\mathbb{v}_1$ es un eigenvector de $A$ correspondiente a $\lambda_1$. De manera similar, se puede demostrar que $\mathbb{v}_2$ es un eigenvector de $A$ correspondiente a $\lambda_2$.
    \item Si $b \neq 0$, podemos resolver la segunda ecuación para $x$, obteniendo
    $$y = \frac{( \lambda - a )x}{b}$$
    Si elegimos $x = b$, entonces $y = \lambda - a$, y obtenemos el eigenvector
    $$\mathbb{v} = \begin{pmatrix} b \\ \lambda - a \end{pmatrix}$$
    En particular, para $\lambda_1$ y $\lambda_2$, los eigenvectores están dados respectivamente por
    $$\mathbb{v}_1 = \begin{pmatrix} b \\ \lambda_1 - a \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} b \\ \lambda_2 - a \end{pmatrix}$$
    Siguiendo un procedimiento similar al anterior inciso, se puede demostrar que estos vectores son eigenvectores de $A$.
    \item Si $b = c = 0$, entonces la matriz es diagonal, lo que significa que sus eigenvectores son simplemente $\mathbb{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ y $\mathbb{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$.
\end{enumerate}

\infoBulle{Cuando hablamos de eigenvalores y eigenvectores, estamos tratando con propiedades especiales de las matrices cuadradas. Un eigenvalor es un número escalar que representa cómo un eigenvector se estira o contrae cuando se multiplica por la matriz. En otras palabras, si tenemos una matriz $A$ y un eigenvector $\mathbb{v}$, entonces tenemos
$$A\mathbb{v} = \lambda \mathbb{v}$$
Ahora, consideremos un escalar no nulo $k$ complejo o real, y multiplicamos el eigenvector $\mathbb{v}$ por $k$, obteniendo $k\mathbb{v}$. Al aplicar la matriz $A$ a $k\mathbb{v}$, tenemos:
$$A (k\mathbb{v}) = k(A\mathbb{v}) = k(\lambda \mathbb{v}) = \lambda (k\mathbb{v})$$
Esto demuestra que $k\mathbb{v}$ también es un eigenvector de $A$ asociado al mismo eigenvalor $\lambda$. Por lo tanto, cualquier múltiplo escalar no nulo de un eigenvector es también un eigenvector de la matriz $A$. En resumen, al multiplicar los eigenvectores de $A$, previamente obtenidos en los incisos (i) y (ii), por un escalar $k$ no nulo, podemos obtener eigenvectores más fácilmente manipulables, los cuales conservarán su condición de eigenvectores de $A$.}

\newpage

\begin{example}
    Tomemos la matriz del ejemplo \ref{ejemplo_eigenvalores_complejos}. Primero calculemos los eigenvalores, sabiendo que
    $$T = 3 - 1 = 2 \quad \text{ y } \quad D = (3)(-1) - (-5)(1) = 2$$
    Por lo tanto, los eigenvalores están dados por
    \begin{align*}
        \lambda_1 & = \frac{2 + \sqrt{(2)^2 - 4(2)}}{2} & \lambda_2 & = \frac{2 - \sqrt{(2)^2 - 4(2)}}{2} \\
        & = 1 - i & & = 1 + i
    \end{align*}
    Para calcular los eigenvectores, observemos que $b \neq 0$ y $c \neq 0$, por lo que podemos utilizar cualquiera de las dos fórmulas proporcionadas en los incisos (i) o (ii). En este caso, dado que $c = 1$, optamos por usar las fórmulas del inciso (i). De esta manera, obtenemos los eigenvectores $\mathbb{v}_1$ y $\mathbb{v}_2$ correspondientes a los eigenvalores $\lambda_1 = 1 - i$ y $\lambda_2 = 1 + i$, respectivamente están dados por:
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} (1 - i) - (-1) \\ 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} (1 + i) - (-1) \\ 1 \end{pmatrix} \\
        & = \begin{pmatrix} 2 - i \\ 1 \end{pmatrix} & & = \begin{pmatrix} 2 + i \\ 1 \end{pmatrix} 
    \end{align*}
    que son los mismos resultados que obtuvimos en el ejemplo \ref{ejemplo_eigenvalores_complejos}.
\end{example}

\begin{example}
    Tomemos la matriz del ejemplo \ref{example_primero_eigenvalores}. Primero calculemos los eigenvalores, sabiendo que
    $$T = 1 + (-1) = 0 \quad \text{ y } \quad D = (1)(-1) - (0)(0) = - 1$$
    Por lo tanto, los eigenvalores están dados por
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(-1)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(-1)}}{2} \\
        & = 1 & & = - 1
    \end{align*}
    Ahora, en este caso observemos que $b = 0$ y $c = 0$. Por lo tanto, los eigenvalores correspondientes a $\lambda_1$ y $\lambda_2$ respectivamente están dados por
    $$\mathbb{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$$
    que son los mismos resultados que obtuvimos en el ejemplo \ref{example_primero_eigenvalores}.
\end{example}

\begin{observation}
    De acuerdo con el teorema fundamental del álgebra, cualquier polinomio de grado $n$ con coeficientes reales o complejos tiene exactamente $n$ raíces, considerando sus multiplicidades (vea el \hyperref[FUNDAMENTAL]{Apéndice C}, pág. \pageref{CONSECUENCIA1_FUNDAMENTAL}). Esto significa, por ejemplo, que el polinomio $(\lambda - 2)^5$ tiene cinco raíces, todas iguales a $2$. Dado que cualquier eigenvalor de $A$ es una raíz de la ecuación característica de $A$, se puede concluir que, contando multiplicidades, toda matriz de tamaño $n \times n$ tiene exactamente $n$ eigenvalores.
\end{observation}

\begin{example}
    Determine los eigenvalores y los eigenvectores correspondientes a cada eigenvalor de la matriz $A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
    \end{bmatrix}$. \\
    \solucion Primero calculemos el polinomio característico de la matriz $A$ como sigue:
    \begin{align*}
        \Det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & 1 & 1 \\
            0 & 1 - \lambda & 1 \\
            0 & 0 & 1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)^3 \\
        & = (-1)^3 (\lambda - 1)^3
    \end{align*}\newpage\noindent
    Por lo tanto, los eigenvalores son las raíces del polinomio $(-1)^3 (\lambda - 1)^3 = 0$, cuya única raíz es $\lambda = 1$. De esta manera, la matriz $A$ tiene un único eigenvalor, $\lambda = 1$, con una multiplicidad algebraica de $3$. A continuación, determinemos el eigenvector asociado con $\lambda = 1$. Sea $\mathbb{v} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ un eigenvector de $A$. Esto significa que, para $\lambda = 1$
    $$\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        a \\
        b \\
        c
    \end{bmatrix} = \begin{bmatrix}
        a \\
        b \\
        c
    \end{bmatrix}$$
    del cual se obtiene el siguiente sistema
    \begin{align*}
        a + b + c & = a \\
        b + c & = b \\
        c & = c
    \end{align*}
    De este sistema se concluye inequívocamente que $c = 0$. Como resultado, también se deduce que $b = 0$ y, por lo tanto, $a$ puede tomar cualquier valor real. Así, el eigenvector correspondiente a $\lambda = 1$ está dado por
    $$\mathbb{v} = \begin{pmatrix}
        a \\
        0 \\
        0
    \end{pmatrix}$$
    En particular, si $a = 1$, obtenemos
    $$\mathbb{v} = \begin{pmatrix}
        1 \\
        0 \\
        0
    \end{pmatrix}$$
\end{example}

\begin{definition}
    Sea $\lambda$ un eigenvalor de una matriz $A$; entonces la multiplicidad geométrica de $\lambda$ es la dimensión del espacio característico correspondiente a $\lambda$ (que es la nulidad de la matriz $A - \lambda I_n$). Esto es, $\Dim (E_{\lambda})$.
\end{definition}

\begin{example}
    Si consideramos el ejemplo anterior, podemos observar que
    $$E_{\lambda} = \Gen \left(\left\{ \begin{pmatrix}
        1 \\
        0 \\
        0
    \end{pmatrix} \right\}\right)$$
    Por lo tanto, $\dim(E_{\lambda}) = 1$. En consecuencia, la multiplicidad geométrica del eigenvalor $\lambda = 1$ es igual a $1$.
\end{example}

\begin{example}
    Determine los eigenvalores de la matriz $A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 1
    \end{bmatrix}$. \\
    \solucion Primero calculemos el polinomio característico de la matriz $A$ usando la regla de Sarrus,
    \begin{align*}
        \Det(A - \lambda I_n) & = \begin{vmatrix}
            1 - \lambda & 1 & 1 \\
            0 & 1 - \lambda & 1 \\
            1 & 0 & 1 - \lambda
        \end{vmatrix} \\
        & = (1 - \lambda)(1 - \lambda)(1 - \lambda) + (1)(1)(1) + (1)(0)(0) \\
        & \hspace{1.5cm} - (1)(1 - \lambda)(1) - (0)(1)(1 - \lambda) - (1 - \lambda)(0)(1) \\
        & = (1 - \lambda)^3 + 1 - 1 + \lambda \\
        & = 1 - 3\lambda + 3\lambda^2 - \lambda^3 + \lambda \\
        & = - \lambda^3 + 3 \lambda^2 - 2\lambda + 1
    \end{align*}\newpage\noindent
    Por lo tanto, los eigenvalores son las raíces del polinomio
    $$- \lambda^3 + 3 \lambda^2 - 2\lambda + 1 = 0$$
    o equivalentemente
    \begin{equation}
        \lambda^3 - 3 \lambda^2 + 2\lambda - 1 = 0 \label{ec:tercer_grado_eigen}
    \end{equation}
    Es fácil probar que la anterior ecuación no tiene raíces enteras ni racionales. Para resolver esta ecuación, es necesario que el lector haya visto la sección \ref{sec:B4} y la sección \ref{disc_tercergrado}. Usando la sustitución $\lambda = y + 1$, reducimos la ecuación \eqref{ec:tercer_grado_eigen} a la forma
    \begin{equation}
        y^3 - y - 1 = 0 \label{ec:tercer_grado_eigen2}
    \end{equation}
    Notemos que $p = -1$ y $q = -1$, por lo cual
    $$\frac{q^2}{4} + \frac{p^3}{27} = \frac{23}{108} > 0$$
    Entonces la ecuación
    $$y^3 - y - 1 = 0$$
    tiene una raíz real y dos raíces complejas conjugadas. Según las expresiones \eqref{APPCOSIPA1} y \eqref{APPCOSIPA2},
    $$u_1=\sqrt[3]{\frac{1}{2}+\frac{\sqrt{69}}{18}} \quad \text{ y } \quad v_1=\sqrt[3]{\frac{1}{2}-\frac{\sqrt{69}}{18}}.$$
    Entones una raíz de \eqref{ec:tercer_grado_eigen2}, es
    $$y_1 = u_1 + v_1 = \sqrt[3]{\frac{1}{2}+\frac{\sqrt{69}}{18}} + \sqrt[3]{\frac{1}{2}-\frac{\sqrt{69}}{18}}$$
    Las otras dos raíces se hallan por las fórmulas \eqref{APPCOSIPA3}. Así
    $$y_2 = -\sqrt[3]{\frac{1}{16}+\frac{\sqrt{69}}{144}}-\sqrt[3]{\frac{1}{16}-\frac{\sqrt{69}}{144}}+\frac{\sqrt[3]{12 \sqrt{3}+4 \sqrt{23}} i-\sqrt[3]{12 \sqrt{3}-4 \sqrt{23}} i}{4}$$
    y
    $$y_3 = -\sqrt[3]{\frac{1}{16}-\frac{\sqrt{69}}{144}}-\sqrt[3]{\frac{1}{16}+\frac{\sqrt{69}}{144}}+\frac{\sqrt[3]{12 \sqrt{3}-4 \sqrt{23}} i-\sqrt[3]{12 \sqrt{3}+4 \sqrt{23}} i}{4}$$
    Finalmente, revirtiendo la sustitución inicial obtenemos que las raíces de la ecuación \eqref{ec:tercer_grado_eigen} está dadas por
    $$\lambda_1 = y_1 + 1, \quad \lambda_2 = y_2 + 1, \quad \lambda_3 = y_3 + 1$$
\end{example}

\begin{observation}
    El cálculo de los eigenvalores y eigenvectores para matrices de dimensión $3 \times 3$ representa un proceso considerablemente más laborioso y complejo que el correspondiente a matrices de $2 \times 2$. En el caso de matrices $2 \times 2$, es posible llevar a cabo el procedimiento de manera relativamente directa, empleando métodos como el que se describe en la página \pageref{metodo_eigen_2x2}. Por otro lado, para matrices de $3 \times 3$, se hace necesario resolver un sistema de ecuaciones más amplio y elaborado para determinar los eigenvectores, lo cual demanda una inversión mayor de tiempo y esfuerzo. Adicionalmente, al tratar con matrices de $3 \times 3$, surge la obligación de solucionar ecuaciones polinómicas de mayor complejidad para hallar los eigenvalores, incrementando así el nivel de dificultad del proceso. La magnitud de las operaciones y la complejidad matemática inherente al cálculo de eigenvalores y eigenvectores para matrices de $3 \times 3$ convierten a esta tarea en un desafío significativamente más exigente en comparación con su análogo de menor dimensión. \newpage
    Por esta razón, en el presente texto de Álgebra Lineal, optamos por concentrarnos exclusivamente en matrices de $2 \times 2$. Esta elección está motivada por el deseo de ofrecer a los lectores una exposición clara y concisa de los conceptos esenciales, evitando sobrecargarlos con la intrincada complejidad que conllevan las matrices de dimensiones superiores. Al restringir nuestro enfoque a matrices de $2 \times 2$, permitimos que los estudiantes se enfoquen en la comprensión profunda de los principios fundamentales y en el desarrollo de habilidades robustas que les servirán de cimiento para abordar conceptos más avanzados en etapas educativas subsiguientes.
\end{observation}

\section{Matrices semejantes y diagonalización}

\begin{definition}
    Sean $A$, $B \in \mathcal{M}_{n \times n}(\RR)$. Se dice que la matriz $B$ es semejante con la matriz $A$ o que la matriz $A$ es semejante con la matriz $B$ si existe una matriz $C$ de $n \times n$ (con entradas reales o complejas) invertible tal que
    $$B = C^{-1} A C$$
\end{definition}

\begin{definition}
    Sea $T: \mathcal{M}_{n \times n}(\RR) \longrightarrow \mathcal{M}_{n \times n}(\CC)$ una transformación definida de la siguiente manera:
    \begin{align*}
        T: \mathcal{M}_{n \times n}(\RR) & \longrightarrow \mathcal{M}_{n \times n}(\CC) \\
        A & \longmapsto T(A) = C^{-1} A C = B
    \end{align*}
    A esta transformación $T$, que lleva la matriz $A$ a la matriz $B$, se le conoce como \emph{transformación de semejanza}.
\end{definition}

\begin{theorem}
    Sean $A$, $B \in \mathcal{M}_{n \times n}(\RR)$ dos matrices semejantes, entonces $A$ y $B$ tienen el mismo polinomio característico y, por consiguiente, tienen los mismos eigenvalores. \\
    \demostracion Como $A$ y $B$ son semejantes, entonces existe una matriz $C$ invertible tal que
    $$B = C^{-1}AC$$
    Así,
    \begin{align*}
        \Det(B - \lambda I_n) & = \Det\left(C^{-1}AC - \lambda I_n\right) \\
        & = \Det\left(C^{-1}AC - \lambda C^{-1}I_nC\right) \\
        & = \Det\left(C^{-1}(AC - \lambda I_nC)\right) \\
        & = \Det\left(C^{-1}(A - \lambda I_n)C\right) \\
        & = \Det\left(C^{-1}\right) \Det(A - \lambda I_n) \Det(C) \\
        & = \Det(A - \lambda I_n)
    \end{align*}
    Esto significa que $A$ y $B$ tienen la misma ecuación característica, y como los eigenvalores son raíces de la ecuación característica, tienen los mismos eigenvalores.
\end{theorem}

\begin{definition}
    Se dice que una matriz $A$ de $n \times n$ es diagonalizable si existe una matriz diagonal $D$ de $n \times n$ tal que $A$ es semejante a $D$.
\end{definition}

\begin{theorem}
    Una matriz $A$ de $n \times n$ es diagonalizable si y solo si tiene $n$ eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_n$ linealmente independientes correspondientes a los eigenvalores $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_n$. En tal caso, la matriz diagonal $D$ semejante a $A$ está dada por
    $$D = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & & \ddots & \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}$$\newpage\noindent
    Si $C$ es una matriz cuyas columnas son eigenvectores linealmente independientes de $A$, entonces
    $$D = C^{-1}AC$$
    \demostracion Sea $A$ una matriz de $n \times n$. Supongamos que $A$ tiene $n$ eigenvectores $\mathbb{v}_1$, $\mathbb{v}_2$, $\dots$, $\mathbb{v}_n$ linealmente independientes correspondientes a los eigenvalores $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_n$ (no necesariamente diferentes). Sean
    $$\mathbb{v}_1 = \begin{pmatrix} c_{11} \\ c_{21} \\ \vdots \\ c_{n1} \end{pmatrix}, \mathbb{v}_2 = \begin{pmatrix} c_{12} \\ c_{22} \\ \vdots \\ c_{n2} \end{pmatrix}, \dots, \mathbb{v}_n = \begin{pmatrix} c_{1n} \\ c_{2n} \\ \vdots \\ c_{nn} \end{pmatrix}$$
    y sea\infoBulle{Si los vectores columna de una matriz de $n \times n$ son linealmente independientes, entonces la matriz tiene rango $n$, lo que significa que su determinante es distinto de cero. Ya que el determinante distinto de cero es una condición necesaria y suficiente para que una matriz sea invertible, podemos concluir que la matriz es invertible si sus vectores columna son linealmente independientes.}
    \begin{equation*}
        C = \left[\begin{array}{cccc}
            \tikzmarkin[ver=style azull]{col 1-a}c_{11} & \tikzmarkin[ver=style azull]{col 2-a}c_{12} & \cdots & \tikzmarkin[ver=style azull]{col n-a}c_{1n} \\
            c_{12} & c_{22} & \cdots & c_{2n} \\
            \vdots & & \ddots & \\
            c_{n1} \tikzmarkend{col 1-a} & c_{n2}\tikzmarkend{col 2-a} & \cdots & c_{nn}\tikzmarkend{col n-a} \\
        \end{array}\right]
        \begin{tikzpicture}[overlay, remember picture]
            \node[below=50pt of col 1-a.south west,xshift=4pt](A) {};
            \node[right=2pt of A] (B) {};
            \node[below=25pt of B] (C) {$\mathbb{v}_1$};
            \draw[-latex] (C) -- (B);

            \node[below=50pt of col 2-a.south west,xshift=4pt](D) {};
            \node[right=2pt of D] (E) {};
            \node[below=25pt of E] (F) {$\mathbb{v}_2$};
            \draw[-latex] (F) -- (E);

            \node[below=50pt of col n-a.south west,xshift=4pt](G) {};
            \node[right=2pt of G] (H) {};
            \node[below=25pt of H] (I) {$\mathbb{v}_n$};
            \draw[-latex] (I) -- (H);
        \end{tikzpicture}
    \end{equation*}
    \,\\ \,\\ \,\\ \,\\
    Entonces $C$ es invertible ya que sus columnas son linealmente independientes. Ahora bien,
    $$AC = \begin{bmatrix}
        A\mathbb{v}_1 & A\mathbb{v}_2 & \cdots & A\mathbb{v}_n
    \end{bmatrix}$$
    Pero por la definición \ref{def:eigenvalor}, se sigue que
    $$AC = \begin{bmatrix}
        \lambda_1\mathbb{v}_1 & \lambda_2\mathbb{v}_2 & \cdots & \lambda_n\mathbb{v}_n
    \end{bmatrix}$$
    Es decir,
    $$AC = \begin{bmatrix}
        \lambda_1c_{11} & \lambda_2c_{12} & \cdots & \lambda_nc_{1n} \\
        \lambda_1c_{21} & \lambda_2c_{22} & \cdots & \lambda_nc_{2n} \\
        \vdots & & \ddots & \\
        \lambda_1c_{n1} & \lambda_2c_{n2} & \cdots & \lambda_nc_{nn}
    \end{bmatrix}$$
    Pero
    \begin{align*}
        CD & = \begin{bmatrix}
            c_{11} & c_{12} & \cdots & c_{1n} \\
            c_{21} & c_{22} & \cdots & c_{2n} \\
            \vdots & & \ddots & \\
            c_{n1} & c_{n2} & \cdots & c_{nn}
        \end{bmatrix} \begin{bmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & & \ddots & \\
            0 & 0 & \cdots & \lambda_n
        \end{bmatrix} \\
        & = \begin{bmatrix}
            \lambda_1c_{11} & \lambda_2c_{12} & \cdots & \lambda_nc_{1n} \\
            \lambda_1c_{21} & \lambda_2c_{22} & \cdots & \lambda_nc_{2n} \\
            \vdots & & \ddots & \\
            \lambda_1c_{n1} & \lambda_2c_{n2} & \cdots & \lambda_nc_{nn}
        \end{bmatrix}
    \end{align*}
    Entonces
    $$AC = BD$$
    y como $C$ es invertible, se sigue que
    $$D = C^{-1}AC$$
    lo que por definición, significa que $D$ es semejante a $A$. Recíprocamente, se procede de manera similar.
\end{theorem}

\begin{theorem}
    Si una matriz $A$ de $n \times n$ tiene $n$ eigenvalores diferentes, entonces $A$ es diagonalizable. \\
    \demostracion Dado que $A$ tiene $n$ eigenvalores distintos, según el teorema \ref{eigenvalores_distintos_eigenvectoresli}, posee $n$ eigenvectores linealmente independientes. Además, con base en el teorema previo, se deduce que $A$ es diagonalizable.
\end{theorem}

\newpage

\begin{example}
    Sea $A = \begin{bmatrix*}[r]
        2 & -1 \\
        5 & -2
    \end{bmatrix*}$. ¿Es diagonalizable? Sí lo es, determine la matriz diagonal. \\
    \solucion En primer lugar, determinemos los eigenvalores de $A$. Utilizando el método expuesto en la página \pageref{metodo_eigen_2x2}, se obtiene que los eigenvalores son
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(1)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(1)}}{2} \\
        & = i & & = - i
    \end{align*}
    De acuerdo con el teorema anterior, como $\lambda_1 \neq \lambda_2$, se concluye que $A$ es diagonalizable. Ahora, determinemos los eigenvectores correspondientes a $\lambda_1 = i$ y $\lambda_2 = -i$. Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (ii), obteniendo así
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} -1 \\ i - 2 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} -1 \\ - i -2 \end{pmatrix} \\
        & = \begin{pmatrix} 1 \\ 2 - i \end{pmatrix} & & = \begin{pmatrix} 1 \\ 2 + i \end{pmatrix}
    \end{align*}
    Ahora bien, sea $C$ la matriz formada por los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$. Es decir,
    \begin{align*}
        C & = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} 1 & 1 \\ 2 - i & 2 + i \end{bmatrix}
    \end{align*}
    Ahora, determinemos $C^{-1}$ mediante la expresión \ref{Jjaksksksisid},
    \begin{align*}
        C^{-1} & = \frac{1}{2i} \begin{bmatrix*}[r]
            2 + i & - 1 \\
            -(2 - i) & 1
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \dfrac{1}{2} - i & \dfrac{1}{2}i \\[2mm]
            \dfrac{1}{2} + i & - \dfrac{1}{2}i
        \end{bmatrix*}
    \end{align*}
    Así, obtenemos
    \begin{align*}
        C^{-1}AC & = \begin{bmatrix*}[r]
            \dfrac{1}{2} - i & \dfrac{1}{2}i \\[2mm]
            \dfrac{1}{2} + i & - \dfrac{1}{2}i
        \end{bmatrix*} \begin{bmatrix*}
            2 & - 1 \\
            5 & - 2
        \end{bmatrix*} \begin{bmatrix}
            1 & 1 \\
            2 - i & 2 + i
        \end{bmatrix} \\
        & = \begin{bmatrix}
            1 + \dfrac{1}{2}i & - \dfrac{1}{2} \\[2mm]
            1 - \dfrac{1}{2}i & - \dfrac{1}{2}
        \end{bmatrix} \begin{bmatrix}
            1 & 1 \\
            2 - i & 2 + i
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            i & 0 \\
            0 & - i
        \end{bmatrix*}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$.
\end{example}

\section{Análisis de matrices simétricas y diagonalización ortogonal}\label{sec:matrices_sim_her_uni}

\begin{theorem}\label{theorem_simetrica1}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ una matriz simétrica. Entonces los eigenvalores de $A$ son números reales. \\
    \demostracion Sea $\lambda$ un eigenvalor de $A$ con eigenvector $\mathbb{v}$, es decir,
    $$A\mathbb{v} = \lambda \mathbb{v}$$\newpage\noindent
    En general, $\lambda \in \CC$, por lo tanto, $\mathbb{v} \in \CC[n]$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \langle \lambda \mathbb{v}, \mathbb{v} \rangle \\
        & = \lambda \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Dado que $A$ es simétrica, entonces $A = A^T$, y según el teorema \ref{JAJSJSJSJJSJSHSTTYQUUQHIAIIQVVCGTQYHHAHAHA}, se sigue que
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \left\langle \mathbb{v}, A^T \mathbb{v} \right\rangle \\
        & = \langle \mathbb{v}, A\mathbb{v} \rangle \\
        & = \langle \mathbb{v}, \lambda \mathbb{v} \rangle \\
        & = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Por lo tanto,
    $$\lambda \langle \mathbb{v}, \mathbb{v} \rangle = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle$$
    pero $\langle \mathbb{v}, \mathbb{v} \rangle = \| \mathbb{v} \| \neq 0$, ya que $\mathbb{v}$ es un eigenvector, es decir, $\mathbb{v} \neq \mathbb{0}$. Dividiendo ambos lados entre $\langle \mathbb{v}, \mathbb{v} \rangle$, obtenemos
    $$\lambda = \overline{\lambda}$$
    y según la proposición \ref{conjugado-real}, se sigue que $\lambda$ es un número real.
\end{theorem}

En el teorema \ref{eigenvalores_distintos_eigenvectoresli}, se demostró que los eigenvectores asociados a eigenvalores distintos son linealmente independientes. Sin embargo, para matrices simétricas reales, el resultado es aún más sorprendente: \emph{los eigenvectores de una matriz simétrica real correspondientes a eigenvalores distintos son ortogonales}.

\begin{theorem}\label{theorem_simetrica2}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$ simétrica. Si $\lambda_1$ y $\lambda_2$ son eigenvalores distintos con eigenvectores reales correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$, entonces $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales. \\
    \demostracion Sea $\lambda_1$ y $\lambda_2$ eigenvalores de $A$ distintos correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \langle \lambda_1 \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Dado que $A$ es simétrica, entonces $A = A^T$. Por el teorema \ref{JAJSJSJSJJSJSHSTTYQUUQHIAIIQVVCGTQYHHAHAHA} y el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \left\langle \mathbb{v}_1, A^T \mathbb{v}_2 \right\rangle \\
        & = \langle \mathbb{v}_1, A\mathbb{v}_2 \rangle \\
        & = \langle \mathbb{v}_1, \lambda_2 \mathbb{v}_2 \rangle \\
        & = \overline{\lambda_2} \langle \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Por lo tanto, consideremos la siguiente expresión
    $$\lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle$$
    De esta ecuación, podemos deducir lo siguiente
    $$(\lambda_1 - \lambda_2) \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$$
    Dado que los eigenvalores $\lambda_1$ y $\lambda_2$ son distintos, podemos afirmar que $\lambda_1 \neq \lambda_2$. Por lo tanto, $\lambda_1 - \lambda_2 \neq 0$. Esto nos lleva a la conclusión de que $\langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$. Según la definición \ref{orto_prodinterno}, esto implica que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales.
\end{theorem}

\newpage

\begin{definition}
    Se dice que una matriz $A \in \mathcal{M}_{n \times n}(\RR)$ es diagonalizable ortogonalmente si existe una matriz ortogonal $Q$ tal que
    $$D = Q^{T}AQ$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$.
\end{definition}

\begin{theorem}\label{theorem_simetrica3}
    Sea $A \in \mathcal{M}_{n \times n}(\RR)$. Entonces $A$ es diagonalizable ortogonalmente si y solo si $A$ es simétrica. \\
    \demostracion Por la definición anterior, $A$ es diagonalizable ortogonalmente si y solo si existe una matriz $Q$ ortogonal tal que
    $$D = Q^{T}AQ$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$. Así
    $$D = Q^{T}AQ$$
    Al multiplicar la ecuación anterior por la izquierda por $Q$, se sigue que
    $$QD = QQ^{T}AQ$$
    Al utilizar el hecho de que $QQ^{T} = I_n$ y al multiplicar por la derecha por $Q^{T}$, obtenemos
    $$QDQ^{T} = AQQ^{T}$$
    De igual manera, al utilizar el hecho de que $QQ^{T} = I_n$, se sigue que
    $$QDQ^{T} = A$$
    Ahora bien, utilizando el teorema \ref{theo:matrixtranspu}, se sigue que
    \begin{align*}
        A^{T} & = \left( QDQ^{T} \right)^{T} \\
        & = \left( Q^{T} \right)^{T} D^{T} Q^{T}
    \end{align*}
    Utilizando el inciso (i) de la proposición \ref{propiedades_transpuesta} y el inciso (iv) de la proposición \ref{propiedades_diagonal}, obtenemos
    \begin{align*}
        A^T & = QDQ^{T} \\
        & = A
    \end{align*}
    Así, $A$ es simétrica y el teorema queda demostrado.
\end{theorem}

\begin{example}
    Consideremos la matriz $A = \begin{bmatrix*}[r]
        1 & -1 \\
        -1 & 1
    \end{bmatrix*}$, la cual es simétrica. Según el teorema \ref{theorem_simetrica1}, podemos inferir que sus eigenvalores son números reales. Además, conforme al teorema \ref{theorem_simetrica2}, los eigenvectores correspondientes a sus eigenvalores son ortogonales. Finalmente, derivado del teorema \ref{theorem_simetrica3}, concluimos que $A$ es diagonalizable ortogonalmente. Para demostrar lo anteriormente mencionado, primero calcularemos los eigenvalores de $A$. Para esto, emplearemos el método detallado en la página \pageref{metodo_eigen_2x2}. Así
    \begin{align*}
        \lambda_1 & = \frac{2 + \sqrt{(2)^2 - 4(0)}}{2} & \lambda_2 & = \frac{2 - \sqrt{(2)^2 - 4(0)}}{2} \\
        & = 2 & & = 0
    \end{align*}
    Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (i), obteniendo
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} 2 - 1 \\ - 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} 0 - 1 \\ - 1 \end{pmatrix} \\
        & = \begin{pmatrix*}[r] 1 \\ -1 \end{pmatrix*} & & = \begin{pmatrix} -1 \\ -1 \end{pmatrix}
    \end{align*}\newpage\noindent
    Para determinar la matriz $Q$, podemos emplear el teorema \ref{Qorto_vectoresorto}. Notemos que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales, ya que $\mathbb{v}_1 \bullet \mathbb{v}_2 = 0$. Por lo tanto,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \begin{pmatrix*}[r]
            \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}}
        \end{pmatrix*} & & = \begin{pmatrix}
            -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}}
        \end{pmatrix}
    \end{align*}
    Así, sea $Q = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$, entonces $Q^T = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$. Por lo tanto,
    \begin{align*}
        Q^TAQ & = \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            1 & -1 \\
            -1 & 1
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \sqrt{2} & -\sqrt{2} \\
            0 & 0
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            2 & 0 \\
            0 & 0
        \end{bmatrix}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$.
\end{example}

En esta sección se han probado resultados para matrices simétricas reales. Estos resultados se pueden extender a matrices complejas.

\begin{definition}
    Si $A = \llparenthesis a_{ij} \rrparenthesis$ es una matriz compleja, entonces la transpuesta conjugada de $A$, denotada por $A^*$, está definida por el elemento $ij$ de $A = \overline{\llparenthesis a_{ij} \rrparenthesis}$. La matriz $A$ se denomina hermitiana si $A^* = A$.
\end{definition}

\begin{example}
    Si $A = \begin{bmatrix}
        5 & 3 + 7i \\
        3 - 7i & 2
    \end{bmatrix}$, entonces $A^* = \begin{bmatrix}
        5 & 3 + 7i \\
        3 - 7i & 2
    \end{bmatrix}$. Por lo tanto, $A^* = A$ y se concluye que $A$ es una matriz hermitiana.
\end{example}

\begin{theorem}\label{inner_prod_A_Ahermitiana}
    Sean $A \in \mathcal{M}_{n \times n}(\CC)$, $\mathbb{x}$, $\mathbb{y} \in \CC[n]$. Entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left\langle \mathbb{x}, A^*\mathbb{y} \right\rangle$$
    \demostracion Consideremos el producto interno usual en $\CC[n]$, es decir,
    $$\langle \mathbb{u}, \mathbb{v} \rangle = u_1 \overline{\mathbb{v}_1} + u_2 \overline{\mathbb{v}_2} + \cdots + u_n \overline{\mathbb{v}_n}$$
    De esta forma, tenemos que si
    $$A\mathbb{x} = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots &  & \ddots & \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix} = \begin{bmatrix}
        \displaystyle\sum_{j=1}^n a_{1j}x_j \\
        \displaystyle\sum_{j=1}^n a_{2j}x_j \\
        \vdots \\
        \displaystyle\sum_{j=1}^n a_{nj}x_j
    \end{bmatrix} \quad \text{ y } \quad \mathbb{y} = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}$$\newpage\noindent
    entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left( \sum_{j=1}^n a_{1j}x_j \right) \overline{y_1} + \left( \sum_{j=1}^n a_{2j}x_j \right) \overline{y_2} + \cdots + \left( \sum_{j=1}^n a_{nj}x_j \right) \overline{y_n}$$
    lo que se traduce en
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left( \sum_{i=1}^n a_{i1} \overline{y_i} \right) x_1 + \left( \sum_{i=1}^n a_{i2} \overline{y_i} \right) x_2 + \cdots + \left( \sum_{i=1}^n a_{in} \overline{y_i} \right) x_n$$
    es decir
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \overline{\overline{\left( \sum_{i=1}^n a_{i1} \overline{y_i} \right)}} x_1 + \overline{\overline{\left( \sum_{i=1}^n a_{i2} \overline{y_i} \right)}} x_2 + \cdots + \overline{\overline{\left( \sum_{i=1}^n a_{in} \overline{y_i} \right)}} x_n$$
    entonces
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \overline{\left( \sum_{i=1}^n \overline{a_{i1} \overline{y_i}} \right)} x_1 + \overline{\left( \sum_{i=1}^n \overline{a_{i2} \overline{y_i}} \right)} x_2 + \cdots + \overline{\left( \sum_{i=1}^n \overline{a_{in} \overline{y_i}} \right)} x_n$$
    de donde se deduce que
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = x_1 \overline{\left( \sum_{i=1}^n \overline{a_{i1}} y_i \right)} + x_2 \overline{\left( \sum_{i=1}^n \overline{a_{i2}} y_i \right)} + \cdots + x_n \overline{\left( \sum_{i=1}^n \overline{a_{in}} y_i \right)}$$
    Pero observemos que
    $$A^* \mathbb{y} = \begin{bmatrix}
        \overline{a_{11}} & \overline{a_{21}} & \cdots & \overline{a_{n1}} \\
        \overline{a_{12}} & \overline{a_{22}} & \cdots & \overline{a_{n2}} \\
        \vdots &  & \ddots & \\
        \overline{a_{1n}} & \overline{a_{2n}} & \cdots & \overline{a_{nn}}
    \end{bmatrix} \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} = \begin{bmatrix}
        \displaystyle\sum_{i=1}^n \overline{a_{i1}} y_i \\
        \displaystyle\sum_{i=1}^n \overline{a_{i2}} y_i \\
        \vdots \\
        \displaystyle\sum_{i=1}^n \overline{a_{in}} y_i
    \end{bmatrix}$$
    así que de la última expresión se hereda que
    $$\langle A\mathbb{x}, \mathbb{y} \rangle = \left\langle \mathbb{x}, A^*\mathbb{y} \right\rangle$$
\end{theorem}

\begin{theorem}\label{theorem_hermitiana1}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$ una matriz hermitiana. Entonces los eigenvalores de $A$ son números reales. \\
    \demostracion Sea $\lambda$ un eigenvalor de $A$ con eigenvector $\mathbb{v}$, es decir,
    $$A\mathbb{v} = \lambda \mathbb{v}$$
    En general, $\lambda \in \CC$, por lo tanto, $\mathbb{v} \in \CC[n]$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \langle \lambda \mathbb{v}, \mathbb{v} \rangle \\
        & = \lambda \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Dado que $A$ es hermitiana, entonces $A = A^*$, y según el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}, \mathbb{v} \rangle & = \left\langle \mathbb{v}, A^* \mathbb{v} \right\rangle \\
        & = \langle \mathbb{v}, A\mathbb{v} \rangle \\
        & = \langle \mathbb{v}, \lambda \mathbb{v} \rangle \\
        & = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle
    \end{align*}
    Por lo tanto,
    $$\lambda \langle \mathbb{v}, \mathbb{v} \rangle = \overline{\lambda} \langle \mathbb{v}, \mathbb{v} \rangle$$\newpage\noindent
    pero $\langle \mathbb{v}, \mathbb{v} \rangle = \| \mathbb{v} \| \neq 0$, ya que $\mathbb{v}$ es un eigenvector, es decir, $\mathbb{v} \neq \mathbb{0}$. Dividiendo ambos lados entre $\langle \mathbb{v}, \mathbb{v} \rangle$, obtenemos
    $$\lambda = \overline{\lambda}$$
    y según la proposición \ref{conjugado-real}, se sigue que $\lambda$ es un número real.
\end{theorem}

\begin{theorem}\label{theorem_hermitiana2}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$ hermitiana. Si $\lambda_1$ y $\lambda_2$ son eigenvalores distintos con eigenvectores correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$, entonces $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales. \\
    \demostracion Sea $\lambda_1$ y $\lambda_2$ eigenvalores de $A$ distintos correspondientes a $\mathbb{v}_1$ y $\mathbb{v}_2$. De la definición del producto interno (definición \ref{def:espaciocomplejo_productointerno}), obtenemos
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \langle \lambda_1 \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Dado que $A$ es hermitiana, entonces $A = A^*$. Por el teorema \ref{inner_prod_A_Ahermitiana} y el teorema anterior, se sigue que
    \begin{align*}
        \langle A\mathbb{v}_1, \mathbb{v}_2 \rangle & = \left\langle \mathbb{v}_1, A^* \mathbb{v}_2 \right\rangle \\
        & = \langle \mathbb{v}_1, A\mathbb{v}_2 \rangle \\
        & = \langle \mathbb{v}_1, \lambda_2 \mathbb{v}_2 \rangle \\
        & = \overline{\lambda_2} \langle \mathbb{v}_1, \mathbb{v}_2 \rangle \\
        & = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle
    \end{align*}
    Por lo tanto, consideremos la siguiente expresión
    $$\lambda_1 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = \lambda_2 \langle \mathbb{v}_1, \mathbb{v}_2 \rangle$$
    De esta ecuación, podemos deducir lo siguiente
    $$(\lambda_1 - \lambda_2) \langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$$
    Dado que los eigenvalores $\lambda_1$ y $\lambda_2$ son distintos, podemos afirmar que $\lambda_1 \neq \lambda_2$. Por lo tanto, $\lambda_1 - \lambda_2 \neq 0$. Esto nos lleva a la conclusión de que $\langle \mathbb{v}_1, \mathbb{v}_2 \rangle = 0$. Según la definición \ref{orto_prodinterno}, esto implica que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales.
\end{theorem}

\begin{definition}
    Una matriz unitaria es una matriz cuadrada cuya inversa es igual a su transpuesta conjugada. Es decir, una matriz $U \in \mathcal{M}_{n \times n}(\CC)$ se llama unitaria si $U$ es invertible y $U^{-1} = U^*$. Formalmente, una matriz $U$ se considera unitaria si cumple con la siguiente propiedad:
    \[ U^* U = I_n \]
\end{definition}

\begin{theorem}\label{Uorto_vectoresorto}
    Dada $U \in \mathcal{M}_{n \times n}(\CC)$. Entonces $U$ es unitaria si y solo si sus vectores columnas son ortonormales. \\
    \demostracion Supongamos que $U$ es una matriz unitaria de $n \times n$. Entonces, por definición de matriz unitaria, $U^* U = I_n$. Dado que la entrada \( (i, j) \) de $U^*U$ es el producto interno entre la $j$-ésima columna de $U^*$ y la $i$-ésima columna de $U$, y este producto interno es igual a $0$ si $i$ y $j$ son distintos y $1$ si $i = j$ (debido a que las columnas de $U$ son ortonormales), entonces la matriz $U^* U$ es la matriz identidad, lo que implica que $U$ es unitaria. Por otro lado, si las columnas de $U$ son ortonormales, entonces la matriz $U^* U$ será la matriz identidad, ya que las entradas fuera de la diagonal serán $0$ y las entradas diagonales serán $1$ (debido a la normalización de los vectores columna). Por lo tanto, $U$ es unitaria.
\end{theorem}

\begin{definition}
    Se dice que una matriz $A \in \mathcal{M}_{n \times n}(\CC)$ es diagonalizable unitariamente si existe una matriz unitaria $U$ tal que
    $$D = U^*AU$$
    donde $D = \operatorname{diag} \{ \lambda_1, \lambda_2, \dots, \lambda_n \}$ y $\lambda_1, \lambda_2, \dots, \lambda_n$ son los eigenvalores de la matriz $A$.
\end{definition}

\newpage

\begin{theorem}\label{theorem_hermitiana3}
    Sea $A \in \mathcal{M}_{n \times n}(\CC)$. Entonces $A$ es diagonalizable unitariamente si y solo si $A$ es hermitiana. \\
    \demostracion Usando la demostración del teorema \ref{theorem_simetrica3}, se puede demostrar este teorema. Este hecho se propone como ejercicio al lector.
\end{theorem}

\begin{example}
    Consideremos la matriz $A = \begin{bmatrix*}[r]
        1 & i \\
        -i & -1
    \end{bmatrix*}$, la cual es hermitiana. Según el teorema \ref{theorem_hermitiana1}, podemos inferir que sus eigenvalores son números reales. Además, conforme al teorema \ref{theorem_hermitiana2}, los eigenvectores correspondientes a sus eigenvalores son ortogonales. Finalmente, derivado del teorema \ref{theorem_hermitiana3}, concluimos que $A$ es diagonalizable unitariamemte. Para demostrar lo anteriormente mencionado, primero calcularemos los eigenvalores de $A$. Para esto, emplearemos el método detallado en la página \pageref{metodo_eigen_2x2}. Así
    \begin{align*}
        \lambda_1 & = \frac{0 + \sqrt{(0)^2 - 4(-2)}}{2} & \lambda_2 & = \frac{0 - \sqrt{(0)^2 - 4(-2)}}{2} \\
        & = \sqrt{2} & & = -\sqrt{2}
    \end{align*}
    Dado que $b \neq 0$ y $c \neq 0$, podemos emplear cualquiera de las dos fórmulas indicadas en la página \pageref{metodo_eigen_2x2}. Optemos por las fórmulas del inciso (ii), obteniendo
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} i \\ \sqrt{2} - 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} i \\ - \sqrt{2} - 1 \end{pmatrix}
    \end{align*}
    Para determinar la matriz $U$, podemos emplear el teorema \ref{Uorto_vectoresorto}. Notemos que los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$ son ortogonales, ya que
    \begin{align*}
        \langle \mathbb{v}_1, \mathbb{v}_2 \rangle & = (i)(-i) + (\sqrt{2} - 1)(- \sqrt{2} - 1) \\
        & = - i^2 + (- 1 + \sqrt{2})(- 1 - \sqrt{2}) \\
        & = 1 + 1 - 2 \\
        & = 0
    \end{align*}
    Por lo tanto,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \frac{1}{\sqrt{4 - 2\sqrt{2}}} \begin{pmatrix} i \\ \sqrt{2} - 1 \end{pmatrix} & & = \frac{1}{\sqrt{4 + 2\sqrt{2}}} \begin{pmatrix} i \\ - \sqrt{2} - 1 \end{pmatrix} \\
        & = \begin{pmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}}
        \end{pmatrix*} & & = \begin{pmatrix}
            \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{pmatrix}
    \end{align*}
    Así, sea
    $$U = \begin{bmatrix*}[r]
        \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
        \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*}$$
    entonces
    $$U^* = \begin{bmatrix*}[r]
        \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
        \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*}$$
    Observe que $U$, en efecto, es unitaria, pues
    $$U^*U = \begin{bmatrix*}[r]
        \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
        \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*} \begin{bmatrix*}[r]
        \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
        \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
    \end{bmatrix*} = \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$\newpage\noindent
    Por lo tanto,
    \begin{align*}
        U^*AU & = \begin{bmatrix*}[r]
            \dfrac{-i}{\sqrt{4 - 2\sqrt{2}}} &  \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} \\[3.5mm]
            \dfrac{-i}{\sqrt{4 + 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            1 & i \\
            -i & -1
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            -\dfrac{\sqrt{4 + 2\sqrt{2}}}{2}i & \dfrac{\sqrt{4 - 2\sqrt{2}}}{2} \\[3.5mm]
            \dfrac{\sqrt{4 - 2\sqrt{2}}}{2}i & \dfrac{\sqrt{4 + 2\sqrt{2}}}{2}
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{i}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{i}{\sqrt{4 + 2\sqrt{2}}} \\[3.5mm]
            \dfrac{\sqrt{2} - 1}{\sqrt{4 - 2\sqrt{2}}} & \dfrac{- \sqrt{2} - 1}{\sqrt{4 + 2\sqrt{2}}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \sqrt{2} & 0 \\
            0 & - \sqrt{2}
        \end{bmatrix*}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$.
\end{example}

\section{Formas cuadráticas y secciones cónicas}

La teoría de las formas cuadráticas tiene su origen en la geometría analítica, más precisamente, en la teoría de las curvas (y superficies) de segundo orden. Es bien sabido que la ecuación de una curva central de segundo orden en el plano, después de trasladar el origen de coordenadas rectangulares al centro de esta curva, tiene la forma
\begin{equation}
    ax^2 + bxy + cy^2 = d \label{cuadratica1}
\end{equation}
Se sabe también que se puede efectuar una rotación de los ejes coordenados en un ángulo $\theta$, o sea, un cambio de las coordenadas $x$ e $y$, por las coordenadas $x_1$ e $y_1$ donde
\begin{equation}
    \begin{aligned}
        x & = x_1 \cos \theta - y_1 \sen \theta \\
        y & = x_1 \sen \theta + y_1 \cos \theta
    \end{aligned} \label{cuadratica2}
\end{equation}
de modo que en las nuevas coordenadas la ecuación de la curva tome la forma “canónica”:
\begin{equation}
    a_1x_1^2 + c_1y_1^2 = d \label{cuadratica3}
\end{equation}
por consiguiente, en esta ecuación, el coeficiente del producto $x_1y_1$ de las indeterminadas es igual a cero. Evidentemente, la transformación de coordenadas \eqref{cuadratica2} se puede interpretar como una transformación lineal de las indeterminadas (véase el ejemplo \ref{tl:rotacion}), la cual, además, no es degenerada, puesto que el determinante de sus coeficientes es igual a la unidad. Esta transformación se aplica al primer miembro de la ecuación \eqref{cuadratica1}. Por lo tanto, se puede decir que mediante la transformación lineal no degenerada \eqref{cuadratica2}, el primer miembro de la ecuación \eqref{cuadratica1} se convierte en el primer miembro de la ecuación \eqref{cuadratica3}.

Numerosas aplicaciones reclamaron la elaboración de una teoría análoga para el caso en que el número de las indeterminadas, en lugar de dos, sea igual a cualquier $n$, y los coeficientes sean, o bien números reales, o bien números complejos cualesquiera. Sin embargo, para nuestros propósitos, nos limitaremos a considerar el caso de dos indeterminadas y coeficientes reales. Al aplicar esta restricción, alcanzamos el siguiente concepto.

\begin{definition}
    Sean $a$, $b$, $c$, $d \in \RR$.
    \begin{enumerate}[label=\roman*)]
        \item Una ecuación cuadrática en dos variables sin términos lineales es una ecuación de la forma
        $$ax^2 + bxy + cy^2 = d$$
        donde $|a| + |b| + |c| \neq 0$.\newpage
        \item Una forma cuadrática en dos variables es una expresión de la forma
        $$F(x, y) = ax^2 + bxy + cy^2$$
        donde $|a| + |b| + |c| \neq 0$.
    \end{enumerate}
\end{definition}

Recordemos que las secciones cónicas, curvas obtenidas al intersectar un plano con un cono de doble nappes, son una clase especial de curvas que incluyen circunferencias, elipses, parábolas y hipérbolas. Estas curvas no solo son fascinantes por su belleza geométrica, sino que también desempeñan un papel fundamental en campos como la óptica, la mecánica celeste y la ingeniería. Una herramienta esencial para el estudio de formas cuadráticas es la matriz simétrica. Esta matriz, denotada como $A$, tiene la propiedad de que al multiplicarla por un vector $\mathbb{v}$ y luego tomar el producto punto con este mismo vector $\mathbb{v}$, obtenemos la expresión $A\mathbb{v} \bullet \mathbb{v}$. Esta matriz simétrica proporciona una representación compacta y eficiente de las formas cuadráticas, lo que facilita su análisis y manipulación en diversas aplicaciones.

Por ejemplo, consideremos la matriz simétrica:
$$A = \begin{bmatrix*}[r] 2 & -1 \\ -1 & 4 \end{bmatrix*}$$
Esta matriz simétrica está asociada con la forma cuadrática
$$F(x, y) = 2x^2 - 2xy + 4y^2$$
Al multiplicarla dicha matriz por el vector $\mathbb{v} = \begin{pmatrix} x \\ y \end{pmatrix}$, obtenemos el vector
$$A\mathbb{v} = \begin{pmatrix} 2x - y \\ -x + 4y \end{pmatrix}$$
Luego, al tomar el producto punto con el mismo vector $\mathbb{v} = \begin{pmatrix} x \\ y \end{pmatrix}$, obtenemos la expresión
$$A\mathbb{v} \bullet \mathbb{v} = 2x^2 - 2xy + 4y^2$$

En general, esta matriz, denotada como $A$, está dada por
$$A = \begin{bmatrix} a & \dfrac{b}{2} \\[1.5mm] \dfrac{b}{2} & c \end{bmatrix}$$
donde $a$, $b$, y $c$ son números reales que representan los coeficientes de la forma cuadrática $ax^2 + bxy + cy^2$. Al multiplicar esta matriz por un vector $\mathbb{v} = \begin{pmatrix} x \\ y \end{pmatrix}$, obtenemos el vector
$$A\mathbb{v} = \begin{pmatrix} ax + \dfrac{by}{2} \\[2mm] \dfrac{bx}{2} + cy \end{pmatrix}$$
Luego, al tomar el producto punto con el mismo vector $\mathbb{v} = \begin{pmatrix} x \\ y \end{pmatrix}$, obtenemos la expresión
\begin{equation}
    A\mathbb{v} \bullet \mathbb{v} = ax^2 + bxy + cy^2 = d \label{PRODLPRODIN}
\end{equation}
que corresponde a la forma cuadrática en cuestión. Dado que $A$ es simétrica, se deduce que es diagonalizable ortogonalmente según el teorema \ref{theorem_simetrica3}. Esto implica la existencia de una matriz $Q \in \mathcal{M}_{2 \times 2}(\RR)$ ortogonal tal que
$$D = \operatorname{diag} \{ \lambda_1, \lambda_2 \} = Q^T A Q$$\newpage\noindent
donde $\lambda_1$ y $\lambda_2$ son los eigenvalores de $A$. Puesto que $Q$ es ortogonal, es decir, $Q^T = Q^{-1}$, se sigue que
$$D = Q^{-1}AQ$$
Al multiplicar la ecuación anterior por la izquierda por $Q$, se sigue que
$$QD = AQ$$
Al multiplicar la ecuación anterior por la derecha por $Q^{-1}$, se llega a
$$QDQ^{-1} = A$$
y dado que $Q^{-1} = Q^T$, entonces
\begin{equation}
    QDQ^T = A \label{DEGAA}
\end{equation}
Sustituyendo \eqref{DEGAA} en \eqref{PRODLPRODIN}, obtenemos que
\begin{align*}
    d & = A\mathbb{v} \bullet \mathbb{v} \\
    & = \left( QDQ^T \right) \mathbb{v} \bullet \mathbb{v} \\
    & = QD \left( Q^T \mathbb{v} \right) \bullet \mathbb{v} \\
    & = Q \left[ D \left( Q^T \mathbb{v} \right) \right] \bullet \mathbb{v} \\
    & = D \left( Q^T \mathbb{v} \right) \bullet Q^T \mathbb{v} \\
    & = D \left( Q^T \mathbb{v} \right) \bullet \left( Q^T \mathbb{v} \right) \\
    & = D \mathbb{v}' \bullet \mathbb{v}'
\end{align*}
donde $\mathbb{v}' = \begin{pmatrix} x_1 \\ y_1 \end{pmatrix} = Q^T \mathbb{v}$. De esta forma,
\begin{equation}
    d = D \mathbb{v}' \bullet \mathbb{v}' \label{CUADSINXY}
\end{equation}
Así,
\begin{align*}
    d & = \begin{bmatrix}
        \lambda_1 & 0 \\
        0 & \lambda_2
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        y_1 
    \end{bmatrix} \bullet \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} \\
    & = \begin{bmatrix}
        \lambda_1 x_1 \\
        \lambda_2 x_2
    \end{bmatrix} \bullet \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} \\
    & = \lambda_1 x_1^2 + \lambda_2 y_1^2
\end{align*}
Por lo tanto, $d = D \mathbb{v}’ \bullet \mathbb{v}’$ representa una forma cuadrática que no contiene el término $x_1y_1$. Así, la ecuación \eqref{CUADSINXY} es una ecuación cuadrática en las nuevas variables $x_1$, $y_1$, sin el término $x_1y_1$.

\begin{example}\label{Example:elipse}
    Considere la ecuación cuadrática en dos variables
    \begin{equation}
        5x^2 - 2xy + 5y^2 = 4 \label{AJAKSIJSYWYUQHVVJQUJAJ}
    \end{equation}
    Por el procedimiento anterior, tenemos que expresar \eqref{AJAKSIJSYWYUQHVVJQUJAJ} en la forma
    \begin{equation}
        A\mathbb{v} \bullet \mathbb{v} = d \label{IAJAJAJAJAHAHHHJQJ}
    \end{equation}
    De esta forma,
    $$\begin{bmatrix*}[r] 5 & -1 \\ -1 & 5 \end{bmatrix*} \begin{bmatrix} x \\ y \end{bmatrix} \bullet \begin{bmatrix} x \\ y \end{bmatrix} = 4$$
    donde $A = \begin{bmatrix*}[r] 5 & -1 \\ -1 & 5 \end{bmatrix*}$ y $\mathbb{v} = \begin{bmatrix} x \\ y \end{bmatrix}$. Siguiendo el método expuesto en la página \pageref{metodo_eigen_2x2}, es fácil determinar los eigenvalores de $A$, los cuales están dados por $\lambda_1 = 6$ y $\lambda_2 = 4$, y utilizando las fórmulas del inciso (i), se obtiene que los eigenvectores de $A$ correspondientes a $\lambda_1$ y $\lambda_2$ están dados por
    $$\mathbb{v}_1 = \begin{pmatrix*}[r] 1 \\ -1 \end{pmatrix*} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix*}[r] -1 \\ -1 \end{pmatrix*}$$\newpage\noindent
    Conforme al teorema \ref{theorem_simetrica2}, los eigenvectores obtenidos anteriormente son ortogonales. Para determinar la matriz $Q$, se aplica el teorema \ref{Qorto_vectoresorto}. Así,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \frac{1}{\sqrt{2}} \begin{pmatrix*}[r] 1 \\ -1 \end{pmatrix*} & & = \frac{1}{\sqrt{2}} \begin{pmatrix*}[r] -1 \\ -1 \end{pmatrix*}
    \end{align*}
    Tomando $Q = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$, entonces $Q^T = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
        -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
    \end{bmatrix*}$. Por tanto,
    \begin{align*}
        Q^TAQ & = \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            5 & -1 \\
            -1 & 5
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            3\sqrt{2} & -3\sqrt{2} \\
            -2\sqrt{2} & -2\sqrt{2}
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{2}} & - \dfrac{1}{\sqrt{2}} \\[3mm]
            -\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            6 & 0 \\
            0 & 4
        \end{bmatrix}
    \end{align*}
    De este modo,
    $$D = Q^T A Q$$
    y en consecuencia,
    $$QDQ^T = A$$
    Al sustituir en \eqref{IAJAJAJAJAHAHHHJQJ}, se obtiene
    \begin{align*}
        4 & = QDQ^T \mathbb{v} \bullet \mathbb{v} \\
        & = DQ^T\mathbb{v} \bullet Q^T\mathbb{v} \\
        & = D\mathbb{v}' \bullet \mathbb{v}'
    \end{align*}
    donde $\mathbb{v}' = \begin{pmatrix} x_1 \\ y_1 \end{pmatrix} = Q^T \mathbb{v}$. Por lo tanto,\sideFigure[\label{ELLIPSEEX_1Y_1}]{
    \begin{tikzpicture}
		\draw[-stealth, thick] (-2.5,0) -- (2.5,0) node[below left] {$x$};
        \draw[-stealth, thick] (0,-2.5) -- (0,2.5) node[below left] {$y$};
        % definición de las variables
        \begin{scope}[rotate=-45]
		    \tikzmath{
			    % parámetros de la elipse (multiplicados por 1.75)
			    \a = 7*sqrt(6)/12; \b = 7/4;
			    \c = sqrt(abs((\a)^2 - (\b)^2));
			    % ec. de la elipse parametrizada
			    function f(\x) {
				    return \a*\b/sqrt((\a*sin(\x))^2 + (\b*cos(\x))^2);
			    };
		    };
		    % def. de las coordenadas importantes
		    \coordinate (C) at (0,0);
		    \coordinate (F1) at (-\c,0);
		    \coordinate (F2) at (+\c,0);
		    \coordinate (A1) at (-\a,0);
		    \coordinate (A2) at (+\a,0);
		    \coordinate (B1) at (0,+\b);
		    \coordinate (B2) at (0,-\b);
            \coordinate (X) at (1,1);
            \coordinate (X2) at (1,0);
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \node[anchor = base, below, rotate = -45] at (F1) {$F_1$};
		    \node[anchor = base, below, rotate = -45] at (F2) {$F_2$};
		    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		    % anotaciones			
		    {[on background layer]
		    \draw[gray!50] (A2) -- ++(0,-2.25);
		    \draw[gray!50, shorten <= 5mm] (F2) -- ++(0,-1.55);
		    \draw[gray!50]
			    ([yshift = -2cm]A2)
				    --node[black, fill = white, rotate = -45] {$a$}
			    ([yshift = -2cm]C);
		    \draw[gray!50]
			    ([yshift = -1.3cm]F2)
				    --node[black, fill = white, rotate = -45] {$c$}
			    ([yshift = -1.3cm]C);
		    \draw[gray!50]
			    (B1) -- ++(-1.25*\a - 0.5,0)
			    (-1.25*\a - 0.2,0) --node[black, fill = white, rotate = -45] {$b$} ++(0,\b);
		    }
            \pic[draw, -latex, angle eccentricity=1.5,
            ] {angle = X--C--X2};
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            % eje horizontal
		    \draw[-stealth, dash pattern=on 3pt off 3pt, thick] (-2.5,0) -- (2.5,0) node[below left, rotate = -45] {$x_1$};
            % eje vertical
		    \draw[-stealth, dash pattern=on 3pt off 3pt, thick] (0,-2.5) -- (0,2.5) node[below left, rotate = -45] {$y_1$};
            \node[fill=white] at (0.5,-0.5) {$315^{\circ}$};
            % elección de un punto arbitrario sobre la elipse
		    \tikzmath{ \th = 30; }
		    \coordinate (P) at (\th:{f(\th)});
            \draw (F1) -- (P) -- (F2);
		    % elipse
		    \draw
			    plot[domain = 0:360, samples = 120] (\x:{f(\x)});
		    % puntos
		    \foreach \p in {C,A1,A2,B1,B2,F1,F2,P}
			    \fill (\p) circle (1.75pt);
        \end{scope}
    \end{tikzpicture}
    }
    $$\begin{bmatrix}
        6 & 0 \\
        0 & 4
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} \bullet \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} = 4$$
    lo que conduce a
    $$6x_1^2 + 4y_1^2 = 4$$
    o bien
    $$\frac{x_1^2}{\left(\sqrt{\dfrac{2}{3}}\right)^2} + \frac{y_1^2}{1^2} = 1$$
    que corresponde a una elipse con $\displaystyle a = \sqrt{\frac{2}{3}}$ y $b = 1$. Desde una perspectiva geométrica, se representa la figura \ref{ELLIPSEEX_1Y_1}.
\end{example}

\begin{example}
    Grafique la curva
    \begin{equation}
        6x^2 + 5xy - 6y^2 = 7 \label{UYHDUYFHUEGFUYGEUGFUYGHTRHF}
    \end{equation}
    \solucion Análogo al ejercicio anterior, tenemos que expresar \eqref{UYHDUYFHUEGFUYGEUGFUYGHTRHF} en la forma
    \begin{equation}
        A\mathbb{v} \bullet \mathbb{v} = d \label{JHEFRFRGUJHRHGUHRUR}
    \end{equation}
    De esta forma,
    $$\begin{bmatrix*}[r]
        6 & \dfrac{5}{2} \\[2mm]
        \dfrac{5}{2} & - 6
    \end{bmatrix*} \begin{bmatrix}
        x \\
        y
    \end{bmatrix} \bullet \begin{bmatrix}
        x \\
        y
    \end{bmatrix} = 7$$
    siendo $A = \begin{bmatrix*}[r]
        6 & \dfrac{5}{2} \\[2mm]
        \dfrac{5}{2} & - 6
    \end{bmatrix*}$ que es una matriz simétrica. Siguiendo el método expuesto en la página \pageref{metodo_eigen_2x2}, es fácil determinar los eigenvalores de $A$, los cuales están dados por $\lambda_1 = 13/2$ y $\lambda_2 = -13/2$, y utilizando las fórmulas del inciso (ii), se obtiene que los eigenvectores de $A$ correspondientes a $\lambda_1$ y $\lambda_2$ están dados por
    $$\mathbb{v}_1 = \begin{pmatrix} \dfrac{5}{2} \\[3mm] \dfrac{1}{2} \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix*}[r] \dfrac{5}{2} \\[3mm] - \dfrac{25}{2} \end{pmatrix*}$$
    Multiplicando por $2$ ambos vectores, obtenemos:
    $$\mathbb{v}_1 = \begin{pmatrix} 5 \\ 1 \end{pmatrix} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix*}[r] 5 \\ - 25 \end{pmatrix*}$$
    Conforme al teorema \ref{theorem_simetrica2}, los eigenvectores obtenidos anteriormente son ortogonales. Para determinar la matriz $Q$, se aplica el teorema \ref{Qorto_vectoresorto}. Así,
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 & \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{v}_2 \|} \mathbb{v}_2 \\
        & = \frac{1}{\sqrt{26}} \begin{pmatrix} 5 \\ 1 \end{pmatrix} & & = \frac{1}{5\sqrt{26}} \begin{pmatrix*}[r] 5 \\ -25 \end{pmatrix*}
    \end{align*}
    Así, $Q = \begin{bmatrix*}[r]
        \dfrac{5}{\sqrt{26}} & \dfrac{1}{\sqrt{26}} \\[3mm]
        \dfrac{1}{\sqrt{26}} & -\dfrac{5}{\sqrt{26}}
    \end{bmatrix*}$, entonces $Q^T = \begin{bmatrix*}[r]
        \dfrac{5}{\sqrt{26}} & \dfrac{1}{\sqrt{26}} \\[3mm]
        \dfrac{1}{\sqrt{26}} & -\dfrac{5}{\sqrt{26}}
    \end{bmatrix*}$. Por tanto
    \begin{align*}
        Q^TAQ & = \begin{bmatrix*}[r]
            \dfrac{5}{\sqrt{26}} & \dfrac{1}{\sqrt{26}} \\[3mm]
            \dfrac{1}{\sqrt{26}} & -\dfrac{5}{\sqrt{26}}
        \end{bmatrix*} \begin{bmatrix*}[r]
            6 & \dfrac{5}{2} \\[2mm]
            \dfrac{5}{2} & - 6
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{5}{\sqrt{26}} & \dfrac{1}{\sqrt{26}} \\[3mm]
            \dfrac{1}{\sqrt{26}} & -\dfrac{5}{\sqrt{26}}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            \dfrac{5\sqrt{26}}{4} & \dfrac{\sqrt{26}}{4} \\[3mm]
            - \dfrac{\sqrt{26}}{4} & \dfrac{5\sqrt{26}}{4}
        \end{bmatrix*} \begin{bmatrix*}[r]
            \dfrac{5}{\sqrt{26}} & \dfrac{1}{\sqrt{26}} \\[3mm]
            \dfrac{1}{\sqrt{26}} & -\dfrac{5}{\sqrt{26}}
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            \dfrac{13}{2} & \phantom{-} 0 \\[2mm]
            0 & - \dfrac{13}{2}
        \end{bmatrix}
    \end{align*}
    De este modo,
    $$D = Q^T A Q$$
    y en consecuencia,
    $$QDQ^T = A$$
    Al sustituir en \eqref{JHEFRFRGUJHRHGUHRUR}, se obtiene
    \begin{align*}
        7 & = QDQ^T \mathbb{v} \bullet \mathbb{v} \\
        & = DQ^T\mathbb{v} \bullet Q^T\mathbb{v} \\
        & = D\mathbb{v}' \bullet \mathbb{v}'
    \end{align*}\newpage\noindent
    donde $\mathbb{v}' = \begin{pmatrix} x_1 \\ y_1 \end{pmatrix} = Q^T \mathbb{v}$. Por lo tanto,\sideFigure[\label{HIPERBOLAAX_1Y_1}]{
    \begin{tikzpicture}
        \draw[-stealth, thick] (-2.5,0) -- (2.5,0) node[below left] {$x$};
        \draw[-stealth, thick] (0,-2.5) -- (0,2.5) node[below left] {$y$};
        \begin{scope}[rotate=22.6]
            \draw[-stealth, dash pattern=on 3pt off 3pt, thick] (-2.5,0) -- (2.5,0) node[below left, rotate = 22.6] {$x_1$};
            % eje vertical
		    \draw[-stealth, dash pattern=on 3pt off 3pt, thick] (0,-2.5) -- (0,2.5) node[below left, rotate = 22.6] {$y_1$};
            % Hipérbola en dos trozos
            \draw[domain=-1.2:1.2, smooth, variable=\x] plot ({14*cosh(\x)/13}, {14*sinh(\x)/13});
            \draw[domain=-1.2:1.2, smooth, variable=\x] plot ({-14*cosh(\x)/13}, {14*sinh(\x)/13});
            % Puntos importantes
            \draw[gray!50, shorten <= 5mm] ({-sqrt(2*(14/13)^2)},0) -- ++(0,-1.75);
            \draw[gray!50]
			    ($({-sqrt(2*(14/13)^2)},0) + (0,-1.5)$)
				    --node[black, fill = white, rotate = 22.6] {$c$}
			    (0,-1.5);
            \filldraw (0,0) circle (1.75pt);
            \filldraw (-14/13,0) circle (1.75pt);
            \filldraw (14/13,0) circle (1.75pt);
            \filldraw ({-sqrt(2*(14/13)^2)},0) circle (1.75pt) node[below,rotate=22.6] {$F_1$};
            \filldraw ({sqrt(2*(14/13)^2)},0) circle (1.75pt) node[below,rotate=22.6] {$F_2$};
            \filldraw (1.5, 1.0441439) circle (1.75pt);
            \draw ({sqrt(2*(14/13)^2)},0) -- (1.5, 1.0441439) -- ({-sqrt(2*(14/13)^2)},0);
        \end{scope}
    \end{tikzpicture}
    }
    $$\begin{bmatrix}
        \dfrac{13}{2} & \phantom{-} 0 \\[2mm]
        0 & - \dfrac{13}{2}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} \bullet \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix} = 7$$
    lo que conduce a
    $$\frac{13}{2} x_1^2 - \frac{13}{2} y_1^2 = 7$$
    o bien
    $$\frac{x_1^2}{\left(\sqrt{\dfrac{14}{13}}\right)^2} - \frac{y_1^2}{\left(\sqrt{\dfrac{14}{13}}\right)^2} = 1$$
    que corresponde a una hipérbola con $\displaystyle a = \sqrt{\frac{14}{13}}$ y $\displaystyle b = \sqrt{\frac{14}{13}}$. Desde una perspectiva geométrica, se representa la figura \ref{HIPERBOLAAX_1Y_1}.
\end{example}

Observemos que, en general, $Q$ es una matriz de rotación dada por
$$Q = \begin{bmatrix*}[r]
    \cos \theta & - \sen \theta \\
    \sen \theta & \cos \theta
\end{bmatrix*}$$
y con ayuda de los coeficientes de la ecuación cuadrática podemos determinar el ángulo $\theta$. Para probarlo, consideremos $Q = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}$ una matriz ortogonal real. Al ser $Q$ real y ortogonal, entonces se sigue que
\begin{align*}
    1 & = \operatorname{det} QQ^{-1} \\
    & = \operatorname{det} QQ^T \\
    & = \operatorname{det} Q \operatorname{det} Q^T \\
    & = \operatorname{det} Q \operatorname{det} Q \\
    & = (\operatorname{det} Q)^2
\end{align*}
Entonces $\operatorname{det} Q = \pm 1$. Si $\operatorname{det} Q = - 1$, se pueden intercambiar los renglones de $Q$ para hacer el determinante de esta nueva $Q$ igual a $1$. De esta forma,
$$\operatorname{det} Q = ad - bc = 1$$
Por otro lado,
$$Q^TQ = \begin{bmatrix}
    a & c \\
    b & d
\end{bmatrix} \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix} = \begin{bmatrix}
    a^2 + c^2 & ab + cd \\
    ab + cd & b^2 + d^2
\end{bmatrix} = \begin{bmatrix}
    1 & 0 \\
    0 & 1
\end{bmatrix}$$
Observemos que de todo lo anterior, obtenemos el siguiente sistema
\begin{align}
    a^2 + c^2 & = 1 \label{GENQROT1} \\
    ab + cd & = 0 \label{GENQROT2} \\
    b^2 + d^2 & = 1 \label{GENQROT3}
\end{align}
De la ecuación \eqref{GENQROT2}, podemos deducir que $ab = -cd$. Si $b \neq 0$, entonces podemos dividir ambos lados de la ecuación entre $b$ para obtener
$$a = -\dfrac{cd}{b}$$
Pero si $b = 0$, entonces $cd = 0$, y como $\operatorname{det} Q = ad - bc = 1$, esto implicaría que $a$ y $d$ son ambos $1$ o $-1$, pero como $a \geq 0$, ambos deben ser $1$. Si $b \neq 0$, entonces sustituyendo $a$ en la ecuación \eqref{GENQROT1}, obtenemos
$$\left(-\frac{cd}{b}\right)^2 + c^2 = 1$$\newpage\noindent
es decir,
$$\frac{c^2d^2}{b^2} + c^2 = 1$$
Multiplicando ambos lados por $b^2$ y reorganizando los términos, obtenemos
$$c^2(d^2 + b^2) = b^2$$
Pero sabemos que $d^2 + b^2 = 1$ por la ecuación \eqref{GENQROT3}, así que
$$c^2 = b^2$$
Esto significa que $c = \pm b$. Pero como $ab + cd = 0$, y $a$ y $d$ tienen el mismo signo (ya que su producto es positivo), $c$ y $b$ deben tener signos opuestos. Por lo tanto, $c = -b$.

Al considerar la ecuación \eqref{GENQROT2}, obtenemos que $ab = -cd$. Si $b \neq 0$, podemos dividir ambos lados de esta ecuación entre $b$, obteniendo
$$a = -\frac{c}{b}d$$
Ahora, si sustituimos $c = -b$, la ecuación se convierte en
$$a = \frac{b}{b}d$$
Por lo tanto, $a = d$.

Finalmente, como $a^2 + c^2 = 1$, podemos decir que $a = \cos \theta$ y $c = \sen \theta$ para algún $\theta \in [0, 2\pi)$. Por lo tanto,
$$Q = \begin{bmatrix*}[r] \cos \theta & - \sen \theta \\ \sen \theta & \cos \theta \end{bmatrix*}$$
Además,
\begin{enumerate}[label=\roman*)]
    \item Si $a \geq 0$ y $c > 0$, entonces $0 < \theta \leq \dfrac{\pi}{2}$ y $\theta = \cos^{-1} a$.
    \item Si $a \geq 0$ y $c < 0$, entonces $\dfrac{3\pi}{2} \leq \theta < 2\pi$ y $\theta = 2\pi - \cos^{-1} a$.
    \item Si $a \leq 0$ y $c > 0$, entonces $\dfrac{\pi}{2} \leq \theta < \pi$ y $\theta = \cos^{-1} a$.
    \item Si $a \leq 0$ y $c < 0$, entonces $\pi < \theta \leq \dfrac{3\pi}{2}$ y $\theta = 2\pi - \cos^{-1} a$.
    \item Si $a = 1$ y $c = 0$, entonces $\theta = \cos^{-1} (1) = \sen^{-1} (0) = 0$.
    \item Si $a = -1$ y $c = 0$, entonces $\theta = \cos^{-1} (-1) = \pi$.
\end{enumerate}

\begin{example}
    Si consideramos la matriz $Q$ obtenida en el ejemplo \ref{Example:elipse}, se tiene que
    $$a = \frac{1}{\sqrt{2}} > 0 \quad \text{ y } \quad c = - \frac{1}{\sqrt{2}} < 0$$
    Por lo tanto, utilizando el inciso ii) de lo mencionado anteriormente, se obtiene que
    \begin{align*}
        \theta & = 2\pi - \cos^{-1} a \\
        & = 2\pi - \frac{\pi}{4} \\
        & = \frac{7\pi}{4} \\
        & = 315^{\circ}
    \end{align*}
    Por lo tanto, \eqref{AJAKSIJSYWYUQHVVJQUJAJ} es la ecuación de una elipse estándar rotada un ángulo de $315^{\circ}$ (o $45^{\circ}$ en el sentido de las manecillas del reloj), lo cual coincide exactamente con lo mostrado en la figura \ref{ELLIPSEEX_1Y_1}.
\end{example}

\newpage

\begin{theorem}[Teorema de los ejes principales]
    Dada la ecuación cuadrática
    \begin{equation}
        ax^2 + bxy + cy^2 = d \label{HSGDGSHFGJHSHFJGDJHSFGJHDGJFGSDJU}
    \end{equation}
    en las variables $x$ e $y$. Entonces existe un único número $\theta \in [0, 2\pi]$ tal que la ecuación \eqref{HSGDGSHFGJHSHFJGDJHSFGJHDGJFGSDJU} se puede escribir en la forma
    $$\lambda_1 x_1^2 + \lambda_2 y_1^2 = d$$
    donde $x_1$ y $x_2$ son los ejes obtenidos al rotar los ejes $x$ e $y$ un ángulo $\theta$ en sentido contrario al de las manecillas del reloj y $\lambda_1$ y $\lambda_2$ son los eigenvalores de la matriz $A = \begin{bmatrix}
        a & \dfrac{b}{2} \\[2mm]
        \dfrac{b}{2} & c
    \end{bmatrix}$. Los ejes $x_1$ e $y_1$ se denominan \emph{ejes principales} de la gráfica de la ecuación cuadrática \eqref{HSGDGSHFGJHSHFJGDJHSFGJHDGJFGSDJU}.
\end{theorem}

\begin{definition}
    Una forma cuadrática en las variables $x_1$, $x_2$, $\dots$, $x_n$ es una expresión dada por
    $$A\mathbb{x} \bullet \mathbb{x} = F(x_1, x_2, \dots, x_n)$$
    siendo $A$ una matriz simétrica y $\mathbb{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$.
\end{definition}

\begin{example}
    Grafique la ecuación
    \begin{equation}
        5x^2 + 8xy + 5y^2 + 4xz + 4yz + 2z^2 = 100 \label{KAJAJAJSJSJJSJSSJJHVVGQGG}
    \end{equation}
    \solucion Dada la ecuación cuadrática en tres variables \eqref{KAJAJAJSJSJJSJSSJJHVVGQGG}, queremos construir la matriz simétrica $A$ que representa los coeficientes de las variables. Para ello:
    \begin{itemize}
        \item Los elementos de la diagonal de $A$ corresponden a los coeficientes de los términos cuadráticos de las variables $x^2$, $y^2$ e $z^2$.
        \item Los elementos fuera de la diagonal son el producto de las variables cruzadas dividido entre $2$.
    \end{itemize}
    Para este caso, la entrada $a_{12}$ corresponde al coeficiente de $xy$ en la ecuación original, que es $8/2 = 4$. La entrada $a_{13}$ corresponde al coeficiente de $xz$, que es $4/2 = 2$. Siguiendo este patrón, construimos la matriz $A$:
    \begin{nscenter}
        \begin{tikzpicture}
            \node (A) at (0,0) {
            $\begin{bmatrix}
                5 & 4 & 2 \\
                4 & 5 & 2 \\
                2 & 2 & 2
            \end{bmatrix}$
            };
            \node[yshift=5pt] (ya) at (A.north) {$y$};
            \node[xshift=-16pt] at (ya) {$x$};
            \node[xshift=16pt] at (ya) {$z$};
            \node[xshift=-5pt] (yi) at (A.west) {$y$};
            \node[yshift=12pt] at (yi) {$x$};
            \node[yshift=-12pt] at (yi) {$z$};
        \end{tikzpicture}
    \end{nscenter}
    Por lo tanto, podemos expresar la ecuación \eqref{KAJAJAJSJSJJSJSSJJHVVGQGG} como
    $$\begin{bmatrix}
        5 & 4 & 2 \\
        4 & 5 & 2 \\
        2 & 2 & 2
    \end{bmatrix} \begin{bmatrix}
        x \\
        y \\
        z
    \end{bmatrix} \bullet \begin{bmatrix}
        x \\
        y \\
        z
    \end{bmatrix} = 100$$
    o simplemente
    $$A \mathbb{x} \bullet \mathbb{x} = 100$$
    Ahora, calculemos los valores propios de $A$, de esta forma
    \begin{align*}
        \operatorname{det} (A - \lambda I_3) & = \begin{vmatrix}
            5 - \lambda & 4 & 2 \\
            4 & 5 - \lambda & 2 \\
            2 & 2 & 2 - \lambda
        \end{vmatrix} \\
        & = - \lambda^3 + 12\lambda^2 - 21\lambda + 10
    \end{align*}\newpage\noindent
    Por lo tanto, los eigenvalores de $A$ son las raíces del polinomio
    $$- \lambda^3 + 12\lambda^2 - 21\lambda + 10 = 0$$
    pero observemos que la anterior expresión es igual a
    $$-(\lambda - 1)(\lambda - 1)(\lambda - 10) = 0$$
    Por lo que, $\lambda_1 = 1$, $\lambda_2 = 1$ y $\lambda_3 = 10$ son las raíces del polinomio característico. Ahora, calculemos los eigenvectores correspondientes a cada $\lambda$. Empecemos con $\lambda_3 = 10$, así obtenemos el siguiente sistema:
    \begin{align*}
        - 5a + 4b + 2c & = 0 \\
        4a - 5b + 2c & = 0 \\
        2a + 2b - 8c & = 0
    \end{align*}
    Si sumamos la primer y segunda ecuación, obtenemos que
    $$-a - b + 4c = 0$$
    y multiplicando por $-2$, se sigue que
    $$2a + 2b - 8c = 0$$
    que es justo la tercer ecuación. Así que solo basta resolver la primer y segunda ecuación. Para ello, podemos restar la segunda ecuación de la primera, obteniendo así
    $$-9a + 9b = 0$$
    Entonces $a = b$, y sustituyendo el valor de $b$ en la primer ecuación obtenemos que $-a + 2c = 0$, es decir, $c = \dfrac{1}{2}a$. Por lo tanto, si $a = 2$
    $$\mathbb{v}_3 = \begin{pmatrix}
        a \\
        a \\[2mm]
        \dfrac{1}{2}a
    \end{pmatrix} = \begin{pmatrix}
        2 \\
        2 \\
        1
    \end{pmatrix}$$
    Procediendo de manera análoga, para $\lambda_1 = 1$, obtenemos el siguiente sistema:
    \begin{align*}
        4a + 4b + 2c & = 0 \\
        4a + 4b + 2c & = 0 \\
        2a + 2b + c & = 0
    \end{align*}
    Observemos que en este caso solo basta resolver la tercer ecuación
    $$2a + 2b + c = 0$$
    Entonces $c = -2a - 2b$, por lo que
    $$\mathbb{v}_1 = \begin{pmatrix}
        a \\
        b \\
        -2a -2b
    \end{pmatrix} = \begin{pmatrix}
        \phantom{-} a \\
        \phantom{-} 0 \\
        -2a
    \end{pmatrix} + \begin{pmatrix}
        \phantom{-} 0 \\
        \phantom{-} b \\
        -2b
    \end{pmatrix}$$
    Por lo tanto,
    $$\mathbb{v}_1 = \begin{pmatrix*}[r]
        1 \\
        0 \\
        -2
    \end{pmatrix*} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix*}[r]
        0 \\
        1 \\
        -2
    \end{pmatrix*}$$
    Observemos que en este caso, $\mathbb{v}_1$ y $\mathbb{v}_2$ no son ortogonales, pues la matriz $A$ tiene eigenvalores no distintos. Así que debemos aplicar el proceso de Gram-Schmid como sigue: Sea $\mathbb{v}_1 = \begin{pmatrix*}[r]
        1 \\
        0 \\
        -2
    \end{pmatrix*}$, entonces $\| \mathbb{v}_1 \| = \sqrt{5}$, por lo que
    \begin{align*}
        \hat{\mathbb{u}}_1 & = \frac{1}{\| \mathbb{v}_1 \|} \mathbb{v}_1 \\
        & = \frac{1}{\sqrt{5}} \begin{pmatrix*}[r]
            1 \\
            0 \\
            -2
        \end{pmatrix*}
    \end{align*}\newpage\noindent
    Luego,
    \begin{align*}
        \mathbb{u}_2 & = \mathbb{v}_2 - (\mathbb{v}_1 \bullet \hat{\mathbb{u}}_1) \hat{\mathbb{u}}_1 \\
        & = \begin{pmatrix*}[r]
            0 \\
            1 \\
            -2
        \end{pmatrix*} - \left[ \begin{pmatrix*}[r]
            0 \\
            1 \\
            -2
        \end{pmatrix*} \bullet \frac{1}{\sqrt{5}} \begin{pmatrix*}[r]
            1 \\
            0 \\
            -2
        \end{pmatrix*} \right] \frac{1}{\sqrt{5}} \begin{pmatrix*}[r]
            1 \\
            0 \\
            -2
        \end{pmatrix*} \\
        & = \begin{pmatrix*}[r]
            0 \\
            1 \\
            -2
        \end{pmatrix*} - \frac{4}{5} \begin{pmatrix*}[r]
            1 \\
            0 \\
            -2
        \end{pmatrix*}
    \end{align*}
    Por lo tanto, $\mathbb{u}_2 = \dfrac{1}{5} \begin{pmatrix*}[r]
        -4 \\
        5 \\
        -2
    \end{pmatrix*}$. Entonces $\| \mathbb{u}_2 \| = \dfrac{3\sqrt{5}}{5}$, por lo que
    \begin{align*}
        \hat{\mathbb{u}}_2 & = \frac{1}{\| \mathbb{u}_2 \|} \mathbb{u}_2 \\
        & = \frac{1}{3\sqrt{5}} \begin{pmatrix*}[r]
            -4 \\
            5 \\
            -2
        \end{pmatrix*}
    \end{align*}
    Observemos que $\mathbb{v}_3 \bullet \hat{\mathbb{u}}_1 = 0$ y $\mathbb{v}_3 \bullet \hat{\mathbb{u}}_2 = 0$, entonces basta normalizar el vector $\mathbb{v}_3$ como sigue
    \begin{align*}
        \hat{\mathbb{u}}_3 & = \frac{1}{\| \mathbb{v}_3 \|} \mathbb{v}_3 \\
        & = \frac{1}{3} \begin{pmatrix}
            2 \\
            2 \\
            1
        \end{pmatrix}
    \end{align*}
    Para determinar la matriz $Q$, podemos emplear el teorema \ref{Qorto_vectoresorto}. Así, sea $Q = \begin{bmatrix}
        \phantom{-} \dfrac{1}{\sqrt{5}} & - \dfrac{4}{3\sqrt{5}} & \dfrac{2}{3} \\[3mm]
        \phantom{-} 0 & \phantom{-} \dfrac{5}{3\sqrt{5}} & \dfrac{2}{3} \\[3mm]
        - \dfrac{2}{\sqrt{5}} & - \dfrac{2}{3\sqrt{5}} & \dfrac{1}{3}
    \end{bmatrix}$, entonces $Q^T = \begin{bmatrix}
        \phantom{-} \dfrac{1}{\sqrt{5}} & 0 & -\dfrac{2}{\sqrt{5}} \\[3mm]
        -\dfrac{4}{3\sqrt{5}} & \dfrac{5}{3\sqrt{5}} & - \dfrac{2}{3\sqrt{5}} \\[3mm]
        \phantom{-} \dfrac{2}{3} & \dfrac{2}{3} & \phantom{-} \dfrac{1}{3}
    \end{bmatrix}$. Por lo tanto,
    \begin{align*}
        Q^T A Q & = \begin{bmatrix}
            \phantom{-} \dfrac{1}{\sqrt{5}} & 0 & -\dfrac{2}{\sqrt{5}} \\[3mm]
            -\dfrac{4}{3\sqrt{5}} & \dfrac{5}{3\sqrt{5}} & - \dfrac{2}{3\sqrt{5}} \\[3mm]
            \phantom{-} \dfrac{2}{3} & \dfrac{2}{3} & \phantom{-} \dfrac{1}{3}
        \end{bmatrix} \begin{bmatrix}
            5 & 4 & 2 \\
            4 & 5 & 2 \\
            2 & 2 & 2
        \end{bmatrix} \begin{bmatrix}
            \phantom{-} \dfrac{1}{\sqrt{5}} & - \dfrac{4}{3\sqrt{5}} & \dfrac{2}{3} \\[3mm]
            \phantom{-} 0 & \phantom{-} \dfrac{5}{3\sqrt{5}} & \dfrac{2}{3} \\[3mm]
            - \dfrac{2}{\sqrt{5}} & - \dfrac{2}{3\sqrt{5}} & \dfrac{1}{3}
        \end{bmatrix} \\
        & = \begin{bmatrix}
            \phantom{-} \dfrac{\sqrt{5}}{5} & 0 & - \dfrac{2\sqrt{5}}{5} \\[3mm]
            -\dfrac{4\sqrt{5}}{15} & \dfrac{\sqrt{5}}{3} & - \dfrac{2\sqrt{5}}{15} \\[3mm]
            \phantom{-} \dfrac{20}{3} & \dfrac{20}{3} & \phantom{-} \dfrac{10}{3}
        \end{bmatrix} \begin{bmatrix}
            \dfrac{2}{3} & \phantom{-} \dfrac{1}{\sqrt{5}} & - \dfrac{4}{3\sqrt{5}} \\[3mm]
            \dfrac{2}{3} & \phantom{-} 0 & \phantom{-} \dfrac{5}{3\sqrt{5}} \\[3mm]
            \dfrac{1}{3} & - \dfrac{2}{\sqrt{5}} & - \dfrac{2}{3\sqrt{5}}
        \end{bmatrix} \\
        & = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 10
        \end{bmatrix}
    \end{align*}
    Por lo tanto,
    $$D = Q^T A Q$$
    de donde se sigue que
    $$QDQ^T = A$$\newpage\sideFigure[\label{ELLIPSX1Y1}]{\centering \begin{tikzpicture}
    \pgfmathsetmacro{\p}{1.0}
    \pgfmathsetmacro{\q}{0.5}
        \begin{axis}[
        xlabel = {$x$},
        ylabel = {$y$},
        zlabel = {$z$},
        view = {60}{30},
        domain = 0 : pi,
        y domain = 0 : 2 * pi,
        z buffer = sort,
        unit vector ratio = 1 1,
        hide axis,
        width=9cm,
        height=9cm,
            xmin = -1,
            xmax =  1,
            ymin = -1,
            ymax =  2,
            zmin = -1,
            zmax =  1.5,
        colormap={}{
            color=(gray) color=(gray)
        },
        declare function = {
            xp(\x, \y) = sin(deg(\x)) * cos(deg(\y));
            yp(\x, \y) = \p * sin(deg(\x)) * sin(deg(\y));
            zp(\x, \y) = \q * cos(deg(\x));
            },
        ]
            \addplot3[patch, fill = gray!20] ({xp(x, y)}, {yp(x, y)}, {zp(x, y)});
            \draw[-stealth] (1, 0, 0) -- (2, 0, 0) node[right] {$x_1$};
            \draw (-1.215, 0, 0) -- (-2, 0, 0);
            \draw[-stealth] (0, \p + 0.05, 0) -- (0, 1.5, 0) node[right] {$y_1$};
            \draw (0, -\p - 0.05, 0) -- (0, -2, 0);
            \draw[-stealth] (0, 0, \q) -- (0, 0, 1.5) node[above] {$z_1$};
            \draw (0, 0, -\q - 0.27) -- (0, 0, -1.5);
        \end{axis}
    \end{tikzpicture}
    }
    \noindent Ahora bien, de la ecuación cuadrática \eqref{KAJAJAJSJSJJSJSSJJHVVGQGG}, tenemos que
    \begin{align*}
        100 & = A\mathbb{x} \bullet \mathbb{x} \\
        & = QDQ^T \mathbb{x} \bullet \mathbb{x} \\
        & = D \left( Q^T \mathbb{x} \right) \bullet \left( Q^T \mathbb{x} \right) \\
        & = D \mathbb{x}' \bullet \mathbb{x}'
    \end{align*}
    donde $\mathbb{x}' = Q^T \mathbb{x}$. Por lo tanto
    $$\begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 10
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        y_1 \\
        z_1
    \end{bmatrix} \bullet \begin{bmatrix}
        x_1 \\
        y_1 \\
        z_1
    \end{bmatrix} = 100$$
    lo que conduce a
    $$x_1^2 + y_1^2 + 10z_1^2 = 100$$
    o bien
    $$\frac{x_1^2}{10^2} + \frac{y_1^2}{10^2} + \frac{z_1^2}{\left( \sqrt{10} \right)^2} = 1$$
    que corresponde a una elipsoide centrada en el origen como se muestra en la figura \ref{ELLIPSX1Y1}.
\end{example}

\section{Forma canónica de Jordan}

Ya se ha visto que las matrices de $n \times n$ con $n$ eigenvectores linealmente independientes se pueden expresar en una forma especialmente sencilla por medio de una transformación de semejanza. Por suerte, como “la mayor parte” de los polinomios tienen raíces diferentes, “la mayor parte” de las matrices tendrán eigenvalores distintos. Sin embargo, las matrices que no son diagonalizables (es decir, que no tienen $n$ eigenvectores linealmente independientes) surgen en la práctica. En este caso, aún es posible demostrar que la matriz es semejante a otra, una matriz más sencilla, pero la nueva matriz no es diagonal y la matriz de transformación $C$ es más difícil de obtener.

Para un análisis detallado de este caso, comenzaremos examinando un tipo de matrices definidas previamente, pero que no habían recibido suficiente atención hasta ahora: las matrices nilpotentes. Por ejemplo, si tomamos la matriz
$$N_2 = \begin{bmatrix}
    0 & 1 \\
    0 & 0
\end{bmatrix}$$
y luego la multiplicamos por ella misma, obtenemos
$$N_2^2 = \begin{bmatrix}
    0 & 1 \\
    0 & 0
\end{bmatrix}\begin{bmatrix}
    0 & 1 \\
    0 & 0
\end{bmatrix} = \begin{bmatrix}
    0 & 0 \\
    0 & 0
\end{bmatrix}$$
Es decir, $N_2$ es una matriz nilpotente de índice de nilpotencia $2$. Si tomamos por ejemplo, la matriz
$$N_3 = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 0
\end{bmatrix}$$
y luego la multiplicamos por ella misma otras dos veces, obtenemos
$$N_3^3 = \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 0
\end{bmatrix}\begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 0
\end{bmatrix}\begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 0
\end{bmatrix} = \begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{bmatrix}$$\newpage\noindent
En general, la matriz de tamaño $k \times k$, está definida como
\begin{equation}
    N_k = \begin{bmatrix}
        0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 1 & \cdots & 0 \\
        \vdots & & & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 1 \\
        0 & 0 & 0 & \cdots & 0
    \end{bmatrix} \label{JAJJAJAJAJAJHQHHQJQUUAUQUWUAUSISIUAJ}
\end{equation}
Observemos que $N_k$ es la matriz con unos arriba de la diagonal principal y ceros en cualquier otra parte. Para un escalar dado $\lambda$ se define la matriz de bloques de Jordan, denotada por $B(\lambda)$, como
$$B(\lambda) = \lambda I_k + N_k = \begin{bmatrix}
    \lambda & 1 & 0 & \cdots & 0 & 0 \\
    0 & \lambda & 1 & \cdots & 0 & 0 \\
    \vdots & & & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda & 1 \\
    0 & 0 & 0 & \cdots & 0 & \lambda
\end{bmatrix}$$
Es decir, $B(\lambda)$ es la matriz de tamaño $k \times k$ con el escalar $\lambda$ en la diagonal, unos arriba de la diagonal y ceros en cualquier otra parte. Observemos además que se puede tener una matriz de bloques de Jordan de tamaño $1 \times 1$ y toma la forma: $B(\lambda) = \begin{bmatrix} \lambda \end{bmatrix}$, o simplemente $B(\lambda) = \lambda$.


Por último, la matriz de Jordan, denotada por $J$, tiene la forma
$$J = \begin{bmatrix}
    B_1(\lambda_1) & 0 & \cdots & 0 \\
    0 & B_2(\lambda_2) & \cdots & 0 \\
    \vdots & & \ddots & \vdots \\
    0 & 0 & \cdots & B_r(\lambda_r)
\end{bmatrix}$$
donde cada $B_j(\lambda_j)$ es una matriz de bloques de Jordan. Entonces una matriz de Jordan es una matriz que tiene en la diagonal matrices de bloques de Jordan y ceros en cualquier otra parte.

\begin{example}
    Los siguientes ejemplos son matrices de Jordan. Los bloques de Jordan se marcaron con lineas:
    \begin{flushleft} % Es difícil controlar los entornos “enumerate” o “task” para este caso. Por lo que usaré una forma alternativa que se parezca a un entorno “task”
        \quad\textbf{i)}\hspace{0.33em}\(
        \settowidth{\dimen0}{$0$}
        \settoheight{\dimen2}{$0$}
        \setlength{\arraycolsep}{4pt}
        \newcolumntype{C}{%
            >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
            w{c}{\dimen0}
        }
        \renewcommand{\arraystretch}{0}
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
        \left[\,\begin{array}{CCC}
        \cline{1-2}
        \|2 & \?1 & 0 \\
        \|0 & \?2 & 0 \\
        \cline{1-3}
        0 & \?0 & \?4 \\
        \cline{3-3}
        \end{array}\,\right]
        \)
        \quad\textbf{ii)}\hspace{0.33em}\(
        \settowidth{\dimen0}{$0$}
        \settoheight{\dimen2}{$0$}
        \setlength{\arraycolsep}{4pt}
        \newcolumntype{C}{%
            >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
            w{c}{\dimen0}
        }
        \renewcommand{\arraystretch}{0}
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
        \left[\,\begin{array}{CCCCC}
        \cline{1-1}
        \| 3 & \| 0 & 0 & 0 & 0 \\
        \cline{1-4}
        0 & \|3 & 1 & \?0 & 0 \\
        0 & \|0 & 3 & \?1 & 0 \\
        0 & \|0 & 0 & \?3 & 0 \\
        \cline{2-5}
        0 & 0 & 0 & \?0 & \?7 \\
        \cline{5-5}
        \end{array}\,\right]
        \)
        \quad\textbf{iii)}\hspace{0.33em}\(
        \settowidth{\dimen0}{$0$}
        \settoheight{\dimen2}{$0$}
        \setlength{\arraycolsep}{4pt}
        \newcolumntype{C}{%
            >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
            w{c}{\dimen0}
        }
        \renewcommand{\arraystretch}{0}
        \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
        \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
        \left[\,\begin{array}{CCCCCCC}
        \cline{1-2}
        \|4 & \?1 & 0 & 0 & 0 & 0 & 0 \\
        \|0 & \?4 & 0 & 0 & 0 & 0 & 0 \\
        \cline{1-5}
        0 & \?0 & 3 & 1 & \?0 & 0 & 0 \\
        0 & \?0 & 0 & 3 & \?1 & 0 & 0 \\
        0 & \?0 & 0 & 0 & \?3 & 0 & 0 \\
        \cline{3-7}
        0 & 0 & 0 & 0 & \?0 & 5 & \?1 \\
        0 & 0 & 0 & 0 & \?0 & 0 & \?5 \\
        \cline{6-7}
        \end{array}\,\right]
        \)
    \end{flushleft}
\end{example}

\begin{example}
    Las única matrices de Jordan de tamaño $2 \times 2$ son $\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}$ y $\begin{bmatrix} \lambda_1 & 1 \\ 0 & \lambda_2 \end{bmatrix}$. En la primera matriz, los números $\lambda_1$ y $\lambda_2$ pueden ser iguales.
\end{example}

\begin{theorem}\label{FORMCANJORDAN}
    Sea $A$ una matriz real o compleja de $n \times n$. Entonces existe una matriz $C$ compleja e invertible de $n \times n$ tal que
    $$C^{-1} A C = J$$
    donde $J$ es una matriz de Jordan cuyos elementos en la diagonal son los eigenvalores de $A$. Más aún, la matriz de Jordan $J$ es única, salvo el orden en que aparecen los bloques de Jordan.
\end{theorem}

\begin{observation}
    La matriz $C$ del anterior teorema no necesita ser única.
\end{observation}

\newpage

\begin{observation}
    La última afirmación del teorema anterior significa, por ejemplo, que si $A$ es semejante a
    \[
    \settowidth{\dimen0}{$0$}
    \settoheight{\dimen2}{$0$}
    \setlength{\arraycolsep}{4pt}
    \newcolumntype{C}{%
        >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
        w{c}{\dimen0}
    }
    \renewcommand{\arraystretch}{0}
    \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
    \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
    J_1 = \left[\,\begin{array}{CCCCCC}
        \cline{1-2}
        \|2 & \?1 & 0 & 0 & 0 & 0 \\
        \|0 & \?2 & 0 & 0 & 0 & 0 \\
        \cline{1-5}
        0 & \?0 & 3 & 1 & \?0 & 0 \\
        0 & \?0 & 0 & 3 & \?1 & 0 \\
        0 & \?0 & 0 & 0 & \?3 & 0 \\
        \cline{3-6}
        0 & 0 & 0 & 0 & \?0 & \?4 \\
        \cline{6-6}
    \end{array}\,\right]
    \]
    entonces $A$ también es semejante a
    \[
    \settowidth{\dimen0}{$0$}
    \settoheight{\dimen2}{$0$}
    \setlength{\arraycolsep}{4pt}
    \newcolumntype{C}{%
        >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
        w{c}{\dimen0}
    }
    \renewcommand{\arraystretch}{0}
    \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
    \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
    J_2 = \left[\,\begin{array}{CCCCCC}
        \cline{1-3}
        \|3 & 1 & \?0 & 0 & 0 & 0 \\
        \|0 & 3 & \?1 & 0 & 0 & 0 \\
        \|0 & 0 & \?3 & 0 & 0 & 0 \\
        \cline{1-4}
        0 & 0 & \?0 & \?4 & 0 & 0 \\
        \cline{4-6}
        0 & 0 & 0 & \?0 & 2 & \?1 \\
        0 & 0 & 0 & \?0 & 0 & \?2 \\
        \cline{5-6}
    \end{array}\,\right] \quad \text{ y } \quad J_3 = \left[\,\begin{array}{CCCCCC}
        \cline{1-1}
        \|4 & \|0 & 0 & 0 & 0 & 0 \\
        \cline{1-3}
        0 & \|2 & \?1 & 0 & 0 & 0 \\
        0 & \|0 & \?2 & 0 & 0 & 0 \\
        \cline{2-6}
        0 & 0 & \?0 & 3 & 1 & \?0 \\
        0 & 0 & \?0 & 0 & 3 & \?1 \\
        0 & 0 & \?0 & 0 & 0 & \?3 \\
        \cline{4-6}
    \end{array}\,\right]
    \]
    y a otras tres matrices de Jordan. Es decir, los bloques de Jordan reales permanecen iguales pero el orden en el que están escritos puede cambiar.
\end{observation}

\begin{definition}
    La matriz $J$ en el teorema \ref{FORMCANJORDAN} se denomina la forma canónica de Jordan de $A$.
\end{definition}

Ahora se verá el procedimiento para calcular la forma canónica de Jordan de cualquier matriz de $2 \times 2$. Si $A$ tiene dos eigenvectores linealmente independientes, ya sabemos qué hacer. Por lo tanto, el único caso de interés ocurre cuando $A$ tiene solo un eigenvector de multiplicidad algebraica $2$ y multiplicidad geométrica $1$. Es decir, se supone que $A$ tiene un único eigenvector independiente $\mathbb{v}_1$ correspondiente a $\lambda$. Esto es, cualquier vector que no es un múltiplo de $\mathbb{v}_1$ no es un eigenvector.

\begin{theorem}
    Sea $A$ una matriz de tamaño $2 \times 2$ que tiene como eigenvalor a $\lambda$ de multiplicidad algebraica $2$ y multiplicidad geométrica $1$. Entonces existe un vector $\mathbb{v}_2$ que satisface la ecuación
    $$(A - \lambda I_2)\mathbb{v}_2 = \mathbb{v}_1$$
    Al vector $\mathbb{v}_2$ se le denomina eigenvector generalizado de $A$ correspondiente al eigenvalor $\lambda$.
\end{theorem}

\begin{example}
    Determinemos la forma canónica de Jordan de la siguiente matriz:
    $$A = \begin{bmatrix*}[r]
        3 & -2 \\
        8 & -5
    \end{bmatrix*}$$
    Utilizando el método expuesto en la página \pageref{metodo_eigen_2x2}, se deduce que $\lambda = -1$ es un eigenvalor de multiplicidad algebraica $2$. Ahora, determinemos los valores propios. Así, sea $\mathbb{v}_1 = \begin{pmatrix} a \\ b \end{pmatrix}$ un vector propio de $A$ correspondiente a $\lambda = -1$, entonces obtenemos el siguiente sistema
    \begin{align*}
        4a - 2b & = 0 \\
        8a - 4b & = 0
    \end{align*}
    por lo que $b = 2a$. En particular, si $a = 1$, el eigenvector está dado por
    $$\mathbb{v}_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$$\newpage\noindent
    Ahora, determinemos el eigenvector generalizado $\mathbb{v}_2$ de la matriz $A$, correspondiente a $\lambda$. Esto es,
    $$(A - \lambda I_2) \mathbb{v}_2 = \mathbb{v}_1$$
    Sea $\mathbb{v}_2 = \begin{pmatrix} a' \\ b' \end{pmatrix}$, entonces sustituyendo en la anterior ecuación obtenemos
    $$\begin{bmatrix*}[r]
        4 & -2 \\
        8 & -4
    \end{bmatrix*} \begin{pmatrix}
        a' \\
        b'
    \end{pmatrix} = \begin{pmatrix}
        1 \\
        2
    \end{pmatrix}$$
    del cual se obtiene el siguiente sistema
    \begin{align*}
        4a' - 2b' & = 1 \\
        8a' - 4b' & = 2
    \end{align*}
    Observemos que son las ecuaciones son iguales, entonces basta resolver una para encontrar los valores de $a$ y $b$. Si $a' = 0$, entonces $b' = - \dfrac{1}{2}$. Por lo tanto
    $$\mathbb{v}_2 = \begin{pmatrix}
        \phantom{-} 0 \\[1.5mm]
        -\dfrac{1}{2}
    \end{pmatrix}$$
    Ahora bien, sea $C$ la matriz formada por los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$. Es decir,
    \begin{align*}
        C & = \begin{bmatrix}
            \mathbb{v}_1 & \mathbb{v}_2
        \end{bmatrix} \\
        & = \begin{bmatrix}
            1 & \phantom{-} 0 \\[1.5mm]
            2 & - \dfrac{1}{2}
        \end{bmatrix}
    \end{align*}
    Ahora, determinemos $C^{-1}$ mediante la expresión \ref{Jjaksksksisid},
    \begin{align*}
        C^{-1} & = -2 \begin{bmatrix}
            - \dfrac{1}{2} & 0 \\[1.5mm]
            - 2 & 1
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            1 & 0 \\
            4 & -2
        \end{bmatrix*}
    \end{align*}
    De esta forma
    \begin{align*}
        C^{-1} A C & = \begin{bmatrix*}[r]
            1 & 0 \\
            4 & -2
        \end{bmatrix*} \begin{bmatrix*}[r]
            3 & -2 \\
            8 & -5
        \end{bmatrix*} \begin{bmatrix}
            1 & \phantom{-} 0 \\[1.5mm]
            2 & -\dfrac{1}{2}
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            3 & -2 \\
            -4 & 2
        \end{bmatrix*} \begin{bmatrix}
            1 & \phantom{-} 0 \\[1.5mm]
            2 & -\dfrac{1}{2}
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            -1 & 1 \\
            0 & -1
        \end{bmatrix*}
    \end{align*}
    que corresponde a la forma canónica de Jordan de la matriz $A$.
\end{example}

\begin{theorem}
    Sea $A \in \mathcal{M}_{2 \times 2}(\RR)$ y $\lambda$ un eigenvalor de multiplicidad algebraica $2$ y multiplicidad geométrica $1$. Entonces la forma canónica de Jordan de $A$ está dada por
    $$J = \begin{bmatrix}
        \lambda & 1 \\
        0 & \lambda
    \end{bmatrix}$$
    \demostracion Sea $\mathbb{v}_1$ el eigenvector de $A$ correspondiente a $\lambda$. Sea $\mathbb{v}_2$ el eigenvector generalizado de $A$, esto es
    \begin{equation}
        (A - \lambda I_2)\mathbb{v}_2 = \mathbb{v}_1 \label{JHHDEFDHFUDFGUYGFYUGFGFE}
    \end{equation}
    Sea
    $$C = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix}$$
    Al multiplicar por $A$ la anterior expresión, obtenemos que
    \begin{align*}
        AC & = A \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} A\mathbb{v}_1 & A\mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} \lambda \mathbb{v}_1 & A\mathbb{v}_2 \end{bmatrix}
    \end{align*}
    Por lo tanto
    \begin{equation}
        AC = \begin{bmatrix} \lambda \mathbb{v}_1 & A\mathbb{v}_2 \end{bmatrix} \label{HSHYUDHHSJJSHFJHJFGDGFDH}
    \end{equation}
    De la expresión \eqref{JHHDEFDHFUDFGUYGFYUGFGFE}, obtenemos que
    $$A\mathbb{v}_2 - \lambda I_2\mathbb{v}_2 = \mathbb{v}_1$$
    es decir,
    $$A\mathbb{v}_2 = \mathbb{v}_1 + \lambda \mathbb{v}_2$$
    y sustituyendo en \eqref{HSHYUDHHSJJSHFJHJFGDGFDH}, se sigue que
    \begin{align*}
        AC & = \begin{bmatrix} \lambda \mathbb{v}_1 & \mathbb{v}_1 + \lambda \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix} \\
        & = C \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}
    \end{align*}
    Por lo tanto
    $$J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix} = C^{-1} A C$$
    que corresponde a la forma canónica de Jordan de $A$.
\end{theorem}

Aunque no se demostrará este hecho, siempre es posible determinar el número de unos arriba de la diagonal en la forma canónica de Jordan de una matriz $A$ de $n \times n$. Sean $\lambda_1$, $\lambda_2$, $\dots$, $\lambda_k$ eigenvalores (distintos) de $A$ con $r_1$, $r_2$, $\dots$, $r_k$ sus respectivas multiplicidades algebraicas y $s_1$, $s_2$, $\dots$, $s_k$ sus respectivas multiplicidades geométricas. Entonces el número de unos arriba de la diagonal en la forma canónica de Jordan de una matriz $A$ de $n \times n$ está dado por
$$(r_1 - s_1) + (r_2 - s_2) + \cdots + (r_k - s_k) = \sum_{i = 1}^{k} r_i - \sum_{i = 1}^{k} s_i = n - \sum_{i = 1}^{k} s_i$$

\begin{example}
    Sea $(\lambda - 3)^3(\lambda - 4)$ el polinomio característico de $A$. Determinemos las posibles formas canónicas de Jordan de $A$. Observemos que la ecuación característica de $A$ tiene como raíces a $\lambda_1 = 3$ (de multiplicidad algebraica $3$) y a $\lambda_2 = 4$ (de multiplicidad algebraica $1$). La multiplicidad de $\lambda_1$ puede ser $s = 1$, $2$, $3$. Para $\lambda_2$, solo tenemos un caso y es $s = 1$. Notemos que en este caso $A$ es de tamaño $4 \times 4$, por lo que su forma canónica de Jordan es también de tamaño $4 \times 4$. Así, para $\lambda_1 = 3$, si $s = 1$, entonces el número de unos arriba de la diagonal está dado por $r - s = 3 - 1 = 2$ y obtendríamos
    \[
    \settowidth{\dimen0}{$0$}
    \settoheight{\dimen2}{$0$}
    \setlength{\arraycolsep}{4pt}
    \newcolumntype{C}{%
        >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
        w{c}{\dimen0}
    }
    \renewcommand{\arraystretch}{0}
    \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
    \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
    J_1 = \left[\,\begin{array}{CCCC}
        \cline{1-3}
        \|3 & 1 & \?0 & 0 \\
        \|0 & 3 & \?1 & 0 \\
        \|0 & 0 & \?3 & 0 \\
        \cline{1-4}
        0 & 0 & \?0 & \?4 \\
        \cline{4-4}
    \end{array}\,\right]
    \]\newpage\noindent
    Para $\lambda_1 = 3$, si $s = 2$, entonces el número de unos arriba de la diagonal está dado por $r - s = 2 - 1 = 1$ y obtendríamos
    \[
    \settowidth{\dimen0}{$0$}
    \settoheight{\dimen2}{$0$}
    \setlength{\arraycolsep}{4pt}
    \newcolumntype{C}{%
        >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
        w{c}{\dimen0}
    }
    \renewcommand{\arraystretch}{0}
    \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
    \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
    J_2 = \left[\,\begin{array}{CCCC}
        \cline{1-2}
        \|3 & \?1 & 0 & 0 \\
        \|0 & \?3 & 0 & 0 \\
        \cline{1-3}
        0 & \?0 & \?3 & 0 \\
        \cline{3-4}
        0 & 0 & \?0 & \?4 \\
        \cline{4-4}
    \end{array}\,\right]
    \]
    Por último, para $\lambda_1 = 3$, si $s = 1$, entonces el número de unos arriba de la diagonal está dado por $r - s = 1 - 1 = 0$ y obtendríamos
    \[
    \settowidth{\dimen0}{$0$}
    \settoheight{\dimen2}{$0$}
    \setlength{\arraycolsep}{4pt}
    \newcolumntype{C}{%
        >{\vrule width 0pt height \dimexpr\dimen2+\arraycolsep\relax depth \arraycolsep}
        w{c}{\dimen0}
    }
    \renewcommand{\arraystretch}{0}
    \newcommand{\?}[1]{\multicolumn{1}{c|}{#1}}
    \renewcommand{\|}[1]{\multicolumn{1}{|c}{#1}}
    J_3 = \left[\,\begin{array}{CCCC}
        \cline{1-1}
        \|3 & \|0 & 0 & 0 \\
        \cline{1-2}
        0 & \|3 & \|0 & 0 \\
        \cline{2-3}
        0 & 0 & \|3 & \|0 \\
        \cline{3-4}
        0 & 0 & 0 & \multicolumn{1}{|c|}{4} \\
        \cline{4-4}
    \end{array}\,\right]
    \]
\end{example}

\begin{example}
    La diagonalización es un método muy útil en el cálculo de potencias altas de una matriz. Si una matriz $A$ es diagonalizable, podemos encontrar una matriz $C$ invertible tal que
    $$C^{-1}AC = D$$
    donde $D$ es una matriz diagonal. Las entradas de la diagonal de $D$ son los eigenvalores de $A$, y las columnas de $C$ son los eigenvectores correspondientes. Una vez que tenemos $D$, calcular por ejemplo, $A^{10}$, se vuelve mucho más sencillo. Dado que $A = CDC^{-1}$, entonces $A^{10} = CD^{10}C^{-1}$. Y dado que $D$ es una matriz diagonal, $D^{10}$ es simplemente la matriz que tiene las potencias $10$ de los elementos de la diagonal de $D$. Por lo tanto, el proceso de diagonalización nos permite calcular $A^{10}$ de manera eficiente, evitando la necesidad de multiplicar la matriz $A$ por sí misma $10$ veces. Esto es especialmente útil cuando estamos trabajando con matrices grandes. Para ilustrar lo anteriormente dicho, tomemos la matriz
    $$A = \begin{bmatrix*}[r]
        3 & - 1 \\
        - 2 & 4
    \end{bmatrix*}$$
    e imaginemos que queremos encontrar $A^{10}$. Primero, calculamos los eigenvalores y los eigenvectores correspondientes. Usando el método expuesto en la página \pageref{metodo_eigen_2x2}, se deduce que $\lambda_1 = 5$ y $\lambda_2 = 2$ son los eigenvalores de $A$. Además, usando las formulas del inciso (ii) se obtiene que
    $$\mathbb{v}_1 = \begin{pmatrix*}[r]
        -1 \\
        2
    \end{pmatrix*} \quad \text{ y } \quad \mathbb{v}_2 = \begin{pmatrix*}[r]
        -1 \\
        -1
    \end{pmatrix*}$$
    Ahora bien, sea $C$ la matriz formada por los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$. Es decir,
    \begin{align*}
        C & = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix*}[r] -1 & -1 \\ 2 & -1 \end{bmatrix*}
    \end{align*}
    Determinemos $C^{-1}$ mediante la expresión \ref{Jjaksksksisid},
    \begin{align*}
        C^{-1} & = \frac{1}{3} \begin{bmatrix*}[r]
            -1 & 1 \\
            -2 & -1
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            -\dfrac{1}{3} & \dfrac{1}{3} \\[2mm]
            -\dfrac{2}{3} & -\dfrac{1}{3}
        \end{bmatrix*}
    \end{align*}
    Así, obtenemos
    \begin{align*}
        C^{-1}AC & = \begin{bmatrix*}[r]
            -\dfrac{1}{3} & \dfrac{1}{3} \\[2mm]
            -\dfrac{2}{3} & -\dfrac{1}{3}
        \end{bmatrix*} \begin{bmatrix*}[r]
            3 & - 1 \\
            -2 & 4
        \end{bmatrix*} \begin{bmatrix*}[r]
            -1 & -1 \\
            2 & -1
        \end{bmatrix*} \\
        & = \begin{bmatrix*}[r]
            -\dfrac{5}{3} & \dfrac{5}{3} \\[2mm]
            -\dfrac{4}{3} & -\dfrac{2}{3}
        \end{bmatrix*} \begin{bmatrix*}[r]
            -1 & -1 \\
            2 & -1
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            5 & 0 \\
            0 & 2
        \end{bmatrix}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$. Observemos que
    $$D^{10} = \begin{bmatrix}
        5^{10} & 0 \\
        0 & 2^{10}
    \end{bmatrix}$$
    y sabemos que
    $$A^{10} = CD^{10}C^{-1}$$
    Entones se sigue que
    \begin{align*}
        A^{10} & = \begin{bmatrix*}[r]
            -1 & -1 \\
            2 & -1
        \end{bmatrix*} \begin{bmatrix}
            5^{10} & 0 \\
            0 & 2^{10}
        \end{bmatrix} \begin{bmatrix*}[r]
            -\dfrac{1}{3} & \dfrac{1}{3} \\[2mm]
            -\dfrac{2}{3} & -\dfrac{1}{3}
        \end{bmatrix*} \\
        & = \begin{bmatrix*}
            -5^{10} & -2^{10} \\
            \phantom{-} 2 \cdot 5^{10} & -2^{10}
        \end{bmatrix*} \begin{bmatrix*}[r]
            -\dfrac{1}{3} & \dfrac{1}{3} \\[2mm]
            -\dfrac{2}{3} & -\dfrac{1}{3}
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            \dfrac{1}{3} \left( 5^{10} + 2^{11} \right) & \dfrac{1}{3} \left( -5^{10} + 2^{10} \right) \\[2mm]
            \dfrac{1}{3} \left( - 2 \cdot 5^{10} + 2^{11} \right) & \dfrac{1}{3} \left( 2 \cdot 5^{10} + 2^{10} \right)
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            3 \, 255 \, 891 & - 3 \, 254 \, 867 \\
            - 6 \, 509 \, 734 & 6 \, 510 \, 758
        \end{bmatrix*}
    \end{align*}
\end{example}

\begin{example}
    Consideremos la matriz
    $$A = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix}$$
    Deseamos calcular $A^n$. Para esto, primero calculemos varias potencias de esta matriz como sigue:
    $$A^2 = AA = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix} \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix} = \begin{bmatrix}
        1 & 2 \\
        1 & 1
    \end{bmatrix}$$
    A continuación,
    $$A^3 = AA^2 = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        1 & 2 \\
        1 & 1
    \end{bmatrix} = \begin{bmatrix}
        1 & 2 \\
        2 & 3
    \end{bmatrix}$$
    Prosigamos con,
    $$A^4 = AA^3 = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        1 & 2 \\
        2 & 3
    \end{bmatrix} = \begin{bmatrix}
        2 & 3 \\
        3 & 5
    \end{bmatrix}$$
    Luego
    $$A^5 = AA^4 = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        2 & 3 \\
        3 & 5
    \end{bmatrix} = \begin{bmatrix}
        3 & 5 \\
        5 & 8
    \end{bmatrix}$$
    Finalmente,
    $$A^6 = AA^5 = \begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        3 & 5 \\
        5 & 8
    \end{bmatrix} = \begin{bmatrix}
        5 & 8 \\
        8 & 13
    \end{bmatrix}$$\newpage\noindent
    Al realizar estos cálculos, se destacan dos aspectos importantes. En primer lugar, este proceso resulta sumamente tedioso. Calcular las potencias de una matriz mediante la multiplicación repetida es ineficiente desde el punto de vista computacional, especialmente para matrices de gran tamaño. La perspectiva de encontrar, por ejemplo, $A^{100}$ es verdaderamente desalentadora, incluso contando con la ayuda de una computadora. En segundo lugar, los términos en el lado de nuestra matriz parecen seguir la secuencia de Fibonacci: $1$, $1$, $2$, $3$, $5$, $8$, $13$, $\dots$. Esto no es una coincidencia. Existe una razón fundamental detrás de este patrón. Veamos cómo actúa esta matriz sobre un vector arbitrario para entender mejor este fenómeno:
    $$\begin{bmatrix}
        0 & 1 \\
        1 & 1
    \end{bmatrix} \begin{bmatrix}
        a \\
        b
    \end{bmatrix} = \begin{bmatrix}
        b \\
        a + b
    \end{bmatrix}$$
    Así, el segundo componente del vector de entrada, $b$, se convierte en el primer componente del vector de salida, y el segundo componente del vector de salida se convierte en la suma de $a$ y $b$. Por ejemplo, pensemos en aplicar este proceso repetidamente a un vector dado, es decir
    $$\begin{bmatrix}
        a \\
        b
    \end{bmatrix} \to \begin{bmatrix}
        b \\
        a + b
    \end{bmatrix}$$
    entonces obtenemos:
    $$\begin{bmatrix}
        1 \\
        1
    \end{bmatrix} \to \begin{bmatrix}
        1 \\
        2
    \end{bmatrix} \to \begin{bmatrix}
        2 \\
        3
    \end{bmatrix} \to \begin{bmatrix}
        3 \\
        5
    \end{bmatrix} \to \begin{bmatrix}
        5 \\
        8
    \end{bmatrix} \to \begin{bmatrix}
        8 \\
        13
    \end{bmatrix} \to \cdots$$
    Este proceso iterativo captura perfectamente la definición de la secuencia de Fibonacci: cada término es la suma de los dos términos anteriores. Así que, dado que multiplicar la matriz $A^n$ por un vector corresponde a pasar por este proceso $n$ veces, descubrir cómo representar $A^n$ puede llevarnos a una fórmula explícita para el término $n$-ésimo de Fibonacci. Tal vez no sea tan obvio que se pueda obtener una fórmula clara para $A^n$, pero observemos que podemos aplicar el mismo método que en el ejemplo anterior. Usando el método expuesto en la página \pageref{metodo_eigen_2x2}, se deduce que
    $$\lambda_1 = \frac{1 + \sqrt{5}}{2} \quad \text{ y } \quad \lambda_2 = \frac{1 - \sqrt{5}}{2}$$
    son los eigenvalores de $A$. Además, usando las formulas del inciso (i) se obtiene que
    \begin{align*}
        \mathbb{v}_1 & = \begin{pmatrix} \dfrac{1 + \sqrt{5}}{2} - 1 \\[3mm] 1 \end{pmatrix} & \mathbb{v}_2 & = \begin{pmatrix} \dfrac{1 - \sqrt{5}}{2} - 1 \\[3mm] 1 \end{pmatrix} \\
        & = \begin{pmatrix} \dfrac{\sqrt{5} - 1}{2} \\[3mm] 1 \end{pmatrix} & & = \begin{pmatrix} -\dfrac{1 + \sqrt{5}}{2} \\[3mm] \phantom{-} 1 \end{pmatrix}
    \end{align*}
    Ahora bien, sea $C$ la matriz formada por los vectores $\mathbb{v}_1$ y $\mathbb{v}_2$. Es decir,
    \begin{align*}
        C & = \begin{bmatrix} \mathbb{v}_1 & \mathbb{v}_2 \end{bmatrix} \\
        & = \begin{bmatrix} \dfrac{\sqrt{5} - 1}{2} & -\dfrac{1 + \sqrt{5}}{2} \\[3mm] 1 & \phantom{-} 1 \end{bmatrix}
    \end{align*}
    Determinemos $C^{-1}$ mediante la expresión \ref{Jjaksksksisid},
    $$C^{-1} = \frac{1}{\sqrt{5}} \begin{bmatrix*}[r]
        1 & \dfrac{1 + \sqrt{5}}{2} \\[3mm]
        -1 & \dfrac{\sqrt{5} - 1}{2}
    \end{bmatrix*} = \begin{bmatrix*}[r]
        \dfrac{1}{\sqrt{5}} & \dfrac{1 + \sqrt{5}}{2\sqrt{5}} \\[3mm]
        -\dfrac{1}{\sqrt{5}} & \dfrac{\sqrt{5} - 1}{2\sqrt{5}}
    \end{bmatrix*}$$\newpage\noindent
    Así, obtenemos
    \begin{align*}
        C^{-1}AC & = \begin{bmatrix*}[r]
            \dfrac{1}{\sqrt{5}} & \dfrac{1 + \sqrt{5}}{2\sqrt{5}} \\[3mm]
            -\dfrac{1}{\sqrt{5}} & \dfrac{\sqrt{5} - 1}{2\sqrt{5}}
        \end{bmatrix*} \begin{bmatrix}
            0 & 1 \\
            1 & 1
        \end{bmatrix} \begin{bmatrix}
            \dfrac{\sqrt{5} - 1}{2} & -\dfrac{1 + \sqrt{5}}{2} \\[3mm]
            1 & \phantom{-} 1
        \end{bmatrix} \\
        & = \begin{bmatrix*}[r]
            \dfrac{\sqrt{5} + 5}{10} & \dfrac{3\sqrt{5} + 5}{10} \\[3mm]
            \dfrac{5 - \sqrt{5}}{10} & \dfrac{5 - 3\sqrt{5}}{10}
        \end{bmatrix*} \begin{bmatrix}
            \dfrac{\sqrt{5} - 1}{2} & -\dfrac{1 + \sqrt{5}}{2} \\[3mm]
            1 & \phantom{-} 1
        \end{bmatrix} \\
        & = \begin{bmatrix}
            \dfrac{1 + \sqrt{5}}{2} & 0 \\[2mm]
            0 & \dfrac{1 - \sqrt{5}}{2}
        \end{bmatrix}
    \end{align*}
    que corresponde a la matriz diagonal $D$ formada por los eigenvalores de $A$. Observemos que
    $$D^n = \begin{bmatrix}
        \left( \dfrac{1 + \sqrt{5}}{2} \right)^n & 0 \\[2mm]
        0 & \left( \dfrac{1 - \sqrt{5}}{2} \right)^n
    \end{bmatrix} = \begin{bmatrix}
        \lambda_1^n & 0 \\
        0 & \lambda_2^n
    \end{bmatrix}$$
    y sabemos que
    $$A^n = CD^nC^{-1}$$
    Entonces se sigue que
    $$A^n = \begin{bmatrix} \dfrac{\sqrt{5} - 1}{2} & -\dfrac{1 + \sqrt{5}}{2} \\[3mm] 1 & \phantom{-} 1 \end{bmatrix} \begin{bmatrix} \lambda_1^n & 0 \\ 0 & \lambda_2^n \end{bmatrix} \begin{bmatrix*}[r] \dfrac{1}{\sqrt{5}} & \dfrac{1 + \sqrt{5}}{2\sqrt{5}} \\[3mm] -\dfrac{1}{\sqrt{5}} & \dfrac{\sqrt{5} - 1}{2\sqrt{5}} \end{bmatrix*}$$
    pero podemos reescribir la anterior expresión. Observemos que
    $$\lambda_1 \lambda_2 = \left( \frac{1 + \sqrt{5}}{2} \right) \left( \frac{1 - \sqrt{5}}{2} \right) = -1$$
    Entonces,
    \begin{align*}
        A^n & = \frac{1}{\sqrt{5}} \begin{bmatrix}
            -\lambda_2 & -\lambda_1 \\
            \phantom{-} 1 & \phantom{-} 1
        \end{bmatrix} \begin{bmatrix}
            \lambda_1^n & 0 \\
            0 & \lambda_2^n
        \end{bmatrix} \begin{bmatrix*}[r]
            1 & \lambda_1 \\
            -1 & -\lambda_2
        \end{bmatrix*} \\
        & = \frac{1}{\sqrt{5}} \begin{bmatrix}
            -\lambda_1^n\lambda_2 & -\lambda_1\lambda_2^n \\
            \phantom{-} \lambda_1^n & \phantom{-} \lambda_2^n
        \end{bmatrix} \begin{bmatrix*}[r]
            1 & \lambda_1 \\
            -1 & -\lambda_2
        \end{bmatrix*} \\
        & = \frac{1}{\sqrt{5}} \begin{bmatrix}
            -\lambda_1^n\lambda_2 + \lambda_1\lambda_2^n & -\lambda_1^{n+1}\lambda_2 + \lambda_1\lambda_2^{n+1} \\[1.5mm]
            \phantom{-} \lambda_1^n - \lambda_2^n & \phantom{-} \lambda_1^{n+1} - \lambda_2^{n+1}
        \end{bmatrix} \\
        & = \frac{1}{\sqrt{5}} \begin{bmatrix}
            \lambda_1^{n-1} - \lambda_2^{n-1} & \lambda_1^n - \lambda_2^n \\[1.5mm]
            \lambda_1^n - \lambda_2^n & \lambda_1^{n+1} - \lambda_2^{n+1}
        \end{bmatrix}
    \end{align*}
    Si comparamos esto con los primeros cálculos que hicimos para $A^n$ al principio, esto nos da una fórmula muy llamativa. El $n$-ésimo número de Fibonacci tiene como fórmula:
    $$F_n = \frac{\lambda_1^n - \lambda_2^n}{\sqrt{5}} = \frac{1}{\sqrt{5}} \left[ \left( \frac{1 + \sqrt{5}}{2} \right)^n - \left( \frac{1 - \sqrt{5}}{2} \right)^n \right]$$
    Ni siquiera es obvio que esta expresión deba dar una expresión racional, y mucho menos los números de Fibonacci.
\end{example}

\newpage

\section{El teorema de Cayley-Hamilton}

El teorema de Cayley-Hamilton es un resultado fundamental en álgebra lineal que establece una relación profunda entre una matriz cuadrada y su polinomio característico. Este teorema permite simplificar muchos cálculos y tiene aplicaciones significativas en diversas áreas, como la teoría de control y la resolución de sistemas de ecuaciones diferenciales.

En esencia, el teorema nos dice que una matriz cuadrada satisface su propio polinomio característico, proporcionando una herramienta poderosa para analizar y manipular matrices. A continuación, enunciaremos y demostraremos este importante teorema, pero antes de hacerlo, revisaremos algunas definiciones y conceptos clave que son necesarios para comprender y demostrar este teorema.

\begin{definition}
    Sean $P_0$, $P_1$, $\dots$, $P_m$ matrices de tamaño $n \times n$ con coeficientes reales o complejos. Entonces la expresión
    $$p(x) = P_0 + P_1x + P_2x^2 + \cdots + P_mx^m$$
    se llama polinomio con coeficientes matriciales.
\end{definition}

\begin{definition}
    Sea $p$ un polinomio con coeficientes matriciales
    $$p(x) = P_0 + P_1x + P_2x^2 + \cdots + P_mx^m$$
    y sea $A$ una matriz de tamaño $n \times n$ con coeficientes reales o complejos. Entonces se define respectivamente el valor derecho y el valor izquierdo del polinomio $p$ en la matriz $A$ como:
    \begin{align*}
        p_{\operatorname{der}}(A) & = P_0 + P_1A + P_2A^2 + \cdots + P_mA^m \\
        p_{\operatorname{izq}}(A) & = P_0 + AP_1 + A^2P_2 + \cdots + A^mP_m
    \end{align*}
    En vez de $p_{\operatorname{der}}(A)$, escribiremos simplemente $p(A)$.
\end{definition}

\begin{example}
    Polinomios con coeficientes matriciales no cumplen algunas de las propiedades de los polinomios con coeficientes  numéricos. Mostremos un ejemplo de polinomios $p$, $q$ con coeficientes matriciales tales que
    $$(pq)(A) \neq p(A)q(A)$$
    Sean
    $$p(x) = \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}x \qquad \text{ y } \qquad q(x) = \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix}$$
    dos polinomios con coeficientes matriciales. Entonces
    $$(pq)(x) = \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix}x$$
    Si consideramos $A = \begin{bmatrix}
        0 & 0 \\
        1 & 0
    \end{bmatrix}$, obtenemos que
    $$(pq)(A) = \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix} \qquad \text{ y } \qquad p(A)q(A) = \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix}$$
\end{example}

\begin{lemma}
    Sean $p$ y $q$ dos polinomios con coeficientes matriciales
    $$p(x) = \sum_{i=0}^m P_ix^i \qquad \text{ y } \qquad q(x) = \sum_{j=0}^s Q_jx^j$$
    donde $P_i$, $Q_j$, $A$ son matrices de tamaño $n \times n$ con coeficientes reales o complejos. Si $A$ conmuta con todos los coeficientes del polinomio $q$, es decir,
    $$Q_j A = AQ_j, \forall j = 0, 1, \dots, s$$\newpage\noindent
    entonces
    $$(pq)(A) = p(A)q(A)$$
\end{lemma}

\begin{theorem}[Teorema de Cayley-Hamilton]
    Toda matriz cuadrada satisface su propia ecuación característica. Es decir, si $p(\lambda) = 0$ es la ecuación característica de $A$, entonces $p(A) = \mathbb{0}$. \\
    \demostracion Sabemos por el teorema \ref{propi:adjuntoid} que para toda matriz cuadrada $B$, se tiene que
    $$B \operatorname{adj}(B) = \operatorname{det}(B) I_n$$
    donde $\operatorname{adj}(B)$ es la matriz adjunta clásica de $B$. Al aplicar este resultado a la matriz $A - \lambda I_n$, obtenemos que
    $$(A - \lambda I_n) \operatorname{adj}(A - \lambda I_n) = \operatorname{det}(A - \lambda I_n) I_n = p(\lambda) I_n$$
    Sean
    $$f(\lambda) = \operatorname{adj}(A - \lambda I_n) \qquad \text{ y } \qquad g(\lambda) = A - \lambda I_n$$
    Estas expresiones son matrices con entradas polinomiales, las vamos a tratar como polinomiales con coeficientes matriciales. Notemos que los coeficientes de $g$ son $I_n$ y $A$, y estas matrices conmutan con $A$. Por el lema anterior, se sigue que
    \begin{align*}
        p(A) & = f(A)g(A) \\
        & = f(A) (A - AI_n) \\
        & = \mathbb{0}
    \end{align*}
\end{theorem}

\begin{example}
    Consideremos la matriz
    $$A = \begin{bmatrix}
        5 & 2 \\
        7 & 1
    \end{bmatrix}$$
    Es fácil deducir que la ecuación característica de $A$ está dada por
    $$p(\lambda) = \lambda^2 - 6\lambda - 9 = 0$$
    Entonces
    \begin{align*}
        p(A) & = A^2 - 6A - 9I_2 \\
        & = \begin{bmatrix}
            39 & 12 \\
            42 & 15
        \end{bmatrix} + \begin{bmatrix}
            -30 & -12 \\
            -42 & -6
        \end{bmatrix} + \begin{bmatrix*}[r]
            -9 & 0 \\
            0 & -9
        \end{bmatrix*} \\
        & = \begin{bmatrix}
            0 & 0 \\
            0 & 0
        \end{bmatrix}
    \end{align*}
\end{example}

\section{Formas bilineales}

Anteriormente hemos discutido las formas cuadráticas, que son un caso especial de las formas bilineales. Ahora, nos adentraremos en el estudio de las formas bilineales, una clase de funciones que toman dos variables definidas en un espacio vectorial y devuelven un escalar. Estas funciones aparecen en el análisis de temas tan diversos como la geometría y el cálculo multivariable. Analizaremos las propiedades básicas de las formas bilineales, con un énfasis particular en las formas bilineales simétricas, y exploraremos algunas de sus aplicaciones en superficies cuadráticas y en el cálculo multivariable.

\begin{definition}
    Sean $V$ y $W$ dos espacios vectoriales sobre un campo $K$, entonces la transformación lineal $B: V \times W \longrightarrow K$ se llama bilineal si satisface las siguientes propiedades:
    \begin{enumerate}[label=\roman*)]
        \item Para todo $\mathbb{v}_1$, $\mathbb{v}_2 \in V$ y $\mathbb{w} \in W$,
        $$B(\mathbb{v}_1 + \mathbb{v}_2, \mathbb{w}) = B(\mathbb{v}_1, \mathbb{w}) + B(\mathbb{v}_2, \mathbb{w})$$\newpage
        \item Para todo $\mathbb{v} \in V$ y $\mathbb{w}_1$, $\mathbb{w}_2 \in W$,
        $$B(\mathbb{v}, \mathbb{w}_1 + \mathbb{w}_2) = B(\mathbb{v}, \mathbb{w}_1) + B(\mathbb{v}, \mathbb{w}_2)$$
        \item Para todo $\mathbb{v} \in V$, $\mathbb{w} \in W$ y $c \in K$,
        $$B(c\mathbb{v}, \mathbb{w}) = cB(\mathbb{v}, \mathbb{w}) = B(\mathbb{v}, c\mathbb{w})$$
    \end{enumerate}
    Si además $V = W$, entonces la transformación $B$ se denomina forma bilineal.
\end{definition}

\begin{example}
    Sea $V = W = \RR[n]$, entonces la expresión
    \begin{equation}
        B(\mathbb{v}_1, \mathbb{v}_2) = A\mathbb{v}_1 \bullet \mathbb{v}_2 \label{FOCUBILI}
    \end{equation}
    donde $A \in \mathcal{M}_{n \times n}(\RR)$ conforma una forma bilineal. Para probarlo, debemos demostrar que cumplen las tres propiedades de la anterior definición. Así
    \begin{enumerate}[label=\roman*)]
        \item Para todo $\mathbb{x}_1$, $\mathbb{x}_2 \in \RR[n]$ e $\mathbb{y} \in \RR[n]$,
        \begin{align*}
            B(\mathbb{x}_1 + \mathbb{x}_2, \mathbb{y}) & = A(\mathbb{x}_1 + \mathbb{x}_2) \bullet \mathbb{y} \\
            & = (A\mathbb{x}_1 + A\mathbb{x}_2) \bullet \mathbb{y} \\
            & = A\mathbb{x}_1 \bullet \mathbb{y} + A\mathbb{x}_2 \bullet \mathbb{y} \\
            & = B(\mathbb{x}_1, \mathbb{y}) + B(\mathbb{x}_2, \mathbb{y})
        \end{align*}
        Por lo tanto, $B(\mathbb{x}_1 + \mathbb{x}_2, \mathbb{y}) = B(\mathbb{x}_1, \mathbb{y}) + B(\mathbb{x}_2, \mathbb{y})$.
        \item Para todo $\mathbb{y} \in \RR[n]$ e $\mathbb{x}_1$, $\mathbb{y}_2 \in \RR[n]$,
        \begin{align*}
            B(\mathbb{x}, \mathbb{y}_1 + \mathbb{y}_2) & = A\mathbb{x} \bullet (\mathbb{y}_1 + \mathbb{y}_2) \\
            & = A\mathbb{x} \bullet \mathbb{y}_1 + A\mathbb{x} \bullet \mathbb{y}_2 \\
            & = B(\mathbb{x}, \mathbb{y}_1) + B(\mathbb{x}, \mathbb{y}_2)
        \end{align*}
        Por lo tanto, $B(\mathbb{x}, \mathbb{y}_1 + \mathbb{y}_2) = B(\mathbb{x}, \mathbb{y}_1) + B(\mathbb{x}, \mathbb{y}_2)$.
        \item Para todo $\mathbb{x}$, $\mathbb{y} \in \RR[n]$ y $c \in \RR$,
        \begin{align*}
            B(c\mathbb{x}, \mathbb{y}) & = A(c\mathbb{x}) \bullet \mathbb{y} \\
            & = cA\mathbb{x} \bullet \mathbb{y} \\
            & = cB(\mathbb{x}, \mathbb{y}) \\
            & = A\mathbb{x} \bullet (c\mathbb{y}) \\
            & = B(\mathbb{x}, c\mathbb{y})
        \end{align*}
        Por lo tanto, $B(c\mathbb{x}, \mathbb{y}) = cB(\mathbb{x}, \mathbb{y}) = B(\mathbb{x}, c\mathbb{y})$.
    \end{enumerate}
    Ya que la expresión \eqref{FOCUBILI} cumple las tres propiedades, entonces es bilineal. Más aún, al ser $V = W$, se sigue que dicha expresión es una forma bilineal.
\end{example}

\newpage

\section{Ejercicios}

\noindent
De los problemas 1 al 27 calcule los eigenvalores y los espacios característicos de la matriz dada. Si la multiplicidad algebraica de un eigenvalor es mayor que $1$, calcule su multiplicidad geométrica.
\begin{tasks}[
    style=enumerate,
    label-offset = 3mm,
    %label-width = 13.97498pt,
    ](2)
    \task $\begin{bmatrix*}[r]-81 & 16 \\ -420 & 83\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-2 & -2 \\ -5 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-12 & 7 \\ -7 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]23 & 12 \\ -42 & -22\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]2 & -1 \\ 5 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-3 & 0 \\ 0 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-62 & -20 \\ 192 & 62\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]3 & 2 \\ -5 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-3 & 2 \\ 0 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-10 & -71 & -19 \\ 3 & 34 & 9 \\ -1 & -61 & -16\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}5 & 4 & 2 \\ 4 & 5 & 2 \\ 2 & 2 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]13 & 3 & 1 \\ -56 & -13 & -4 \\ -14 & -3 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & -3 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & 2 & 2 \\ 0 & 2 & 1 \\ -1 & 2 & 2\end{bmatrix*}$
    %\task $\begin{bmatrix*}[r]260 & 0 & \\ -1 & 0 & 1 \\ -1 & -2 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-2 & 5 & 0 \\ 5 & -2 & 0 \\ 0 & 0 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]7 & -2 & -4 \\ 3 & 0 & -2 \\ 6 & -2 & -3\end{bmatrix*}$
    %\task $\begin{bmatrix*}[r]-662 & 5 & \\ 0 & 3 & 0 \\ -10 & 0 & 9\end{bmatrix*}$
    \task $\begin{bmatrix*}1 & 2 & 4 \\ 0 & 2 & 3 \\ 0 & 0 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]4 & 6 & 6 \\ 1 & 3 & 2 \\ -1 & -5 & -2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]18 & 42 & 26 & -10 \\ 22 & 70 & 37 & -17 \\ -20 & -60 & -31 & 15 \\ 62 & 186 & 104 & -44\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]4 & 1 & 0 & 1 \\ 2 & 3 & 0 & 1 \\ -2 & 1 & 2 & -3 \\ 2 & -1 & 0 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & 0 & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b \neq 0$
    \task $\begin{bmatrix*}a & 0 & 0 & 0 \\ 0 & a & b & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & c & 0 \\ 0 & 0 & a & d \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b c d \neq 0$
    \task $\begin{bmatrix*}a & b & 0 & 0 \\ 0 & a & c & 0 \\ 0 & 0 & a & 0 \\ 0 & 0 & 0 & a\end{bmatrix*}$, $b c \neq 0$
    \task $\begin{bmatrix*}3 & 1 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 4 & 1 \\ 0 & 0 & 0 & 4\end{bmatrix*}$
\end{tasks}
\begin{enumerate}[start=27]
    \item Demuestre que para cualesquiera $a$, $b \in \RR$, la matriz $A=\begin{bmatrix*}[r]a & b \\ -b & a\end{bmatrix*}$ tiene eigenvalores $a \pm i b$.
\end{enumerate}
De los problemas 28 al 34 suponga que la matriz $A$ tiene eigenvalores $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$.
\begin{enumerate}[resume]
    \item Demuestre que los eigenvalores de $A^{T}$ son $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$.\newpage
    \item Demuestre que los eigenvalores de $\alpha A$ son $\alpha \lambda_{1}, \alpha \lambda_{2}, \dots, \alpha \lambda_{k}$.
    \item Demuestre que $A^{-1}$ existe si y sólo si $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k} \neq 0$.
    \item Si $A^{-1}$ existe, demuestre que los eigenvalores de $A^{-1}$ están dados por $\dfrac{1}{\lambda_{1}}, \dfrac{1}{\lambda_{2}}, \dots, \dfrac{1}{\lambda_{k}}$.
    \item Demuestre que la matriz $A-\alpha I_n$ tiene como eigenvalores a los escalares $\lambda_{1}-\alpha, \lambda_{2}-\alpha, \dots, \lambda_{k}-\alpha$.
    \item Demuestre que los eigenvalores de $A^{2}$ son $\lambda_{1}^{2}, \lambda_{2}^{2}, \dots, \lambda_{k}^{2}$.
    \item Demuestre que los eigenvalores de $A^{m}$ son $\lambda_{1}^{m}, \lambda_{2}^{m}, \dots, \lambda_{k}^{m}$ para $m=1,2,3, \dots$
    \item Sea $\lambda$ un eigenvalor de $A$ con $\mathbb{v}$ como el eigenvector correspondiente. Sea $p(\lambda)=a_{0}+a_{1} \lambda+a_{2} \lambda^{2}+\cdots+a_{n} \lambda^{n}$. Defina la matriz $p(A)$ por $p(A)=a_{0} I_n+a_{1} A+a_{2} A^{2}$ $+\cdots+a_{n} A^{n}$. Demuestre que, en efecto, $p(A) \mathbb{v}=p(\lambda) \mathbb{v}$.
    \item Utilizando el resultado del problema 35, demuestre que si $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ son eigenvalores de $A$, entonces $p\left(\lambda_{1}\right), p\left(\lambda_{2}\right), \dots, p\left(\lambda_{k}\right)$ son vectores característicos de $p(A)$.
    \item Demuestre que si $A$ es una matriz diagonal, entonces los eigenvalores de $A$ son las componentes de la diagonal de $A$.
    \item Sea $A_{1}=\begin{bmatrix*}2 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{2}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{3}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$, $A_{4}=\begin{bmatrix*}2 & 1 & 0 & 0 \\ 0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2\end{bmatrix*}$.
    Demuestre que para cada matriz $\lambda=2$ es un eigenvalor con multiplicidad algebraica $4$. En cada caso calcule la multiplicidad geométrica de $\lambda=2$.
    \item Sea $A$ una matriz real de $n \times n$. Demuestre que si $\lambda$ es un eigenvalor complejo de $A$ con eigenvector $\mathbb{v}$, entonces $\overline{\lambda}$ es un eigenvalor de $A$ con eigenvector $\overline{\mathbb{v}}$.
    \item Una matriz de probabilidad es una matriz de $n \times n$ que tiene dos propiedades:
    \begin{enumerate}
        \item $a_{i j} \geq 0$ para toda $i$ y $j$.
        \item La suma de las componentes en cada columna es $1$.
    \end{enumerate}
    Demuestre que $1$ es un eigenvalor de toda matriz de probabilidad.
    \item Sea $A=\begin{bmatrix*}a & b \\ c & d\end{bmatrix*}$ una matriz de $2 \times 2$. Suponga que $b \neq 0$. Sea $m$ una raíz (real o compleja) de la ecuación
    $$b m^{2}+(a-d) m-c=0$$
    Demuestre que $a+b m$ es un eigenvalor de $A$ con eigenvector correspondiente $\mathbb{v}=\begin{pmatrix*}1 \\ m\end{pmatrix*}$. Esto proporciona un método sencillo para calcular los valores y vectores característicos de las matrices de $2 \times 2$.
    \item Sea $A=\begin{bmatrix*}[r]a & 0 \\ c & d\end{bmatrix*}$ una matriz de $2 \times 2$. Demuestre que $d$ es un eigenvalor de $A$ con eigenvector correspondiente $\begin{pmatrix*}1 \\ 0\end{pmatrix*}$.\newpage
    \item Sea $A=\begin{bmatrix*}[r]\alpha & \beta \\ -\beta & \alpha\end{bmatrix*}$, donde $\alpha, \beta \in \RR$. Encuentre los eigenvalores de la matriz $B=A^{T} A$.
\end{enumerate}
De los problemas 44 al 53 encuentre la matriz ortogonal $Q$ que diagonaliza la matriz simétrica dada. Después verifique que $Q^{T} A Q=D$, una matriz diagonal cuyas componentes diagonales son los eigenvalores de $A$.
\begin{tasks}[
    style=enumerate,
    label-offset = 3mm,
    start=44,
    %label-width = 13.97498pt,
    ](2)
    %\task $\begin{bmatrix*}3 & 2 \\ 2 & 3\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]3 & 4 \\ 4 & -3\end{bmatrix*}$
    \task $\begin{bmatrix*}2 & 1 \\ 1 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-4 & 2 \\ 2 & 5\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 \\ -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & -1 \\ -1 & 1 & -1 \\ -1 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]20 & 16 & -4 \\ 16 & 32 & 16 \\ -4 & 16 & 20\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]-1 & 2 & 2 \\ 2 & -1 & 2 \\ 2 & 2 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$
    \task $\begin{bmatrix*}[r]1 & -1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 2\end{bmatrix*}$
\end{tasks}
\begin{enumerate}[start=54]
    \item Sea $Q$ una matriz ortogonal simétrica. Demuestre que si $\lambda$ es un eigenvalor de $Q$, entonces $\lambda= \pm 1$.
    \item La matriz $A$ es ortogonalmente semejante a la matriz $B$ si existe una matriz ortogonal $Q$ tal que $B=Q^{T} A Q$. Suponga que $A$ es ortogonalmente semejante a $B$ y que $B$ es ortogonalmente semejante a $C$. Demuestre que $A$ es ortogonalmente semejante a $C$.
    \item Demuestre que si $Q=\begin{bmatrix*}a & b \\ c & d\end{bmatrix*}$ es ortogonal, entonces $b= \pm c$.
    \item Suponga que $A$ es una matriz simétrica real para la que todos sus eigenvalores son cero. Demuestre que $A$ es la matriz cero.
    \item Demuestre que si una matriz real $A$ de $2 \times 2$ tiene eigenvectores ortogonales, entonces $A$ es simétrica.
    \item Sea $A$ una matriz real antisimétrica. Demuestre que todo eigenvalor de $A$ es de la forma $i \alpha$, donde $\alpha \in \RR$ e $i=\sqrt{-1}$. Es decir, demuestre que todo eigenvalor de $A$ es un número imaginario.
    \item Demuestre que los eigenvalores de una matriz hermitiana compleja de $n \times n$ son reales.
    \item Si $A$ es una matriz hermitiana de $n \times n$, demuestre que los eigenvectores correspondientes a eigenvalores distintos son ortogonales.
    \item Repitiendo la demostración del teorema \ref{theorem_simetrica2}, pero sustituyendo $\overline{\mathbb{v}}_i^{t}$ por $\mathbb{v}_i^{t}$ donde sea adecuado, demuestre que cualquier matriz hermitiana de $n \times n$ tiene $n$ eigenvectores ortonormales.
    \item Encuentre una matriz unitaria $U$ tal que $U^* A U$ es diagonal, donde $A=\begin{bmatrix*}0 & 3-2 i \\ 3+2 i & 0\end{bmatrix*}$.
    \item Haga lo mismo que en el problema anterior para $A=\begin{bmatrix*}-2 & -3+2 i \\ -3-4 i & 2\end{bmatrix*}$.
    \item Demuestre que el determinante de una matriz hermitiana es real.
\end{enumerate}

\newpage\noindent
Del problema 66 al 83 escriba la ecuación cuadrática en la forma $A \mathbb{v} \bullet \mathbb{v}=d$ (donde $A$ es una matriz simétrica) y elimine el término $x y$ rotando los ejes un ángulo $\theta$. Escriba la ecuación en términos de las nuevas variables e identifique la sección cónica obtenida.
\begin{tasks}[
    style=enumerate,
    label-offset = 3mm,
    start=66,
    %label-width = 13.97498pt,
    ](2)
    \task $4 x^{2}-2 x y+4 y^{2}=25$
    \task $3 x^{2}-2 x y-5=0$
    \task $4 x^{2}+4 x y+y^{2}=9$
    \task $-x^{2}-6 \sqrt{3} x y+5 y^{2}=8$
    \task $4 x^{2}+4 x y-y^{2}=9$
    \task $x y=1$
    \task $2 x^{2}-4 \sqrt{3} x y+6 y^{2}=10$
    \task $x y=a$, con $a>0$
    \task $4 x^{2}+2 x y+3 y^{2}+2=0$
    \task $4 x^{2}-8 \sqrt{3} x y-4 y^{2}=1$
    \task $x^{2}+4 x y+4 y^{2}-6=0$
    \task $-x^{2}+2 x y-y^{2}=0$
    \task $\displaystyle\frac{24 x^{2}}{5}-\frac{24 x y}{5}+\frac{31 y^{2}}{5}=1$
    \task $x^{2}-2 x y+3 y^{2}=5$
    \task $3 x^{2}-6 x y+5 y^{2}=36$
    \task $\displaystyle\frac{2 x^{2}}{25}-\frac{72 x y}{25}+\frac{23 y^{2}}{25}=1$
    \task $x^{2}+x y+y^{2}=5$
    \task $6 x^{2}+5 x y-6 y^{2}+7=0$
\end{tasks}
\begin{enumerate}[start=84]
    \item ¿Cuáles son las formas posibles de la gráfica de $a x^{2}+b x y+c y^{2}=0$?
\end{enumerate}
De los problemas 85 al 88 escriba la forma cuadrática en términos de las nuevas variables $x^{\prime}$, $y^{\prime}$ y $z^{\prime}$ de manera que no estén presentes los términos de productos cruzados ($x y$, $x z$, $y z$).
\begin{enumerate}[resume]
    \item $x^{2}-2 x y+y^{2}-2 x z-2 y z+z^{2}$
    \item $\displaystyle\frac{27 x^{2}}{16}-\frac{7 \sqrt{3} x y}{8}-\frac{9 x z}{4}+\frac{41 y^{2}}{16}-\frac{3 \sqrt{3} y z}{4}-\frac{z^{2}}{4}=1$
    \item $x^{2}+x y+y^{2}+3 x z+z^{2}$
    \item $3 x^{2}+4 x y+2 y^{2}+4 x z+4 z^{2}$
\end{enumerate}
De los problemas 89 al 91 encuentre una matriz simétrica $A$ tal que la forma cuadrática se pueda escribir en la forma $A \mathbb{x} \bullet \mathbb{x}$.
\begin{enumerate}[resume]
    \item $x_{1}^{2}+2 x_{1} x_{2}+x_{2}^{2}+4 x_{1} x_{3}+6 x_{2} x_{3}+3 x_{3}^{2}+7 x_{1} x_{4}-2 x_{2} x_{4}+x_{4}^{2}$
    \item $x_{1}^{2}-3 x_{2}^{2}+9 x_{3}^{2}-7 x_{4}^{2}+3 x_{1} x_{3}-2 x_{2} x_{3}+4 x_{1} x_{4}$
    \item $3 x_{1}^{2}-7 x_{1} x_{2}-2 x_{2}^{2}+x_{1} x_{3}-x_{2} x_{3}+3 x_{3}^{2}-2 x_{1} x_{4}+x_{2} x_{4}-4 x_{3} x_{4}-6 x_{4}^{2}+3 x_{1} x_{5}-5 x_{3} x_{5}+x_{4} x_{5}-x_{5}^{2}$
    \item Suponga que para algún valor de $d$ diferente de cero, la gráfica de $a x^{2}+b x y+c y^{2}=d$ es una hipérbola. Demuestre que la gráfica es una hipérbola para cualquier otro valor de $d$ diferente de cero.
    \item Demuestre que si $a \neq c$, el término $x y$ en la ecuación cuadrática \eqref{cuadratica1} se elimina rotando un ángulo $\theta$, si $\theta$ está dado por $\displaystyle\cot 2 \theta=\frac{a-c}{b}$.
    \item Demuestre que si $a=c$ en el problema anterior, entonces el término $x y$ se elimina rotando un ángulo $\displaystyle\pm \frac{\pi}{4}$.
    \item Suponga que una rotación convierte a
    $$a x^{2}+b x y+c y^{2}$$
    en $$a^{\prime}\left(x^{\prime}\right)^{2}+b^{\prime}\left(x y^{\prime}\right)+c^{\prime}\left(y^{\prime}\right)^{2}$$
    Demuestre que:
    \begin{enumerate}
        \item $a+c=a^{\prime}+c^{\prime}$
        \item $b^{2}-4 a c=\left(b^{\prime}\right)^{2}-4 a^{\prime} c^{\prime}$
    \end{enumerate}\newpage
    \item Se dice que una forma cuadrática $F(\mathbb{x})=F\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ es positiva definida si $F(\mathbb{x})>0$ para toda $\mathbb{x} \in \RR[n]$ y $F(\mathbb{x})=0$ si y sólo si $\mathbb{x} = \mathbb{0}$. Demuestre que $F$ es positiva definida si y positiva definida sólo si la matriz simétrica $A$ asociada a $F$ tiene valores característicos positivos.
    \item Se dice que una forma cuadrática $F(\mathbb{x})$ es positiva semidefinida si $F(\mathbb{x}) \geq 0$ para todo $\mathbb{x} \in$ $\RR[n]$. Demuestre que $F$ es positiva semidefinida si y sólo si los valores característicos de la matriz simétrica asociada a $F$ son todos no negativos.
\end{enumerate}
Las definiciones de formas cuadráticas negativa definida y negativa semidefinida son las definiciones en los problemas 96 y 97 sustituyendo $>0$ por $<0$ y $\geq 0$ por $\leq 0$, respectivamente. Una forma cuadrática es indefinida si no es de los tipos anteriores. De los problemas 98 al 109 determine si la forma cuadrática dada es positiva definida, positiva semidefinida, negativa definida, negativa semidefinida o indefinida.
\begin{multienumerate}
    \setcounter{multienumi}{97}
    \mitemxxx{$3 x^{2}+2 y^{2}$}{$4 x^{2}-10 x y^{2}+2 y^{2}$}{$-3 x^{2}-3 y^{2}$}
    \mitemxxx{$3 x^{2}-2 y^{2}$}{$9 x^{2}+1-10 y^{2}$}{$x^{2}+2 x y+2 y^{2}$}
    \mitemxxx{$x^{2}-2 x y+2 y^{2}$}{$10 x y-5 y^{2}$}{$-x^{2}+4 x y-3 y^{2}$}
    \mitemxxx{$-2 x^{2}+x y-2 y^{2}$}{$-3 x^{2}-2 x y-y^{2}$}{$-x^2 + 8xy - 2y^2$}
\end{multienumerate}
\begin{enumerate}[start=110]
    \item Sea $A$ la representación matricial simétrica de la ecuación cuadrática \eqref{cuadratica1} con $d \neq 0$. Sean $\lambda_{1}$ y $\lambda_{2}$ los valores característicos de $A$. Demuestre que \eqref{cuadratica1} es la ecuación de
    \begin{enumerate}
        \item una hipérbola si $\lambda_{1} \lambda_{2}<0$ y
        \item un círculo, elipse o sección cónica degenerada si $\lambda_{1}$ y $\lambda_{2}>0$.
    \end{enumerate}
\end{enumerate}
De los problemas 111 al 128 determine si la matriz dada es una matriz de Jordan.
\begin{multienumerate}
    \setcounter{multienumi}{110}
    \mitemxxxx{$\begin{bmatrix*}3 & 4 \\ 0 & 3\end{bmatrix*}$}{$\begin{bmatrix*}[r]1 & 1 \\ 0 & -6\end{bmatrix*}$}{$\begin{bmatrix*}1 & 0 \\ 0 & 0\end{bmatrix*}$}{$\begin{bmatrix*}3 & 0 \\ 0 & 3\end{bmatrix*}$}
    \mitemxxx{$\begin{bmatrix*}1 & 2 \\ 0 & 1\end{bmatrix*}$}{$\begin{bmatrix*}1 & 0 & 0 \\ 0 & 3 & 1 \\ 0 & 0 & 3\end{bmatrix*}$}{$\begin{bmatrix*}[r]-3 & 1 & 0 \\ 0 & -3 & 0 \\ 1 & 0 & 1\end{bmatrix*}$}
    \mitemxxx{$\begin{bmatrix*}3 & 1 & 0 \\ 0 & 3 & 1 \\ 0 & 0 & 3\end{bmatrix*}$}{$\begin{bmatrix*}3 & 1 & 0 \\ 0 & 3 & 1 \\ 0 & 0 & 2\end{bmatrix*}$}{$\begin{bmatrix*}0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{bmatrix*}$}
    \mitemxxx{$\begin{bmatrix*}1 & 1 & 0 \\ 0 & 3 & 1 \\ 0 & 0 & 3\end{bmatrix*}$}{$\begin{bmatrix*}1 & 0 & 0 \\ 0 & 3 & 1 \\ 0 & 0 & 3\end{bmatrix*}$}{$\begin{bmatrix*}1 & 2 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0\end{bmatrix*}$}
    \mitemxxx{$\begin{bmatrix*}1 & 0 & 0 & 0 & 0 \\ 0 & 2 & 1 & 0 & 0 \\ 0 & 0 & 2 & 1 & 0 \\ 0 & 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 0 & 2\end{bmatrix*}$}{$\begin{bmatrix*}1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 2 & 0 & 0 \\ 0 & 0 & 1 & 2 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1\end{bmatrix*}$}{$\begin{bmatrix*}2 & 0 & 0 & 0 & 0 \\ 0 & 3 & 1 & 0 & 0 \\ 0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 5 & 1 \\ 0 & 0 & 0 & 0 & 5\end{bmatrix*}$}
    \mitemxx{$\begin{bmatrix*}a & 0 & 0 & 0 & 0 \\ 0 & b & 0 & 0 & 0 \\ 0 & 0 & c & 0 & 0 \\ 0 & 0 & 0 & d & 0 \\ 0 & 0 & 0 & 0 & e\end{bmatrix*}$}{$\begin{bmatrix*}a & 1 & 0 & 0 & 0 \\ 0 & a & 0 & 0 & 0 \\ 0 & 0 & c & 1 & 0 \\ 0 & 0 & 0 & c & 1 \\ 0 & 0 & 0 & 0 & c\end{bmatrix*}$}
\end{multienumerate}

\newpage\noindent
De los problemas 129 al 132 encuentre una matriz invertible $C$ que transforme la matriz de $2 \times 2$ a su forma canónica de Jordan.
\begin{multienumerate}
    \setcounter{multienumi}{128}
    \mitemxxxx{$\begin{bmatrix*}6 & 1 \\ 0 & 6\end{bmatrix*}$}{$\begin{bmatrix*}[r]-12 & 7 \\ -7 & 2\end{bmatrix*}$}{$\begin{bmatrix*}[r]-10 & -7 \\ 7 & 2\end{bmatrix*}$}{$\begin{bmatrix*}[r]4 & -1 \\ 1 & 2\end{bmatrix*}$}
\end{multienumerate}
\begin{enumerate}[start=133]
    \item Sea $A$ una matriz de $3 \times 3$. Suponga que $\lambda$ es un valor característico de $A$ con multiplicidad algebraica $3$ y multiplicidad geométrica $1$ y sea $\mathbb{v}_{1}$ el vector característico correspondiente.
    \begin{enumerate}
        \item Demuestre que existe una solución, $\mathbb{v}_{2}$, al sistema $(A-\lambda I) \mathbb{v}_{2}=\mathbb{v}_{1}$ tal que $\mathbb{v}_{1}$ y $\mathbb{v}_{2}$ son linealmente independientes.
        \item Con $\mathbb{v}_{2}$ definido en el inciso a), demuestre que existe una solución, $\mathbb{v}_{3}$, al sistema $(A-\lambda I) \mathbb{v}_{3}=\mathbb{v}_{2}$ tal que $\mathbb{v}_{1}, \mathbb{v}_{2}$ y $\mathbb{v}_{3}$ son linealmente independientes.
        \item Demuestre que si $C$ es una matriz cuyas columnas son $\mathbb{v}_{1}, \mathbb{v}_{2}$ y $\mathbb{v}_{3}$, entonces
        $$C^{-1} A C=\begin{bmatrix*}
            \lambda & 1 & 0 \\
            0 & \lambda & 1 \\
            0 & 0 & \lambda
        \end{bmatrix*}$$
    \end{enumerate}
\end{enumerate}
De los problemas 134 al 136 aplique el procedimiento descrito en el problema anterior para reducir la matriz dada mediante una transformación de semejanza a su forma canónica de Jordan.
\begin{enumerate}[start=134]
    \item $A=\begin{bmatrix*}[r]-4 & -3 & -1 \\ 11 & 9 & 3 \\ -29 & -25 & -8\end{bmatrix*}$
    \item Haga lo mismo para $A=\begin{bmatrix*}[r]-1 & -2 & -1 \\ -1 & -1 & -1 \\ 2 & 3 & 2\end{bmatrix*}$.
    \item Haga lo mismo para $A=\begin{bmatrix*}[r]-1 & -18 & -7 \\ 1 & -13 & -4 \\ -1 & 25 & 8\end{bmatrix*}$.
    \item Una matriz $A$ de $n \times n$ es nilpotente si existe un entero $k$ tal que $A^{k}=0$. Si $k$ es el entero más pequeño de este tipo, entonces $k$ se denomina índice de nilpotencia de $A$. Demuestre que si $k$ es el índice de nilpotencia de $A$ y si $m \geq k$, entonces $A^{m}=0$.
    \item Sea $N_{k}$ la matriz definida por la ecuación \eqref{JAJJAJAJAJAJHQHHQJQUUAUQUWUAUSISIUAJ}. Demuestre que $N_{k}$ es nilpotente con índice de nilpotencia $k$.
    \item Escriba todas las matrices de Jordan de $4 \times 4$ posibles.
\end{enumerate}
De los problemas 140 al 148 está dado el polinomio característico de una matriz $A$. Escriba todas las posibles formas canónicas de Jordan de $A$.
\begin{multienumerate}
    \setcounter{multienumi}{139}
    \mitemxxx {$(\lambda+3)^{2}(\lambda+1)$}{$(\lambda-1)^{3}(\lambda+1)^{2}$}{$(\lambda-3)^{3}(\lambda+4)$}
    \mitemxxx{$(\lambda-3)^{4}$}{$(\lambda-4)^{3}(\lambda+3)^{2}$}{$(\lambda-6)(\lambda+7)^{4}$}
    \mitemxxx{$(\lambda-2)(\lambda+2)^{5}$}{$(\lambda+7)^{5}$}{$(\lambda-1)^{2}(\lambda+4)^{3}$}
\end{multienumerate}
\begin{enumerate}[start=149]
    \item Usando la forma canónica de Jordan, demuestre que para cualquier matriz $A$ de $n \times n$, $\operatorname{det} A=\lambda_{1} \lambda_{2} \cdots \lambda_{n}$, donde $\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}$ son los valores característicos de $A$.
\end{enumerate}